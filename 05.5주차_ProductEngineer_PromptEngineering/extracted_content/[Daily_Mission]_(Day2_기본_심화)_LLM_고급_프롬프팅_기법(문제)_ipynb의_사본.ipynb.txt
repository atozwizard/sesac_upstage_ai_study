# Day2 | 고급 프롬프팅 기법 실습 (기본 + 심화)

## 이 실습을 왜 하나요?
Day1에서는 **(1) 데이터 로드 → (2) 프롬프트 생성 → (3) 모델 호출(API) → (4) 정답 추출 → (5) 채점/평가**까지의 기본 파이프라인을 만들었습니다.

하지만 실제로는 **같은 모델/같은 데이터**라도 프롬프트 구조에 따라 다음과 같은 오류가 크게 달라집니다.
- 계산/추론 과정 생략(논리 점프)
- 조건 누락/오독
- 선택지(A/B/C/D) 혼동
- 답변 형식이 불안정해서 자동 채점이 어려움

Day2에서는 **모델과 데이터는 고정**하고, **프롬프트(입력 텍스트)만 바꿔가며** 결과를 비교합니다.  
즉, “코딩”보다 **프롬프트 설계**에 초점을 둔 실습입니다.

---

## 오늘의 목표
- Day1에서 사용한 **MMLU 로딩/평가 코드**를 그대로 유지
- Day1에서 사용한 **프롬프트 생성 함수**를 그대로 유지
- Day2에서 새로 다루는 기법(Zero-shot CoT, Plan-and-Solve, Self-Ask, Self-Consistency, Self-Verification, Self-Refine)을 **MMLU에 적용**
- **Day1 결과 vs Day2 결과**를 같은 조건에서 비교

---

## 노트북 구성
- Part 0: 환경/API 설정 (Day1과 동일)
- Part 1: MMLU 데이터 로드 (Day1과 동일)
- Part 2: Day1 프롬프트(베이스라인/커스텀) 재사용 + 평가
- Part 3: Day2 고급 프롬프팅(기본편) — 여러분이 프롬프트를 설계하는 구간
- Part 4: Day2 고급 프롬프팅(심화편) — 여러분이 프롬프트를 설계하는 구간

---

## 학습자가 실제로 하는 일

이 강의는 *프롬프트 엔지니어링*이 목적이므로,
대부분의 실습은 `### Implement Here ###` 블록에서 **프롬프트 문자열을 설계**하는 것입니다.

> ⚠️ API key가 필요합니다.

> ⚠️ 원활한 실습을 위해 코랩(Colab) 환경에서 실습 진행하시는 것을 권장합니다.


# (Colab 권장) 필요한 라이브러리 설치
# - 로컬에서 이미 설치되어 있다면 생략 가능
!pip -q install openai datasets tqdm pandas


## 0. API 설정 (Day1과 동일)

아래 3개 코드는 **Day1 노트북과 동일한 형태**입니다.
- `os.environ["UPSTAGE_API_KEY"]` 에 본인 키를 넣어주세요.
- 이미 환경변수로 키를 관리한다면 `os.environ[...] = ...` 줄은 주석 처리하고 사용해도 됩니다.


import os

# 본인 키로 교체하세요 (또는 환경변수로 이미 설정되어 있다면 이 줄을 주석 처리하세요)
os.environ["UPSTAGE_API_KEY"] = "up_w8c0bYvkD7x9OUmUjJrzPlRv8GlLV"


import time
from openai import OpenAI

# Upstage(OpenAI-compatible) endpoint
client = OpenAI(
    api_key=os.environ["UPSTAGE_API_KEY"],
    base_url="https://api.upstage.ai/v1",
)




def call_model_api(prompt, model="solar-mini-250422"):
    """Day1과 동일: 단일 프롬프트 → 응답 텍스트 반환"""
    start_time = time.time()
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=False,
        temperature=0.0,
        max_tokens=256,
    )
    elapsed_time = time.time() - start_time
    output_text = response.choices[0].message.content
    return output_text, elapsed_time


### (Day2용) temperature 샘플링이 필요할 때만 쓰는 헬퍼
Day1의 `call_model_api()`는 그대로 두고,  
Day2 기법(예: self-consistency)에서 **temperature 샘플링**이 필요할 때만 아래 래퍼를 사용합니다.


MODEL_NAME = "solar-mini-250422"

def call_model_api_with_params(prompt, *, model=MODEL_NAME, temperature=0.0, max_tokens=256):
    start_time = time.time()
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=False,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    elapsed_time = time.time() - start_time
    output_text = response.choices[0].message.content
    return output_text, elapsed_time

# call_model_api와 call_model_api_with_params를 모두 포함하는 generate를 위한 래퍼
class APILLM:
    def __init__(self, model: str = MODEL_NAME):
        self.model = model

    def generate(self, prompt: str, *, temperature: float = 0.0, max_tokens: int = 256):
        text, _ = call_model_api_with_params(
            prompt,
            model=self.model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return text


## 1. MMLU 데이터 로드 (Day1과 동일)

- Hugging Face `datasets`로 `cais/mmlu` 를 불러옵니다.
- 하나의 subject를 선택해 `dev/test`를 사용합니다.
- `dev`는 few-shot 예시(Example)로 쓰고, `test`는 평가용으로 씁니다.


from datasets import load_dataset, get_dataset_config_names

# 전체 subject 목록을 확인하고 싶으면 주석을 해제하세요.
subjects = get_dataset_config_names("cais/mmlu")
print("Number of subjects:", len(subjects))
print(subjects[:20])

SUBJECT = "high_school_physics"  # 원하는 subject로 바꿔도 됩니다. 예: "high_school_physics"
dataset = load_dataset("cais/mmlu", SUBJECT)

print("Loaded subject:", SUBJECT)
print("Splits:", list(dataset.keys()))
print("Example (dev[0]) keys:", dataset["dev"][0].keys())
dataset["dev"][0]


## 2. Day1 프롬프트 생성 코드 (그대로 재사용)

Day2에서는 “새로운 기법”을 추가하지만, 비교를 위해 Day1에서 사용했던 프롬프트 생성 함수를 그대로 유지합니다.

- `format_mmlu_prompt_basic(sample)`: **Baseline**
- `format_mmlu_prompt_custom(sample, ...)`: instruction/persona/icl/cot 조합형


# Day1: sample이 들어왔을 때 기본적인 prompt로 만들기 위한 함수 (basic formatting)
def format_mmlu_prompt_basic(sample):
    question = sample["question"]
    choices = sample["choices"]
    prompt = question + "\n\n"
    for i, choice in enumerate(choices):
        prompt += f"{chr(ord('A') + i)}. {choice}\n"
    return prompt

# Day1: 구체적인 지시사항 작성
def build_instruction():
    return (
        "You are given a multiple-choice question.\n"
        "Choose the correct answer from A, B, C, or D.\n"
        "[Just Answer with the one letter only.]\n\n"
    )

# Day1: persona 부여
def build_persona():
    return (
        "You are an expert with deep knowledge across many academic domains.\n\n"
    )

# Day1: chain-of-thought instruction
def build_cot_instruction():
    return (
        "Think step by step before answering.\n\n"
    )

# Day1: ICL 예시 1개 (dev[0])를 사용
def build_icl_examples():
    idx = dataset["dev"][0]["answer"]
    letter = chr(ord('A') + idx)
    return (
        "Example:\n"
        f"Question: {dataset['dev'][0]['question']}\n"
        f"A. {dataset['dev'][0]['choices'][0]}\n"
        f"B. {dataset['dev'][0]['choices'][1]}\n"
        f"C. {dataset['dev'][0]['choices'][2]}\n"
        f"D. {dataset['dev'][0]['choices'][3]}\n"
        f"Answer: {letter}\n\n"
    )

# Day1: custom prompt
def format_mmlu_prompt_custom(
    sample,
    use_instruction=False,
    use_persona=False,
    use_icl=False,
    use_cot=False
):
    prompt = ""
    if use_persona:
        prompt += build_persona()
    if use_icl:
        prompt += build_icl_examples()
    if use_cot:
        prompt += build_cot_instruction()
    if use_instruction:
        prompt += build_instruction()

    # 문제 본문
    prompt += sample["question"] + "\n\n"
    for i, choice in enumerate(sample["choices"]):
        prompt += f"{chr(ord('A') + i)}. {choice}\n"
    prompt += "\nAnswer:"
    return prompt


## 3. 유틸리티: 정답 파싱/채점/평가

MMLU는 정답이 0~3 인덱스로 들어있습니다.  
우리는 이를 `A/B/C/D`로 변환해 채점합니다.


import re
from typing import List, Dict, Tuple
from collections import Counter

def gold_letter(sample) -> str:
    idx = int(sample["answer"])
    return chr(ord("A") + idx)

def extract_mmlu_answer(text: str) -> str:
    """LLM 출력에서 A/B/C/D 하나를 추출 (강건하게)"""
    if text is None:
        return ""

    t = text.strip()

    # 1) Answer: X 형태 우선
    m = re.search(r"(?i)\bAnswer\s*:\s*([ABCD])\b", t)
    if m:
        return m.group(1).upper()

    # 2) 'The answer is X' 형태
    m = re.search(r"(?i)\b(answer\s+is|final\s+answer\s+is)\s*[:\-]?\s*([ABCD])\b", t)
    if m:
        return m.group(2).upper()

    # 3) 출력이 'A' 혹은 'A.' 같은 경우
    # m = re.search(r"\b([ABCD])\b\.?\s*$", t)
    pattern = r"\b([A-D])\b"
    matches = re.findall(pattern, t)
    if matches:
        return matches[0]

    # 4) fallback: 첫 등장 알파벳 (너무 공격적이면 오답 위험)
    m = re.search(r"\b([ABCD])\b", t)
    return m.group(1).upper() if m else ""

def is_correct(pred_letter: str, gold: str) -> bool:
    return (pred_letter or "").strip().upper() == (gold or "").strip().upper()

def majority_vote(cands: List[str]) -> str:
    """다수결. 동률이면 사전순(A<B<C<D)"""
    cnt = Counter([c for c in cands if c])
    if not cnt:
        return ""
    best_freq = max(cnt.values())
    best = sorted([k for k, v in cnt.items() if v == best_freq])[0]
    return best

import re

def extract_answer_from_output(output_text, choices=("A", "B", "C", "D")):
    """
    모델의 출력 문자열에서 선택지(A/B/C/D) 중 하나를 추출하는 함수

    Parameters
    ----------
    output_text : str
        모델이 생성한 원본 출력 텍스트
    choices : tuple
        허용할 선택지 문자 (기본값: A, B, C, D)

    Returns
    -------
    str or None
        추출된 정답 (예: "A"), 추출 실패 시 None
    """

    if output_text is None:
        return None

    output_text = output_text.strip()

    pattern = r"\b([A-D])\b"
    matches = re.findall(pattern, output_text)

    if matches:
        return matches[0]  # 가장 먼저 등장한 선택지 사용

    # 2. 추출 실패
    return None

## 3-1. 추론 과정 로깅 시스템

각 고급 프롬프팅 기법의 추론 과정을 단계별로 기록하는 로깅 시스템을 구현합니다.

import os
from datetime import datetime
from pathlib import Path

class ReasoningLogger:
    """각 프롬프팅 기법의 추론 과정을 단계별로 기록하는 로거"""
    
    def __init__(self, log_dir="reasoning_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.current_log = []
        self.method_name = None
        self.sample_idx = None
        
    def start_logging(self, method_name: str, sample_idx: int, question: str):
        """새로운 추론 과정 로깅 시작"""
        self.method_name = method_name
        self.sample_idx = sample_idx
        self.current_log = []
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.log(f"\n{'='*80}")
        self.log(f"추론 과정 기록 시작")
        self.log(f"기법: {method_name}")
        self.log(f"샘플 인덱스: {sample_idx}")
        self.log(f"시작 시간: {timestamp}")
        self.log(f"{'='*80}\n")
        self.log(f"문제:\n{question}\n")
        
    def log_step(self, step_name: str, content: str, print_console: bool = True):
        """단계별 추론 과정 기록"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"\n[단계: {step_name}] ({timestamp})\n"
        log_entry += "-" * 80 + "\n"
        log_entry += f"{content}\n"
        log_entry += "-" * 80
        
        self.current_log.append(log_entry)
        
        if print_console:
            print(log_entry)
    
    def log(self, message: str, print_console: bool = True):
        """일반 메시지 기록"""
        self.current_log.append(message)
        if print_console:
            print(message, end="")
    
    def save_log(self):
        """현재 추론 과정을 파일로 저장"""
        if not self.current_log:
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.method_name}_sample{self.sample_idx}_{timestamp}.txt"
        filepath = self.log_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("".join(self.current_log))
            f.write(f"\n\n{'='*80}\n")
            f.write(f"기록 완료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*80}\n")
        
        print(f"\n✅ 추론 과정이 저장되었습니다: {filepath}\n")
        return filepath

# 전역 로거 인스턴스
reasoning_logger = ReasoningLogger()

### 로깅 시스템 사용 예시

로깅 시스템을 테스트하려면 아래 셀을 실행하세요:

# 로깅 시스템 테스트 예시
# 단일 샘플에 대해 각 기법의 추론 과정을 확인할 수 있습니다

test_sample = dataset["test"][0]  # 첫 번째 테스트 샘플

print("=" * 80)
print("로깅 시스템 테스트")
print("=" * 80)
print("\n테스트할 문제:")
print(f"Q: {test_sample['question']}")
for i, c in enumerate(test_sample['choices']):
    print(f"{chr(ord('A')+i)}. {c}")
print(f"\n정답: {gold_letter(test_sample)}")
print("\n" + "=" * 80)

# Zero-shot CoT 테스트
print("\n[테스트 1] Zero-shot CoT")
result = run_method_single_call("zero_shot_cot", test_sample, temperature=0.7, sample_idx=0, enable_logging=True)

print("\n" + "=" * 80)
print("테스트 완료! reasoning_logs/ 폴더를 확인하세요.")
print("=" * 80)

## 4. Day1 결과 재현 (Baseline vs Day1 Custom)



from tqdm import tqdm
import pandas as pd

llm = APILLM(MODEL_NAME)

NUM_SAMPLES = 50  # 비용/시간을 고려해 적당히 조절하세요
eval_samples = dataset["test"].select(range(NUM_SAMPLES))

def run_single_call(prompt_builder, sample, *, temperature=0.8):
    prompt = prompt_builder(sample)
    out = llm.generate(prompt, temperature=temperature)
    pred = extract_mmlu_answer(out)
    gold = gold_letter(sample)
    return {"pred": pred, "gold": gold, "correct": is_correct(pred, gold), "raw": out, "prompt": prompt}

# Day1 baseline
def day1_baseline_builder(sample):
    return format_mmlu_prompt_basic(sample)

# Day1 "강화" 프롬프트(Instruction + Persona + ICL + CoT)
def day1_custom_builder(sample):
    return format_mmlu_prompt_custom(
        sample,
        use_instruction=True,
        use_persona=True,
        use_icl=True,
        use_cot=True
    )


# Part 3. Day2 고급 프롬프팅 (기본편)

이 파트에서 여러분이 할 일은 **“프롬프트 템플릿(문장)”을 설계**하는 것입니다.  
코드 로직(루프/파싱/채점)은 최대한 제공됩니다.

아래 함수들의 `### Implement Here ###` 블록을 채우면, 같은 MMLU 문제를 서로 다른 프롬프트 전략으로 풀 수 있습니다.

---

## TODO A1. 프롬프트 템플릿 4종
- Zero-shot CoT
- Plan-and-Solve
- CoT few-shot
- Self-Ask

> 목표: **정답률** 뿐 아니라, **출력 형식의 안정성**(정답 추출이 잘 되는가)도 함께 개선해보세요.

> ⚠️ **해당 기법과 함께 적절한 프롬프트를 섞어 최종 프롬프트를 구성합니다.**

> ⚠️ 미션에서 사용하는 solar 모델은 영어에서 더 잘 작동한다는 점을 유의하세요.


# 아래 5개 함수는 모두 "MMLU sample(dict)"을 입력으로 받아
# 모델에게 줄 프롬프트 문자열(str)을 반환해야 합니다.

FEWSHOT_DEMO = {
    "question": "What is 2 + 2?",
    "choices": ["3", "4", "5", "22"],
    "answer": "B",
    "reasoning": "2 + 2 equals 4, so the correct option is B.",
}


def _format_question_with_choices(sample) -> str:
    prompt = sample["question"] + "\n\n"
    for i, c in enumerate(sample["choices"]):
        prompt += f"{chr(ord('A')+i)}. {c}\n"
    return prompt

def build_zero_shot_cot_prompt(sample) -> str:
    """Zero-shot CoT: 'Let's think step by step' 트리거 포함"""

    return (
        "You are solving a multiple-choice question. Think step by step to reason through the problem.\n"
        "After your reasoning, provide your final answer as a single letter (A, B, C, or D).\n\n"
        + _format_question_with_choices(sample)
        + "\nLet's think step by step:\n"
    )

def build_plan_and_solve_prompt(sample) -> str:
    """Plan-and-Solve: 반드시 'Plan:' 과 'Solve:' 토큰 포함"""
    return (
        "You are solving a multiple-choice question. First, create a plan, then solve step by step.\n"
        "Format your response as:\n"
        "Plan: [your plan]\n"
        "Solve: [step-by-step solution]\n"
        "Answer: [single letter A, B, C, or D]\n\n"
        + _format_question_with_choices(sample)
        + "\nPlan:\n"
    )

def build_cot_fewshot_prompt(sample) -> str:
    """CoT few-shot: (Q, Reasoning, Answer) 예시 1개 이상 포함"""
    demo = FEWSHOT_DEMO
    demo_block = (
        "Example:\n"
        f"Question: {demo['question']}\n"
        f"A. {demo['choices'][0]}\n"
        f"B. {demo['choices'][1]}\n"
        f"C. {demo['choices'][2]}\n"
        f"D. {demo['choices'][3]}\n"
        f"Reasoning: {demo['reasoning']}\n"
        f"Answer: {demo['answer']}\n\n"
    )
    return (
        '''
        You are given a multiple-choice question.
        Return only one letter (A, B, C, or D) in the format.
        '''
        + demo_block
        + _format_question_with_choices(sample)
    )

def build_self_ask_prompt(sample) -> str:
    """Self-Ask: 아래 스캐폴딩을 포함
- Are follow up questions needed here:
- Follow up:
- Intermediate answer:
- So the final answer is:
- Answer:
"""
    return (
        "You are an expert problem solver. Break down complex questions into simpler sub-questions.\n"
        "Use the following format to think through the problem:\n\n"
        + _format_question_with_choices(sample)
        + "\nAre follow up questions needed here: "
    )

## TODO A2. 검증/개선 기반 기법

이제는 “한 번 답하고 끝”이 아니라, **답을 여러 번 생성/검증/개선**하는 파이프라인을 만듭니다.

- Self-Consistency: 여러 번 샘플링 후 다수결
- Self-Refine: 피드백을 통해 답을 개선

여기서도 핵심은 프롬프트 설계입니다.아래 프롬프트 함수(TODO)를 채우세요.

⚠️ **해당 기법과 함께 적절한 프롬프트를 섞어 최종 프롬프트를 구성합니다.**

⚠️ 미션에서 사용하는 solar 모델은 영어에서 더 잘 작동한다는 점을 유의하세요.


def run_method_single_call(method: str, sample, *, temperature=0.0) -> Dict:
    builders = {
        "zero_shot_cot": build_zero_shot_cot_prompt,
        "plan_and_solve": build_plan_and_solve_prompt,
        "cot_fewshot": build_cot_fewshot_prompt,
        "self_ask": build_self_ask_prompt
    }
    prompt = builders[method](sample)
    out = llm.generate(prompt, temperature=temperature)
    pred = extract_mmlu_answer(out)
    gold = gold_letter(sample)
    return {"method": method, "pred": pred, "gold": gold, "correct": is_correct(pred, gold), "raw": out, "prompt": prompt}

# self_consistency
def self_consistency_answer(sample, *, n: int = 5, temperature: float = 0.7) -> Tuple[List[str], str]:
    base_prompt = build_plan_and_solve_prompt(sample)  # 권장: 구조화된 reasoning 유도
    cands = []
    for _ in range(n):
        out = llm.generate(base_prompt, temperature=temperature)
        cands.append(extract_mmlu_answer(out))
    maj = majority_vote(cands)
    return cands, maj


# self_refine
def self_refine_answer(sample) -> Tuple[str, str, str]:
    # 1) 초기 생성 (일부러 baseline 성격)
    init_prompt = format_mmlu_prompt_custom(sample, use_instruction=True, use_persona=False, use_icl=False, use_cot=False)
    init_out = llm.generate(init_prompt, temperature=0.6)

    # 2) 피드백 생성
    fb_prompt = build_feedback_prompt(sample, init_out)
    fb_out = llm.generate(fb_prompt, temperature=0.8)

    # 3) 개선 생성
    ref_prompt = (
        "You are revising your previous answer based on feedback.\n"
        "Return only the final answer letter as: Answer: X\n\n"
        f"QUESTION:\n{sample['question']}\n\n"
        f"CHOICES:\n" +
        "\n".join([f"{chr(ord('A')+i)}. {c}" for i,c in enumerate(sample['choices'])]) +
        "\n\n"
        f"PREV_OUTPUT:\n{init_out}\n\n"
        f"FEEDBACK:\n{fb_out}\n"
    )
    ref_out = llm.generate(ref_prompt, temperature=0.9)
    return init_out, fb_out, ref_out

def build_feedback_prompt(sample, prev_output: str) -> str:
    """[FEEDBACK] 이전 답변을 보고 오류 가능성을 짚어주는 피드백 생성"""
    choices_block = "\n".join([f"{chr(ord('A')+i)}. {c}" for i, c in enumerate(sample["choices"])])
    return (
        "You are an expert reviewer evaluating a previous answer to a multiple-choice question.\n"
        "Critically examine the previous output for logical errors or misconceptions.\n"
        "Provide constructive feedback and determine the correct answer.\n\n"
        "Review the following:\n"
        "- Are there any logical errors in the reasoning?\n"
        "- Are there any misconceptions or incorrect assumptions?\n"
        "- What is the correct answer and why?\n\n"
        f"Question: {sample['question']}\n"
        f"Choices:\n{choices_block}\n\n"
        f"Previous output:\n{prev_output}\n\n"
        "Feedback:\n"
    )

## TODO A3. Day2 방법들 평가 & Day1과 비교

아래 셀을 실행하면, Day1 방식과 Day2 방식의 정확도를 같은 데이터/모델 조건에서 비교할 수 있습니다.

- 구현이 덜 된 상태라면 정확도가 낮게 나올 수 있습니다.
- Self-consistency / verification / refine는 한 문제당 호출이 많으므로 샘플 수를 작게 두는 것을 권장합니다.


# 평가용 샘플
NUM_SAMPLES_LIGHT = 50   # 1-call methods
NUM_SAMPLES_HEAVY = 5    # multi-call methods (self-consistency/verification/refine)

eval_light = dataset["test"].select(range(NUM_SAMPLES_LIGHT))
eval_heavy = dataset["test"].select(range(NUM_SAMPLES_HEAVY))

report_rows = []

# Day1 비교군
for name, builder in [("day1_baseline", day1_baseline_builder), ("day1_custom", day1_custom_builder)]:
    correct = 0
    for sample in tqdm(eval_light, desc=name):
        r = run_single_call(builder, sample, temperature=0.9)
        correct += int(r["correct"])
    report_rows.append({"method": name, "n": len(eval_light), "acc": correct/len(eval_light)})

# Day2 1-call methods
day2_methods = ["zero_shot_cot", "plan_and_solve", "cot_fewshot", "self_ask"]
for m in day2_methods:
    correct = 0
    for sample in tqdm(eval_light, desc=m):
        r = run_method_single_call(m, sample, temperature=0.7)
        correct += int(r["correct"])
    report_rows.append({"method": m, "n": len(eval_light), "acc": correct/len(eval_light)})

# Day2 multi-call methods
# Self-Consistency
correct = 0
for sample in tqdm(eval_heavy, desc="self_consistency"):
    cands, maj = self_consistency_answer(sample, n=5, temperature=0.7)
    gold = gold_letter(sample)
    correct += int(is_correct(maj, gold))
report_rows.append({"method": "self_consistency", "n": len(eval_heavy), "acc": correct/len(eval_heavy)})

# Self-Refine
correct = 0
for sample in tqdm(eval_heavy, desc="self_refine"):
    init_out, fb_out, ref_out = self_refine_answer(sample)
    pred = extract_mmlu_answer(ref_out)
    gold = gold_letter(sample)
    correct += int(is_correct(pred, gold))
report_rows.append({"method": "self_refine", "n": len(eval_heavy), "acc": correct/len(eval_heavy)})

pd.DataFrame(report_rows).sort_values("acc", ascending=False)


# Part 4. 심화편 | Self-Verification

LLM이 생성한 단일 답변을 그대로 신뢰하지 않고,
여러 개의 후보 답을 만든 뒤 각 답을 스스로 다시 검증(Self-Verification)하여 가장 신뢰도 높은 최종 답을 선택하는 방법을 구현해봅니다.

전체 흐름:
1) 동일한 문제에 대해 여러 후보 답(candidate)을 생성
2) 각 후보 답을 명확한 '결론 문장'으로 재서술(REWRITE)
3) 문제를 다시 풀어 후보 답의 정답 여부를 검증(VERIFY)
4) 검증 점수(Score)를 기준으로 최적의 답을 선택

⚠️ **해당 기법과 함께 적절한 프롬프트를 섞어 최종 프롬프트를 구성합니다.**

⚠️ 미션에서 사용하는 solar 모델은 영어에서 더 잘 작동한다는 점을 유의하세요.


def build_rewrite_prompt(sample, cand_answer: str) -> str:
    """[REWRITE] 후보 답을 '선언문(결론)'으로 재서술하도록 유도"""
    choices_block = "\n".join([f"{chr(ord('A')+i)}. {c}" for i, c in enumerate(sample["choices"])])
    return (
        "Rewrite the candidate answer as a clear, declarative statement (conclusion).\n"
        "The rewritten statement should clearly state what the answer is and why it is correct.\n\n"
        f"Question: {sample['question']}\n"
        f"Choices:\n{choices_block}\n\n"
        f"Candidate answer: {cand_answer}\n\n"
        "Rewritten as a declarative statement:\n"
    )

def build_verify_prompt(sample, cand_answer: str, rewritten: str) -> str:
    """[VERIFY] 후보 답이 맞는지 다시 풀어서 검증하고 Score: 0/1만 출력하도록 유도"""
    choices_block = "\n".join([f"{chr(ord('A')+i)}. {c}" for i, c in enumerate(sample["choices"])])
    return (
        "Verify whether the candidate answer is correct by solving the problem again.\n"
        "Think through the problem step by step and determine if the candidate answer is correct.\n"
        "At the end, output only: Score: 0 (if incorrect) or Score: 1 (if correct)\n\n"
        f"Question: {sample['question']}\n"
        f"Choices:\n{choices_block}\n\n"
        f"Candidate answer statement:\n{rewritten}\n"
        f"Candidate answer letter: {cand_answer}\n\n"
        "Verification:\n"
    )


## 추론 과정 로그 확인

로깅이 활성화된 경우, `reasoning_logs/` 폴더에 각 기법의 추론 과정이 텍스트 파일로 저장됩니다.

각 파일은 다음 정보를 포함합니다:
- 문제 내용
- 각 단계별 추론 과정
- 프롬프트와 모델 출력
- 최종 결과 (정답 여부)

파일명 형식: `{기법명}_sample{인덱스}_{타임스탬프}.txt`

# 저장된 로그 파일 목록 확인
import os
from pathlib import Path

log_dir = Path("reasoning_logs")
if log_dir.exists():
    log_files = sorted(log_dir.glob("*.txt"))
    print(f"총 {len(log_files)}개의 추론 과정 로그가 저장되어 있습니다:\n")
    for f in log_files:
        print(f"  - {f.name}")
else:
    print("아직 로그 파일이 생성되지 않았습니다. 평가를 실행하면 로그가 생성됩니다.")

def self_verification_answer(sample, *, n: int = 5, temperature: float = 0.7) -> Tuple[List[str], List[int], str]:
    # 1) 후보 생성
    base_prompt = build_plan_and_solve_prompt(sample)
    cands = []
    for _ in range(n):
        out = llm.generate(base_prompt, temperature=temperature)
        cands.append(extract_mmlu_answer(out))

    # 2) 후보 검증
    scores = []
    for cand in cands:
        rewrite_prompt = build_rewrite_prompt(sample, cand)
        rewritten = llm.generate(rewrite_prompt, temperature=1.0)

        verify_prompt = build_verify_prompt(sample, cand, rewritten)
        v = llm.generate(verify_prompt, temperature=1.0)

        m = re.search(r"Score\s*:\s*(\d+)", v)
        scores.append(int(m.group(1)) if m else 0)

    # 3) 최고 점수 후보 선택(동률이면 사전순)
    best = max(scores) if scores else 0
    best_cands = [c for c, s in zip(cands, scores) if s == best]
    chosen = sorted(best_cands)[0] if best_cands else (majority_vote(cands) if cands else "")
    return cands, scores, chosen

# Self-Verification
correct = 0
for sample in tqdm(eval_heavy, desc="self_verification"):
    cands, scores, chosen = self_verification_answer(sample, n=5, temperature=0.7)
    gold = gold_letter(sample)
    correct += int(is_correct(chosen, gold))
report_rows.append({"method": "self_verification", "n": len(eval_heavy), "acc": correct/len(eval_heavy)})

pd.DataFrame(report_rows).sort_values("acc", ascending=False)



내가 무얼 보아야 하는 것이오?  
수치가 변하는 것이 없소이다