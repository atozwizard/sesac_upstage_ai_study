[03강] LLM 고급 프롬프팅 기법 (1) 
© 2025 Upstage Co., Ltd. 조현수 
이화여자대학교 인공지능학과 조교수 
2 강의 목표 
학습 목표 
•LLM 고급 프롬프팅 기법에 대한 분류 체계에 대해 이해할 수 있다. 
•해당 체계 중 지시사항 및 예시 관련 프롬프팅 기법을 학습할 수 있다. 

3 01. LLM 고급 프롬프팅 기법 분류 
(1) LLM 고급 프롬프팅 기법 분류 (예시, 지시사항, 검증) 
02. 지시사항 관련 LLM 프롬프팅 기법 
(1) Zero-shot CoT 
(2) Plan-and-Solve Prompting 
03. 예시 관련 LLM 프롬프팅 기법 
(1) Chain-of-Thought(CoT) 
(2) Self-Ask 
(3) Least-to-most prompting 목차 
4 LLM 고급 프롬프팅 기법 분류 01
LLM 고급 프롬프팅 기법 분류 (지시사항, 예시, 검증) 
5 LLM 고급 프롬프팅 기법 분류 (지시사항, 예시) 03 LLM 고급 프롬프팅 기법 (1) 
1) 지시사항 관련 LLM 프롬프팅 기법 
•프롬프트에 관련 적절한 지시사항을 넣어 LLM 출력을 유도하는 방법 
•기법 예시 : Zero-shot-CoT / Plan-and-Solve prompting 
2) 예시 관련 LLM 프롬프팅 기법 
•프롬프트에 관련 예시를 기입하여 LLM 출력을 유도하는 방법 
•기법 예시 : CoT / Self-Ask / least-to-most prompting 
지시사항 
 응답 역할 부여 
예시 주입 문제 
6 지시사항 관련 LLM 프롬프팅 기법 02
Zero-shot-CoT / Plan-and-Solve(PS) prompting 
7 지시사항 관련 LLM 프롬프팅 기법 03 LLM 고급 프롬프팅 기법 (1) 
지시사항 관련 LLM 프롬프팅 기법 기법 개요 
•(입력, 출력) 예시 없이, 프롬프트에 포괄적인 전략을 담은 지시사항 을 포함하여 성능 향상을 유도하는 기법 
LLM 입력(질문) 출력 기본 입출력 과정 
LLM 입력(질문) 
전략 담은 출력 지시사항 관련 LLM 프롬프팅 기법 
포괄적 전략 담은 지시사항 
8 지시사항 관련 LLM 프롬프팅 기법 03 LLM 고급 프롬프팅 기법 (1) 
프롬프트에 지시사항을 어떤 식으로 제공할 수 있을까? 
•정답을 내기 전에 추론 과정을 생성 하도록 지시사항을 추가 - Zero-shot-CoT prompting 
•문제 해결을 위해 먼저 계획을 세우고 , 그 계획을 단계적으로 수행 하도록 지시사항을 추가 - Plan-and-Solve (PS) prompting 
LLM 입력(질문) 
전략 담은 출력 지시사항 관련 LLM 프롬프팅 기법 
포괄적 전략 담은 지시사항 
9 Zero-shot CoT 03 LLM 고급 프롬프팅 기법 (1) 
Intuition 
•답변하기 전에 생각하라고 말하면 더 잘 풀 수 있을까? 
•사람도 “차근차근 생각해 보고 답해봐”라는 말을 들으면, 즉흥적으로 답하기보다 한 번 더 정리하고 숙고해서 답하는 경우가 
많음 
•모델이 바로 정답을 내도록 두기보다, 먼저 추론 과정을 생성하도록 지시하면 문제 해결 성능이 향상될 수 있을 것임 
[1] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213. 

10 Zero-shot CoT 03 LLM 고급 프롬프팅 기법 (1) 
Zero-shot-CoT prompting 개요 
•지시사항에 “트리거 문장”을 추가하여 LLM이 질문에 답하기 전에 추론 과정을 유도하는 방법 
•트리거 문장 예시 : 단계별로 생각해보자. / 논리적으로 생각해보자 
•범용적(task-agnostic)인 방법: 과제별로 프롬프트를 수정하지 않고도 다양한 추론 과제에서 추론 과정을 유도 
•2단계로 진행(추론 추출, 정답 추출) 
[1] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213. Q: 한 저글러는 공 16개를 저글링할 수 있다. 공의 절반은 
골프공이고, 골프공의 절반은 파란색이다. 파란색 골프공은 몇 
개인가? 
Q: 한 저글러는 공 16개를 저글링할 수 있다. 공의 절반은 
골프공이고, 골프공의 절반은 파란색이다. 파란색 골프공은 몇 
개인가? 
A: 단계별로 생각해 보자. 기본 입출력 과정 
Zero-shot-CoT LLM 
LLM 정답은 8이다. ❌
전체 공은 16개이다. 이 중 절반은 골프공이므로 
골프공은 8개이다. 골프공의 절반이 파란색이므로 
파란색 골프공은 4개이다. 
11 Zero-shot-CoT prompting 과정 
•1단계: 추론 추출 
•프롬프트 구성 : 입력(질문) + 연쇄적 사고를 끌어내기 위한 트리거 문장 
•출력 : 추론 과정을 담은 문장 
•2단계: 정답 추출 
•프롬프트 구성 : 1단계 입력 + 1단계 출력 + 정답 추출용 트리거 문장 
•출력 : 최종 답변 
[1] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." Advances in neural information processing systems 35 (2022): 22199-22213. Zero-shot CoT 03 LLM 고급 프롬프팅 기법 (1) 
Q: 한 저글러는 공 16개를 저글링할 수 있다. 공의 절반은 
골프공이고, 골프공의 절반은 파란색이다. 파란색 골프공은 몇 
개인가? 
A: 단계별로 생각해 보자. LLM 전체 공은 16개이다. 이 중 절반은 골프공이므로 골프공은  
8개이다. 골프공의 절반이 파란색이므로 파란색 골프공은  
4개이다. 
Q: 한 저글러는 공 16개를 저글링할 수 있다. 공의 절반은 
골프공이고, 골프공의 절반은 파란색이다. 파란색 골프공은 몇 
개인가? 
A: 단계별로 생각해 보자. 
전체 공은 16개이다. 이 중 절반은 골프공이므로 골프공은 8개이다.  
골프공의 절반이 파란색이므로 파란색 골프공은 4개이다. LLM 4
12 Intuition 
•바로 추론하기보다, 먼저 계획을 세우고 단계별로 수행하라고 하면 더 잘 풀까? 
•사람도 문제를 풀 때 ʻ계획 → 실행’ 순서로 접근하라는 지시를 받으면, 즉흥적으로 풀 때보다 더 체계적이고 정확한 결과를 내는 
경우가 많음 
•계획을 먼저 생성하게 한 뒤 그 계획을 따라가게 하면 단계 누락이나 논리적 오류를 줄이고 더 안정적인 답변을 얻을 수 있을 
것임 
[2] Wang, Lei, et al. "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models." arXiv preprint arXiv:2305.04091 (2023). Plan-and-Solve prompting 03 LLM 고급 프롬프팅 기법 (1) 
계획을 세우고 
단계별로 
수행해봐 
13 Plan-and-Solve(PS) prompting 개요 
•지시사항에 “트리거 문장”을 추가하여 LLM이 문제를 두 단계에 걸쳐 해결(문제 계획 수립+하위 과제 수행)하도록 유도하는 방법 
•트리거 문장 예시 : 먼저 문제를 이해하고 해결을 위한 계획을 세운 다음, 그 계획을 단계별로 수행하자 
•Zero-shot CoT의 단계 누락 문제를 해결 
•Zero-shot Co과 동일하게 2단계로 진행 
[2] Wang, Lei, et al. "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models." arXiv preprint arXiv:2305.04091 (2023). Plan-and-Solve prompting 03 LLM 고급 프롬프팅 기법 (1) 
Q: 학생이 20명인 댄스 수업에서 20%는 컨템포러리 댄스를 
수강했고, 남은 학생 중 25%는 재즈 댄스를 수강했으며, 나머지는 
힙합 댄스를 수강했다. 전체 학생 중 힙합 댄스를 수강한 학생의 
비율은 몇 퍼센트인가? 
A: 단계별로 생각해 보자. LLM 먼저, 전체 학생의 20%가 컨템포러리 댄스를 수강했으므로 80%가 
남는다. 다음으로, 남은 80%의 25%가 재즈 댄스를 수강했는데, 이는  
원래 전체 학생의 25%가 재즈 댄스를 수강했다는 의미가 된다.  그러면 
원래 전체의 55%가 남는다. 
마지막으로, 남은 학생들(55%)이 힙합 댄스를 수강했다. 따라서 전체 
학생의 55%가 힙합 댄스를 수강했다. ❌
(추론과정 오류) 
Q: 학생이 20명인 댄스 수업에서 20%는 컨템포러리 댄스를 
수강했고, 남은 학생 중 25%는 재즈 댄스를 수강했으며, 나머지는 
힙합 댄스를 수강했다. 전체 학생 중 힙합 댄스를 수강한 학생의 
비율은 몇 퍼센트인가? 
A: 먼저 문제를 이해하고 문제를 해결하기 위한 계획을 세워보자.  
그런 다음 계획에 따라 단계별로 문제를 해결하자. LLM 계획: 
1단계: 컨템포러리 댄스와 재즈 댄스를 수강한 학생 수의 합을 계산한다. 
2단계: 힙합 댄스를 수강한 학생 수를 계산한다. 
3단계:  힙합 댄스를 수강한 학생의 비율을 계산한다. 
해결: 
1단계: 20명의 20%는 4명이다. 남은 16명의 25%는 4명이다. 따라서 
컨템포러리 댄스와 재즈 댄스를 수강한 학생은 총 8명이다. 
2단계: 남은 12명은 힙합 댄스를 수강했다. 
3단계: 힙합 댄스를 수강한 학생의 비율은 12/20 = 60%이다. ✔
Zero-shot CoT 
PS
14 Zero-shot 지시 기반 기법의 한계 
•일관성 부족: 트리거만으로는 답변 구조와 품질이 매번 들쭉날쭉함 
•단계 누락: 추론을 하긴 해도 중간 계산이나 조건 처리가 빠지기 쉬움 
•그럴듯한 오답: 과정이 길어질수록 틀린 결론을 설득력 있게 포장할 위험 
•형식 통제 약함: 답만 원해도 장황해지거나 원하는 포맷으로 안 나올 수 있음 
•예시기반 방법론 
Zero-shot 기법들의 한계 03 LLM 고급 프롬프팅 기법 (1) 
15 예시 관련 LLM 프롬프팅 기법 03
Chain-of-Thought(CoT) / Self-Ask / Least-to-most prompting 

16 예시 관련 LLM 프롬프팅 기법 03 LLM 고급 프롬프팅 기법 (1) 
[Recap] ICL 중 Few-shot Prompting 
•프롬프트에 N개의 (입력, 출력) 예시 제공하는 프롬프팅 기법 
LLM 입력 예시 1 
 출력 예시 1 
입력 예시 n 
출력 예시 n …
입력(질문) 출력 LLM 입력(질문) 출력 기본 입출력 과정 
Few-shot Prompting 
17 예시 관련 LLM 프롬프팅 기법 03 LLM 고급 프롬프팅 기법 (1) 
프롬프트에 예시를 어떤 식으로 제공할 수 있을까? 
•정답 출력을 위한 중간 추론 과정 을 예시에 포함 -  CoT prompting 
•정답을 위해 필요한 후속 질문과 그에 대한 답변을 단계적으로 생성 하는 방식을 예시에 포함 - Self-Ask 
•질문을 하위 질문들로 분해 하고 분해된 하위 질문을 순차적으로 해결 하도록 예시를 구성 - Least-to-most prompting 
LLM 입력 예시 1 
 출력 예시 1 
입력 예시 n 
출력 예시 n …
입력(질문) 출력 Few-shot Prompting 
LLM 입력 예시 1 or 변형된 입력1 
 전략 담은 출력 예시1 
입력 예시 n or 변형된 입력n 
 전략 담은 출력 예시n …
입력(질문) 전략 담은 출력 예시 관련 LLM 프롬프팅 기법 중간 추론 과정 
출력 예시 n 후속질문 
중간 답변 
후속 질문 
하위 질문 
하위 질문 출력 예시 n 
하위 질문 
중간 답변 
하위 질문 
출력 예시 n CoT prompting Self-Ask 
Least-to-most prompting 
18 예시 관련 LLM 프롬프팅 기법 03 LLM 고급 프롬프팅 기법 (1) 
프롬프트에 예시를 제공하면 왜 도움이 될까? 
•LLM 텍스트 생성 방식 
•LLM은 문장을 한 번에 완성하는 것이 아니라, 확률에 기반해 다음 단어를 하나씩 예측하며 텍스트 생성 
•이러한 방식의 모델을 Autoregressive(자기회귀) 모델이라고 함 
•즉, 이전 단어들이 이후 단어 예측에 직접적인 영향을 미침 
•예시가 효과적인 이유 
•LLM은 입력된 문맥과 패턴을 바탕으로 다음 출력 결정 
•질문 이전에 (입력, 출력) 예시를 제공하면 예시의 패턴을 파악하여 유사한 형식으로 출력할 확률 상승 
•따라서 프롬프트에 예시를 포함하면 의도한 형태의 결과를 더 안정적으로 얻을 수 있음 
i 번째 단어를  예측 이전 단어들의  정보를  가지고 
19 Chain-of-Thought(CoT) 03 LLM 고급 프롬프팅 기법 (1) 
Intuition 
•바로 답하기보다, ʻ생각하게’ 하면 더 잘 풀까? 
•사람도 머릿속 풀이 과정을 거치면 더 어려운 문제를 잘 푸는 경우가 많음 
•모델이 중간 과정을 출력하게 하면 성능이 오르지 않을까? 
•중간 과정을 출력하게 하면 모델이 더 많은 토큰(추론 단계)을 생성함 
•이는 추론을 위해 추가적인 inference-time compute(테스트 시 계산량)를 쓰게 만드는 효과(일종의 test-time scaling)로 
볼 수 있음 
•LLM도 정답만 바로 내기 보다 중간 추론 단계를 생성하도록 유도하면 더 어려운 문제를 잘 풀 수 있을 것임 
[3] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837. 
VS
Few-shot Prompting 
•프롬프트에 N개의 (입력, 출력) 예시 제공하는 프롬프팅 기법 
•입력에 대한 출력이 단순 답으로 구성 
20 Chain-of-Thought(CoT) 03 LLM 고급 프롬프팅 기법 (1) 
[3] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837. LLM Q: 로저는 테니스공 5개를 가지고 있다. 
그는 테니스공 캔 2개를 더 산다. 각 캔에는 
테니스공이 3개씩 들어 있다. 지금 
테니스공은 모두 몇 개인가? 
A: 정답은 11이다. 
Q(실제 질문):  구내식당에는 사과가 23개 
있었다. 점심을 만들기 위해 20개를 
사용하고 6개를 더 샀다면, 사과는 몇 개가 
남아 있는가? A: 정답은 27이다. (입력, 출력) 예시 
Chain-of-Thought(CoT) Prompting 
•예시에 중간 추론 과정을 추가하여 CoT 생성을 유도하는 방법 
•Chain-of-Thought(CoT) : 최종 출력으로 이어지는 일련의 자연어 중간 추론 과정 
•프롬프트 구성 : (입력, CoT를 포함한 출력) 예시 + 입력(질문) 
21 Chain-of-Thought(CoT) 03 LLM 고급 프롬프팅 기법 (1) 
[3] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837. LLM Q: 로저는 테니스공 5개를 가지고 있다. 
그는 테니스공 캔 2개를 더 산다. 각 캔에는 
테니스공이 3개씩 들어 있다. 지금 
테니스공은 모두 몇 개인가? 
A: 로저는 처음에 공 5개를 가지고 있었다.  
테니스공이 3개씩 들어 있는 캔 2개는 총  
6개이다. 5 + 6 = 11 . 정답은 11이다. 
Q(실제 질문):  구내식당에는 사과가 23개 
있었다. 점심을 만들기 위해 20개를 
사용하고 6개를 더 샀다면, 사과는 몇 개가 
남아 있는가? A: 구내식당에는 원래 사과가 23개  
있었다. 점심을 만들기 위해 20개를  
사용했다. 따라서 23 - 20 = 3개가  
남았다. 사과를 6개 더 샀으므로 3 + 6  
= 9개이다. 정답은 9이다. (입력, 출력) 예시 
22 Self-Ask 03 LLM 고급 프롬프팅 기법 (1) 
Intuition 
•바로 답하기보다, 필요한 질문을 하나씩 던지게 하면 더 잘 풀 수 있을까? 
•사람도 복잡한 문제를 풀 때 “이걸 풀려면 무엇을 먼저 알아야 하지?”라고 스스로 질문을 나누어 접근하는 경우가 많음 
•질문에 답하기 위해 필요한 질문을 하나씩 던지고 답변하는 과정을 명시적으로 유도하면 더 어려운 문제를 잘 풀 수 있을 것임 
[4] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837. 질문 
테오도어 해커와 해리 본 워킨스 
중 누가 더 오래 살았는가? 입력 예시 1 
… 출력 예시 1 후속 질문 1 
테오도어 해커는 사망 당시 몇 
살이었는가? 중간 답변  1
테오도어 해커는 사망 
당시 65세였다. 
후속 질문 2 
해리 본 워킨스는 사망 당시 몇 
살이었는가? 중간 답변  2
해리 본 워킨스는 사망 
당시 69세였다. 최종 답변 
최종 답은: 해리 본 워킨스이다. 
23 Self-Ask 03 LLM 고급 프롬프팅 기법 (1) 
Self-Ask 개요 
•예시를 통해 모델 스스로 후속 질문 과 그에 대한 답변 을 생성한 뒤 최종 답변 을 출력하도록 유도하는 방법 
•CoT와 달리, 모델이 최종 답변을 내놓기 전까지의 과정이 구조적 임(후속 질문, 중간 답변으로 구분) 
•CoT와 동일하게, 후속질문-중간답변 그리고  최종 답변까지의 과정이 모델에 의해 자동 으로 진행 
•프롬프트 구성 : ( 입력, 후속질문 필요 여부 ,후속질문, 중간답) 예시 + 입력(질문) + 후속질문 필요 여부 
•후속질문 필요 여부 문구 : Are follow up questions needed here: 
[4] Press, Oﬁr, et al. "Measuring and narrowing the compositionality gap in language models." Findings of the Association for Computational Linguistics: EMNLP 2023. 2023. 예.
후속 질문:  초전도성은 언제 발견되었는가? 
중간 답변:  초전도성은 1911년에 발견되었다. 
후속 질문:  1911년 당시 미국 대통령은 누구였는가? 
중간 답변:  윌리엄 하워드 태프트. 
따라서 최종 답은: 윌리엄 하워드 태프트이다. 질문 : 테오도어 해커와 해리 본 워킨스 중 누가 더 오래 
살았는가? 
후속 질문이 필요한가? : 예.
후속 질문:  테오도어 해커는 사망 당시 몇 살이었는가? 
중간 답변:  테오도어 해커는 사망 당시 65세였다. 
후속 질문:  해리 본 워킨스는 사망 당시 몇 살이었는가? 
중간 답변:  해리 본 워킨스는 사망 당시 69세였다. 
따라서 최종 답은: 해리 본 워킨스이다. 
실제 질문 : 초전도성이 발견되었을 당시 미국 대통령은 
누구였는가? 
후속 질문이 필요한가? :LLM (입력, 출력) 예시 LLM 
입력(질문) 전략 담은 출력 후속질문 
중간 답변 
후속 질문 
출력 예시 n 
24 Self-Ask 03 LLM 고급 프롬프팅 기법 (1) 
Self-ask + Search Engine 
•Self-ask + 검색 엔진으로 구성하여 성능 개선하는 방법 
•후속 질문이 명확하게 구분 되어 있어 검색 엔진을 통한 답변 가능하기 때문 
•self-ask와 동일한 프롬프트 사용 
•과정 : ① 모델에 프롬프트 입력 ② 모델이 후속 질문 생성 및  “중간 답변:” 출력 시 모델 생성 멈춤 
③ 후속 질문을 검색 엔진 API로 보내 답변 획득 ④ 검색 엔진 답을 프롬프트에 추가 ⑤ 모델이 이후 생성 재개 
[4] Press, Oﬁr, et al. "Measuring and narrowing the compositionality gap in language models." Findings of the Association for Computational Linguistics: EMNLP 2023. 2023. 예.
추가 질문: 초전도성은 언제 발견되었는가? 
중간 답변: 질문:  테오도어 해커와 해리 본 워킨스 중 누가 더 오래 
살았는가? 
추가 질문이 필요한가?:  예.
추가 질문:  테오도어 해커는 사망 당시 몇 살이었는가? 
중간 답변: 테오도어 해커는 사망 당시 65세였다. 
추가 질문: 해리 본 워킨스는 사망 당시 몇 살이었는가? 
중간 답변: 해리 본 워킨스는 사망 당시 69세였다. 
따라서 최종 답은: 해리 본 워킨스이다. 
질문: 초전도성이 발견되었을 당시 미국 대통령은 
누구였는가? 
추가 질문이 필요한가?: LLM (입력, 출력) 예시 
Search 
Engine 쿼리:  초전도성은 언제 
발견되었는가? 
검색 결과: 초전도성은 
1911년에 발견되었다. ① ②
③④
25 Least-to-most prompting 03 LLM 고급 프롬프팅 기법 (1) 
Intuition 
•복잡한 문제를 한 번에 풀기보다, 더 쉬운 하위 문제들로 나누면 잘 풀 수 있을까? 
•복잡한 문제는 그 자체로는 어렵지만, 더 단순한 하위 문제들로 분해하면 각각은 해결 가능해지는 경우가 많음 
•사람도 문제를 풀 때 ʻ분해 → 순차 해결’ 전략을 자주 사용함 
•“이 문제를 풀려면 어떤 단계들이 필요하지?”라고 먼저 하위 문제로 나눈 뒤 
•하나씩 풀어가면서, 앞 단계의 결과를 활용해 다음 단계를 차례로 해결해 나감 
•복잡한 문제를 더 쉬운 하위 문제로 나눈 후 순차적으로 해결하면 더 어려운 문제를 잘 풀 수 있을 것임 
[5] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837. 질문 
에이미는 미끄럼틀 꼭대기까지 
올라가는 데 4분이 걸린다. 
미끄럼틀을 내려오는 데는 
1분이 걸린다. 워터슬라이드는 
15분 후에 문을 닫는다. 
문을 닫기 전까지 에이미는 몇 
번 미끄럼틀을 탈 수 있을까? 입력 예시 1 
 출력 예시 1 하위 질문 
한 번 탈 때 걸리는 시간은 
얼마인가? 
최종 질문 
문을 닫기 전까지 에이미는 몇 
번 미끄럼틀을 탈 수 있을까? 중간 답변 
에이미는 올라가는 데 4분, 내려오는 데 1분이 걸린다. 4 
+ 1 = 5. 따라서 한 번 탈 때 5분이 걸린다. 
최종 답변 
워터슬라이드는 15분 후에 문을 닫는다.  한 번 탈 때 
5분이 걸린다 . 에이미는 문을 닫기 전까지 15 ÷ 5 = 3 
번 미끄럼틀을 탈 수 있다. 
26 Least-to-most prompting 03 LLM 고급 프롬프팅 기법 (1) 
Least-to-most prompting 개요 
•각 단계마다 예시를 제공(few-shot prompting)하여 문제 분해(1단계) 및 문제 순차 해결(2단계)을 유도하는 방법 
[5] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." arXiv preprint arXiv:2205.10625 (2022). 단계 1: 질문을 하위 질문으로 분해 
단계 2: 하위 질문을 순차적으로 해결 입력 예시 
입력(질문) 하위 질문 예시 
하위 질문 예시 하위 질문1 LLM 하위 질문2 
하위 질문 예시 
최종 답변 예시 중간 답변 예시 
하위 질문 예시 중간 답변 LLM 
입력(하위 질문1) 
하위 질문 예시 
최종 답변 예시 중간 답변 예시 
하위 질문 예시 
최종 답변 LLM 
입력(하위 질문1) 
중간 답변 
입력(하위 질문2) 
27 Least-to-most prompting 03 LLM 고급 프롬프팅 기법 (1) 
Least-to-most prompting 과정 
•단계 1: 분해(Decomposition) 
•복잡한 문제를 더 쉬운 하위 문제들의 목록으로 분해 
•분해 프롬프트 구성 : (입력, 분해된 문제들) 예시 + 입력(질문) 
[5] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." arXiv preprint arXiv:2205.10625 (2022). Q(입력 예시) : 인구의 성별 분포는 남성 50.2%, 여성 
49.8%였다. 성인 인구 중 20~29세는 29명으로 전체의 
14.6%, 30~39세는 28명으로 14.1%, 40~49세는 
36명으로 18.2%, 50~59세는 31명으로 15.7%를 
차지했다. 40~49세가 아닌 사람들의 비율은 몇 
퍼센트인가? 
A(하위 질문 예시) : “40~49세가 아닌 사람들의 비율은  
몇 퍼센트인가? ”를 풀기 위해서는 먼저 “ 40~49세  
인구의 비율은 얼마인가? ”를 구해야 한다. 
Q(실제 질문) : 에이미는 미끄럼틀 꼭대기까지 올라가는 
데 4분이 걸린다. 미끄럼틀을 내려오는 데는 1분이 
걸린다. 워터슬라이드는 15분 후에 문을 닫는다. 문을 
닫기 전까지 에이미는 몇 번 미끄럼틀을 탈 수 있을까? A: “문을 닫기 전까지 몇 번 미끄럼틀을 탈  
수 있는가? ” 를 풀기 위해서는 먼저 “ 한 번  
탈 때 걸리는 시간은 얼마인가? ”를 구해야 
한다. LLM (입력, 출력) 예시 입력 예시 
입력(질문) 하위 질문 예시 
하위 질문 예시 하위 질문1 LLM 하위 질문2 
28 Least-to-most prompting 03 LLM 고급 프롬프팅 기법 (1) 
Least-to-most prompting 과정 
•단계 2: 하위 문제 해결(Subproblem solving) 
•이전 하위 문제들의 답을 활용하여 하위 문제들을 순차적으로 해결 
•하위 문제 해결 프롬프트 구성 :(하위 문제, 답) 예시 + (이전에 해결된 하위 질문, 답) 목록 + 다음 하위 질문 
[5] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." arXiv preprint arXiv:2205.10625 (2022). 인구의 성별 분포는 남성 50.2%, 여성 49.8%였다. 성인 인구 중 20~29세는 
29명으로 전체의 14.6%, 30~39세는 28명으로 14.1%, 40~49세는 36명으로 
18.2%, 50~59세는 31명으로 15.7%를 차지했다. 
Q(하위 질문 예시) : 40~49세 인구의 비율은 얼마인가? 
A(중간 답변 예시):  “40~49세는 36명으로 18.2%이다. ” 따라서 답은 18.2%
이다. 
Q(하위 질문 예시) : 40~49세가 아닌 사람들의 비율은 몇 퍼센트인가? 
A(최종 답변 예시):  40~49세가 18.2%라는 것을 알고 있다. 따라서 100% − 
18.2% = 81.8%가 40~49세가 아니다. 정답은 81.8%이다. 
에이미는 미끄럼틀 꼭대기까지 올라가는 데 4분이 걸린다. 미끄럼틀을 
내려오는 데는 1분이 걸린다. 워터슬라이드는 15분 후에 문을 닫는다. 
Q(실제 하위 질문1) : 한 번 탈 때 걸리는 시간은 얼마인가? LLM A(중간 답변):  에이미는 올라가는 데 
4분, 내려오는 데 1분이 걸린다. 4 + 
1 = 5. 따라서 한 번 탈 때 5분이 
걸린다. 
인구의 성별 분포는 남성 50.2%, 여성 49.8%였다. 성인 인구 중 20~29세는 29명으로 
전체의 14.6%, 30~39세는 28명으로 14.1%, 40~49세는 36명으로 18.2%, 
50~59세는 31명으로 15.7%를 차지했다. 
Q(하위 질문 예시) : 40~49세 인구의 비율은 얼마인가? 
A(중간 답변 예시):  “40~49세는 36명으로 18.2%이다. ” 따라서 답은 18.2%이다. 
Q(하위 질문 예시) : 40~49세가 아닌 사람들의 비율은 몇 퍼센트인가? 
A(최종 답변 예시):  40~49세가 18.2%라는 것을 알고 있다. 따라서 100% − 18.2% = 
81.8%가 40~49세가 아니다. 정답은 81.8%이다. 
에이미는 미끄럼틀 꼭대기까지 올라가는 데 4분이 걸린다. 미끄럼틀을 내려오는 데는 
1분이 걸린다. 워터슬라이드는 15분 후에 문을 닫는다. 
Q(실제 하위 질문1) : 한 번 탈 때 걸리는 시간은 얼마인가? 
A(중간 답변):  에이미는 올라가는 데 4분, 내려오는 데 1분이 걸린다. 4 + 1 = 5. 따라서 
한 번 탈 때 5분이 걸린다. 
Q(실제 하위 질문2) : 문을 닫기 전까지 에이미는 몇 번 미끄럼틀을 탈 수 있을까? LLM A(최종 답변): 워터슬라이드는 15분 
후에 문을 닫는다. 한 번 탈 때 5분이 
걸린다. 에이미는 문을 닫기 전까지 15 
÷ 5 = 3 번 미끄럼틀을 탈 수 있다. 하위 질문 예시 
최종 답변 예시 중간 답변 예시 
하위 질문 예시 중간 답변 LLM 
입력(하위 질문1) 
하위 질문 예시 
최종 답변 예시 중간 답변 예시 
하위 질문 예시 
최종 답변 LLM 
입력(하위 질문1) 
중간 답변 
입력(하위 질문2) 
29 예시 관련 LLM 프롬프팅 기법 03 LLM 고급 프롬프팅 기법 (1) 
CoT vs. Self-Ask vs. Least-to-most Prompting 
구분 Chain-of-Thought (CoT) Self-Ask Least-to-most Prompting 
공통점 예시를  통해 바로 정답을  출력하지  않고 중간 과정을  거치도록  유도
핵심 
아이디어 중간 추론 과정(CoT)  을 
출력하게  유도정답 전에 필요한  후속 질문과  
답변 을 생성하도록  유도복잡한  문제를  하위 문제로  
분해 하고 순차적으로  해결
구조화 연속적  자연어  추론
( 구조화  X)후속 질문 / 중간 답변으로  
명시적  구분분해 단계와  해결 단계가  
명확히  분리
프롬프트  
예시 
구성( 입력, CoT 포함 출력) 
예시( 입력, 후속질문 , 중간답 , 
최종답 ) 예시( 입력, 분해된  하위 문제들 ) 예시 
+ ( 하위 문제, 답) 예시
30 Summary 03 LLM 고급 프롬프팅 기법 (1) 
LLM 고급 프롬프팅 기법 (1) 
•LLM 고급 프롬프팅 기법 분류 
•지시사항 관련 LLM 프롬프팅 기법 / 예시 관련 LLM 프롬프팅 기법 / 검증 관련 LLM 프롬프팅 기법 
•지시사항 관련 LLM 프롬프팅 기법 
•Zero-shot-CoT : 지시사항에 “트리거 문장”을 추가하여 LLM이 질문에 답하기 전에 추론 과정을 유도하는 방법 
•Plan-and-Solve(PS) prompting : 지시사항에 “트리거 문장”을 추가하여 LLM이 문제를 두 단계에 걸쳐 해결 
    (문제 계획 수립+하위 과제 수행)하도록 유도하는 방법 

31 Summary 03 LLM 고급 프롬프팅 기법 (1) 
LLM 고급 프롬프팅 기법 (1) 
•예시 관련 LLM 프롬프팅 기법 
•Chain-of-Thought(CoT) : 예시를 제공(few-shot prompting)하여 CoT 생성을 유도하는 방법 
•Self-Ask : 예시를 제공(few-shot prompting)하여 모델 스스로 후속 질문과 그에 대한 답변을 생성한 뒤 최종 답변 출력을 
유도하는 방법 
•Least-to-most prompting : 각 단계마다 예시를 제공(few-shot prompting)하여 문제 분해(1단계) 및 문제 순차 해결 
     (2단계)을 유도하는 방법 
www.upstage.ai © 2025 Upstage Co., Ltd. 

