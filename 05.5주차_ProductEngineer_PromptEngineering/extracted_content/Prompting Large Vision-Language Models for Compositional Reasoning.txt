Prompting Large Vision-Language Models for Compositional Reasoning
Timothy Ossowski1, Ming Jiang3, Junjie Hu1,2
1Department of Computer Science,2Department of Biostatistics and Medical Informatics
University of Wisconsin, Madison, WI, USA
3Department of Human-centered Computing, Indiana University, Indianapolis, IN, USA
ossowski@wisc.edu ,mj200@iu.edu ,junjie.hu@wisc.edu
Abstract
Vision-language models such as CLIP have
shown impressive capabilities in encoding texts
and images into aligned embeddings, enabling
the retrieval of multimodal data in a shared
embedding space. However, these embedding-
based models still face challenges in effectively
matching images and texts with similar visio-
linguistic compositionality, as evidenced by
their performance on the recent Winoground
dataset. In this paper, we argue that this limita-
tion stems from two factors: the use of single
vector representations for complex multimodal
data, and the absence of step-by-step reasoning
in these embedding-based methods. To address
this issue, we make an exploratory step using
a novel generative method that prompts large
vision-language models (e.g., GPT-4) to de-
pict images and perform compositional reason-
ing. Our method outperforms other embedding-
based methods on the Winoground dataset, and
obtains further improvement of up to 10% accu-
racy when enhanced with the optimal descrip-
tion.1More importantly, we provide a fine-
grained error analysis of our method’s outputs,
highlighting the key bottleneck in understand-
ing image contents by existing VLMs.
1 Introduction
Recent advancements in vision-language models
(VLMs) have rapidly accelerated progress in multi-
modal tasks such as visual question answering (An-
tol et al., 2015) and image captioning (Lin et al.,
2014). Large vision-language encoders such as
CLIP (Radford et al., 2021) and UNITER (Chen
et al., 2020) have been trained to learn a joint em-
bedding space for combining visual and textual in-
formation. These aligned multimodal embeddings
have been widely used for zero-shot image-text re-
trieval (Plummer et al., 2015) and other challenging
multimodal matching tasks (Thrush et al., 2022).
1Code is available at https://github.com/tossowski/
KeyComp .Notably among these approaches, CACR (Pandey
et al., 2022) and IAIS (Ren et al., 2021) further
improve the multimodal representations by incen-
tivizing relation alignment during pretraining.
Despite remarkable advances, the embedding-
based methods still encounter difficulties in various
compositional reasoning tasks, particularly in the
recent Winoground task (Thrush et al., 2022). This
task evaluates the capability of VLMs to under-
stand the compositional relation among objects in
the images and their associated captions (see details
in §3). One primary limitation of embedding-based
methods is their reliance on compressing intricate
compositional information from an image or a text
into a single vector representation , which typically
emphasizes object encoding but is limited in distin-
guishing the nuanced relationships between objects
and their context in the image and caption. To
address this limitation, we propose an alternative
generative approach that utilizes the fine-grained
reasoning capabilities of large generative models in
comprehending image content and matching corre-
sponding texts. In contrast to traditional generative
methods that train specific modules for visual ques-
tion answering (Wang et al., 2022b; Uehara et al.,
2022), we use a tuning-free prompt-based method.
Specifically, our keyword-guided compositional
reasoning method ( KEYCOMP ) prompts a VLM
for depicting images based on the keywords de-
tected from the query text, and then uses a stronger
LLM to analyze the image description for match-
ing corresponding texts. Our method design is
mainly based on two considerations. First, our
analysis (§4) shows that directly prompting gener-
ative VLMs like MiniGPT4 (Zhu et al., 2023) or
BLIP-2 (Li et al., 2023) still poses a challenge for
the model in identifying key image and text con-
tents for further reasoning. Thus, proper guidance
is necessary to instruct the VLM to focus on key
image regions for image description. Second, we
believe that existing LLMs (e.g., GPT-4) possessarXiv:2401.11337v1  [cs.CV]  20 Jan 2024
Keyword-guided Image
Description by VLM
A: Snake eats bird
B: Bird eats snakeA lightbulb
surrounding some
plantsA B
Keyword Detection
e.g., {bird, snake, eat}Prompt-based
Reasoning by LLMDecision
e.g., Cap B/Img BText Score T ask Image Score T askFigure 1: Illustration of our generative method for the
Winoground task. Appendix C shows more detailed
descriptions and model outputs. Text Score Task: Our
method chooses the more appropriate caption given a
single image. Image Score Task: Our method chooses
the best image given a single caption.
A bottle in w aterImage Score T ask Description
Image A : This image
shows a bottle floating in
the w ater. It appears to be
made of glass and has a
cork in the top ...
Image B : This is a clear
plastic bottle filled with
water. It has a blue cap on
top and is standing upright
on a white surface.
Keyword Detection
{Bottle, W ater, In}Prompt-based
Reasoning 
Select the best image
for the caption
"A bottle in w ater".
Think step b y step and
start y our answer with
A or B .
Decision
A:  The image clearly
shows a glass bottle
partially submerged in
a body of w ater
A B
Figure 2: A detailed example for the image score task.
stronger language reasoning capabilities than the
available VLMs, thus we use a LLM for multi-step
reasoning instead of a weaker VLM used in concur-
rent prompting-based methods (You et al., 2023;
Shen et al., 2023; Wu et al., 2023).
We conduct thorough quantitative and qualitative
analyses of our method against existing embedding-
based methods on Winoground. Overall, KEY-
COMP achieves a state-of-the-art image score on
Winoground, surpassing the best embedding-based
method by a clear margin of 5.1% image score. Our
method excels at multi-step reasoning for complex
examples (Fig. 9) and unusual images (Fig. 17).
More importantly, further error analysis of the fail-
ure cases (Appendix G) reveals a bottleneck in the
image description quality of VLMs, shedding in-
sights for future directions. Particularly, VLMs
still struggle with describing spatial reasoning and
LLMs may misinterpret VLM descriptions with
complex syntax structures.
2 Method
This section first describes the Winoground tasks,
and then introduces KEYCOMP in three steps (il-
lustrated in Fig. 1 and Fig. 2).
Problem Definition Winoground (Thrush et al.,
2022) has two fundamental matching tasks to as-sess the visio-linguistic compositionality of VLMs.
Given two aligned image-caption pairs (I0, C0)
and(I1, C1), a VLM is asked to select the correct
caption from {C0, C1}for a single image (either I0
orI1) in the text score task. In the image score task,
a VLM is asked to select the correct image from
{I0, I1}for a single caption (either C0orC1).
Step 1: Keyword Detection Our method first
detects a keyword set K={w1, . . . , w k}automat-
ically from the caption text to help guide the VLM
to describe the relevant contents of the images. The
caption text is either a single caption in the image
task or the concatenation of two captions in the text
task. When detecting keywords, we use SpaCY2to
identify nouns, verbs, prepositions, and adjectives.
The intuition is that nouns in a sentence usually de-
scribe the key entities in the scene, and adjectives
depict the attributes of the entities (e.g., color, size,
shape), while verbs and prepositions usually reveal
a relation between two entities in an image.
Step 2: Keyword-guided Description Next, we
use a pretrained VLM fVLM such as MiniGPT4 or
BLIP-2 to generate text descriptions for images. To
ensure the VLM focuses on relevant image details
for image-text matching, we use the keywords Kto
guide the generation of image descriptions. Specifi-
cally, we append all extracted keywords in Kto the
end of a text “[Instruction]” to produce a prompt
PK. The prompt is then used to generate a high-
quality description of the entities and relations for
an image Iaby Eq. (2). Example descriptions and
prompts are presented in Appendix C and D.
PK=“[Instruction] w1, . . . ,wk” (1)
DK,a=fVLM(Ia, PK), a∈[0,1] (2)
Step 3: LLM Reasoning & Explanation As
LLMs have demonstrated an impressive zero-shot
language reasoning ability (Brown et al., 2020), we
prompt a LLM fLLMto perform reasoning on the
generated image descriptions and the given cap-
tions, and select an answer for Winoground tasks.
Additionally, inspired by recent chain-of-thought
prompting (Wei et al., 2022), we also add another
explanation instruction, such as “Think step by
step", to prompt the LLM to explain its answer
selection. Specifically, in the image task for se-
lecting the correct image from I0, I1for a caption
Ca, we construct a prompt Pimg,aby concatenating
2https://spacy.io/
Cawith a selection instruction, the two generated
image descriptions and an explanation instruction.
Similarly, we construct another prompt Ptxt,afor
an image Iain the text task. Finally, we feed the
prompt to the LLM to get a text output ycontaining
the selection and the explanation.
Pimg,a=“[T] [ Ca] [T] [ DK,0] [DK,1] [T]” (3)
Ptxt,a=“[T] [ DK,a] [T] [ C0] [C1] [T]” (4)
y=fLLM(Pt,a), t∈ {img, txt } (5)
where all “[T]” placeholders are the texts surround-
ing the key information in the template to construct
the prompt. Appendix B shows all prompt variants.
3 Experimental Settings
Dataset & Evaluation The Winoground dataset
consists of 400 items, each containing two image-
caption pairs (I0, C0),(I1, C1). While the images
in each item may be completely different, the two
captions {C0, C1}have an identical set of objects,
only in a different order. A model is evaluated by
the following text, image, and group scores.
•Text Score : The model is asked to pick the cor-
responding caption from {C0, C1}for a single
image Ia. The model gets a score of 1 if and only
if it picks the correct caption for both I0andI1.
•Image Score : The model is asked to pick the
corresponding image from {I0, I1}for a caption
Ca. The model gets a score of 1 if and only if it
picks the correct image for both C0andC1.
•Group Score : The model achieves a group score
of1for the item if and only if it receives a text
score of 1 and an image score of 1.
Methods in Comparison We compare our gen-
erative method with strong embedding-based meth-
ods, i.e., IAIS (Ren et al., 2021), CACR (Pandey
et al., 2022) and CLIP (Radford et al., 2021). These
methods choose the better caption or image by com-
puting a similarity score between their multimodal
embeddings and selecting the one with the highest
score. In contrast, our generative method generates
a text output for selection. We use string match-
ing on the model output to extract the selection for
evaluation. If the LLM generates an invalid output
(e.g., “Neither”) indicating neither answer matches
the query, we consider the prediction incorrect de-
spite a slight underestimation of our method. Our
experiments show that this invalid output occurs
about 10% of the time in the test set.Model Selection & Hyperparameters To pro-
duce higher-quality descriptions of images, we use
an instruction-tuned VLM, i.e., MiniGPT4 (Zhu
et al., 2023) with a backbone LLM of Vicuna-13b .
All descriptions are generated using a temperature
of 1 and a beam size of 10. With a lower beam
size and temperature, we observe notably worse de-
scriptions. For the reasoning component, we utilize
OpenAI’s ChatGPT API with the GPT-3.5-turbo
andGPT-4 models using a temperature of 1 and the
default values of the other hyperparameters.
4 Results and Analysis
Overall Performance Table 1 reports the overall
text, image, and group scores of our method com-
pared with existing embedding-based approaches.
Since LLMs are generative probabilistic models,
they do not have deterministic outputs. Therefore,
we report the average score across 3 different runs
of the LLM reasoning (Step 3) along with the stan-
dard deviation of our method. We observe several
findings: (1) Most notably, our method achieves a
significantly better image score than prior works,
reaching a new state-of-the-art image score. (2)
Our text and group scores are also competitive
with existing works, even when evaluating accu-
racy with strict string matching. (3) Despite the
stochastic nature of our method, the standard devia-
tion of LLM reasoning between runs rarely exceeds
1−2%. (4) However, we observe a significant dif-
ference in the image description quality from the
VLM (Step 2). To estimate an upper bound, we
instruct MiniGPT4 to sample 5 descriptions and
manually select the best image description to feed
into LLMs for reasoning. This leads to a large gain
of 12.4% text score and 3.2% image score, indicat-
ing a potential direction of automatically selecting
the best image descriptions for improvement.
Method LLM Text Image Group
KEYCOMP∗GPT-4 43.5 ±0.7 28.7 ±2.1 18.2 ±0.9
KEYCOMP∗GPT-3.5 42.7 ±0.8 27.8 ±0.7 17.4 ±0.3
KEYCOMP GPT-3.5 30.3 ±1.6 24.6 ±1.2 12.4 ±1.2
IAIS - 42.5 19.5 16.0
CACR - 39.3 17.8 14.3
CLIP - 30.8 10.5 8.0
Table 1: Text, Image, and Group scores scaled by 100 in
Winoground. We report the average scores and standard
deviation in 3 different runs of LLM reasoning (Step 3).
Asterisk indicates an upper bound, where we select the
best caption among 5 generated descriptions.
Image Description Quality Matters. Fig. 3 dis-
plays the average text score of our method on var-
0204060
Visually Difficult
5.00%Ambiguously Correct
7.50%Unusual Image
10.00%Unusual Text
10.75%Complex Reasoning
19.5%No Tag
43.0%LXMERT UNITER CLIP KEYCOMP GPT-3.5 (Ours) KeyCOMP GPT-4* (ours)Figure 3: Fine-grained text score performance across different question categories. We give specific examples from
each category in Appendix C. Percentages on the x-axis indicate each question type’s proportion of the dataset. To
ensure representative results, question categories comprising less than 5% of the dataset are excluded.
ious question categories curated by Diwan et al.
(2022). We provide correct examples from each
category in Appendix C, D, and the image score
results in Appendix E. Notably, keyword guidance
improves image description quality for generative
approaches. When providing high-quality image
descriptions, our method obtains significant gains
in the categories of unusual images (Fig. 17) and
complex reasoning (Fig. 9). For instance in Fig. 9,
when a VLM depicts a rabbit and a turtle correctly,
the LLM has the commonsense knowledge to pick
the faster one. When prompted with keywords (e.g.,
“people” and “windows”) in Fig. 17, the VLM is
more likely to describe an unusual crayon draw-
ing correctly. This suggests that KEYCOMP has
the potential to surpass embedding-based methods
when external knowledge is necessary for complex
reasoning or when precise image information from
keywords is required for unusual images.
Error Analysis and Findings We also manually
perform fine-grained error analysis on our method’s
outputs and illustrate three main categories of er-
rors in Appendix G. We identify a bottleneck of
image content understanding capability of VLMs
and highlight three key findings: 1) the VLM often
struggles to describe spatial relationships between
two objects, especially for two objects with similar
colors (e.g., bushes and hedge in Fig. 19); 2) the
LLM occasionally infers the wrong answer due to
the misinterpretation of a detailed VLM description
with a complicated syntax structure (see Fig. 20); 3)
the VLM produces inaccurate descriptions of scene
elements that are out of focus or missing parts of
objects (e.g., a man’s head is outside of Fig. 21).
Prompt and Model Ablations Table 2 presents
the effect of different prompts on our method. We
observe that guiding VLM output with keywords
results in the most significant improvement ( 2vs
5). Tuning the prompts for ChatGPT ( 3-5) pro-vides further gains, suggesting future improvement
with more sophisticated prompting. Prompting the
LLM with chain of thought instructions yields a
small gain of 1.3% text score and 2.3% image score
over a simpler selection prompt ( 5vs3). Using a
VLM to answer questions directly ( 1) leads to in-
ferior performance, with only 2.0 group score and
11.9% lower image score than KEYCOMP. We also
explore different VLM/LLM sizes in Appendix F.
ID Prompt Method Text Image Group
1 MiniGPT4 end-to-end 20.8±0.3 12 .5±1.7 2 .0±0.3
2 No Keyword Guidance 21.6±1.0 21 .8±0.8 9 .4±1.0
3 Multiple Choice 29.0±0.3 22 .3±0.4 11 .9±0.4
4 Explanation Prompting 26.3±1.3 23 .9±1.5 11 .5±0.6
5 KEYCOMP 30.3±1.6 24.6 ±1.2 12.4 ±1.2
6 Multiple Choice∗35.2±1.2 27 .5±0.7 14 .8±0.7
7 Explanation Prompting∗36.1±0.4 28 .8±0.6 15 .9±0.6
8 KEYCOMP∗42.7±0.8 27.8 ±0.7 17.4 ±0.3
Table 2: Average performance and standard deviation of
prompt variants. To isolate the effect of the prompt, the
bottom three rows use the same image description. We
show detailed prompt templates in Appendix B. Aster-
isk indicates an upper bound, where we select the best
caption among 5 generated MiniGPT4 descriptions.
5 Discussion and Future Works
We propose to use VLMs to describe key enti-
ties and relations in images to perform composi-
tional reasoning. We empirically demonstrate the
effectiveness of our method against embedding-
based approaches and end-to-end VLM methods
on Winoground. More importantly, we showcase
successful examples, categorize common errors
made by generative approaches, and identify a key
bottleneck of image content understanding of ex-
isting VLMs. We hope to shed insights into future
works of image content understanding regarding
(1) effective prompting strategies to guide VLMs to
focus on key image regions; (2) spatial reasoning
of objects by VLMs; and (3) accurate interpretation
of out-of-focus or partial objects by VLMs.
Limitations
Our method shows promising results over embed-
ding based methods and end-to-end VLMs. How-
ever, small errors in the early steps can accumulate
and lead to erroneous reasoning. Using VLMs in
a pipeline, as reported by (You et al., 2023), is of-
ten limited by their performance. This bottleneck
explains why we noticed significant improvement
when we manually selected the best descriptions
generated by MiniGPT4. Additionally, designing a
universal prompt for image reasoning can be chal-
lenging, and the optimal prompt may change with
model updates. Appendix B illustrates the effect
of changing prompts, and future work may con-
sider automatic prompt learning techniques such
as prefix tuning (Li and Liang, 2021). Lastly, our
method uses a non-deterministic reasoner, resulting
in slight output variations even with 0 temperature.
Although the variance is low, this may pose issues
for downstream tasks.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems , 35:23716–23736.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and
Devi Parikh. 2015. Vqa: Visual question answering.
InProceedings of the IEEE international conference
on computer vision , pages 2425–2433.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li,
and Mohamed Elhoseiny. 2023. Video chatcaptioner:
Towards the enriched spatiotemporal descriptions.
arXiv preprint arXiv:2304.04227 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed
El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. 2020. Uniter: Universal image-text
representation learning. In Computer Vision–ECCV2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part XXX , pages 104–
120. Springer.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE conference
on computer vision and pattern recognition , pages
248–255. Ieee.
Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath,
and Kyle Mahowald. 2022. Why is winoground
hard? investigating failures in visuolinguistic compo-
sitionality. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing .
Tanmay Gupta and Aniruddha Kembhavi. 2023. Vi-
sual programming: Compositional visual reasoning
without training. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition , pages 14953–14962.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen
Li, and Tom Duerig. 2021. Scaling up visual and
vision-language representation learning with noisy
text supervision. In International Conference on
Machine Learning , pages 4904–4916. PMLR.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. In International Conference on Ma-
chine Learning .
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang,
Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan
Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
2022. Grounded language-image pre-training. In
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 10965–
10975.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 2021 Conference of the Associa-
tion for Computational Linguistics .
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2023a. Improved baselines with visual instruc-
tion tuning.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023b. Visual instruction tuning.
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-
feng Gao. 2023. Chameleon: Plug-and-play compo-
sitional reasoning with large language models. arXiv
preprint arXiv:2304.09842 .
Oscar Mañas, Pau Rodriguez, Saba Ahmadi, Aida Ne-
matzadeh, Yash Goyal, and Aishwarya Agrawal.
2023. Mapl: Parameter-efficient adaptation of uni-
modal pre-trained models for vision-language few-
shot prompting. In Proceedings of the 17th Confer-
ence of the European Chapter of the Association for
Computational Linguistics .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan
Salakhutdinov, and Louis-Philippe Morency. 2022.
Cross-modal attention congruence regularization for
vision-language relation alignment. arXiv preprint
arXiv:2212.10549 .
Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2015. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. In Proceedings of the IEEE
international conference on computer vision , pages
2641–2649.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models
from natural language supervision. In International
Conference on Machine Learning , pages 8748–8763.
PMLR.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015. Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances
in neural information processing systems , 28.
Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men,
An Yang, Jingren Zhou, Xu Sun, and Hongxia Yang.
2021. Learning relation alignment for calibrated
cross-modal retrieval. In Proceedings of the Joint
Conference of the 59th Annual Meeting of the Associ-
ation for Computational Linguistics .
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580 .
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami,
Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. 2022. Flava: A founda-
tional language and vision alignment model. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15638–15650.Sanjay Subramanian, Medhini Narasimhan, Kushal
Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia
Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.
2023. Modular visual question answering via code
generation. In Proceedings of the 2023 Conference
of the Association for Computational Linguistics .
Dídac Surís, Sachit Menon, and Carl V ondrick. 2023.
Vipergpt: Visual inference via python execution for
reasoning. arXiv preprint arXiv:2303.08128 .
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet
Singh, Adina Williams, Douwe Kiela, and Candace
Ross. 2022. Winoground: Probing vision and lan-
guage models for visio-linguistic compositionality.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 5238–
5248.
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-
timodal few-shot learning with frozen language mod-
els. In Advances in Neural Information Processing
Systems , volume 34, pages 200–212.
Kohei Uehara, Nan Duan, and Tatsuya Harada. 2022.
Learning to ask informative sub-questions for visual
question answering. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition , pages 4681–4690.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai
Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren
Zhou, and Hongxia Yang. 2022a. Ofa: Unifying ar-
chitectures, tasks, and modalities through a simple
sequence-to-sequence learning framework. In Inter-
national Conference on Machine Learning , pages
23318–23340. PMLR.
Ruonan Wang, Yuxi Qian, Fangxiang Feng, Xiaojie
Wang, and Huixing Jiang. 2022b. Co-VQA : An-
swering by interactive sub question sequence. In
Findings of the Association for Computational Lin-
guistics: ACL 2022 , pages 2396–2408, Dublin, Ire-
land. Association for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. In Advances in neural information
processing systems .
Chenfei Wu, Shengming Yin, Weizhen Qi, Xi-
aodong Wang, Zecheng Tang, and Nan Duan.
2023. Visual chatgpt: Talking, drawing and edit-
ing with visual foundation models. arXiv preprint
arXiv:2303.04671 .
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-
react: Prompting chatgpt for multimodal reasoning
and action. arXiv preprint arXiv:2303.11381 .
Haoxuan You, Rui Sun, Zhecan Wang, Long Chen,
Gengyu Wang, Hammad A Ayyubi, Kai-Wei Chang,
and Shih-Fu Chang. 2023. Idealgpt: Iteratively de-
composing vision and language reasoning via large
language models. arXiv preprint arXiv:2305.14985 .
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .
Appendix
A Related Work
Vision Language Models Language Models and Vision Models by themselves have shown impressive
performance in their respective tasks, but combining them to perform vision-language reasoning remains
a challenging problem. Popular encoder-based VLMs such as CLIP (Radford et al., 2021) or ALIGN (Jia
et al., 2021) perform contrastive learning on large datasets of image-text pairs, displaying remarkable
zero-shot transfer to unseen tasks such as image classification (Deng et al., 2009) and image-text retreival
(Plummer et al., 2015). GLIP (Li et al., 2022) improves the pretraining stage of CLIP by introducing phrase
level grounding, allowing for richer semantic representations for the object detection task (Ren et al.,
2015). Recent sequence-to-sequence VLMs such as OFA (Wang et al., 2022a) and FLA V A (Singh et al.,
2022) pretrain on a larger variety of vision-language tasks and objectives to allow for more sample-efficient
pretraining and cross-task trasnfer.
Other recent works combine the information from different modalities by introducing lightweight
tunable parameters to connect frozen image and text encoders. Flamingo (Alayrac et al., 2022) freezes a
vision encoder and inserts trainable cross attention layers to a frozen LLM to integrate visual features,
achieving state of the art few-shot performance. BLIP-2 (Li et al., 2023) also freezes a vision encoder and
LLM, aligning the modalities with a lightweight transformer called the Q-former. Works such as Frozen
(Tsimpoukelli et al., 2021) and MAPL (Mañas et al., 2023) propose methods which freeze a LLM, but
train a visual encoder to represent images as continuous embeddings, resulting in a multimodal few-shot
learner. Inspired by the success of instruction tuning LLMs with human feedback with ChatGPT (Ouyang
et al., 2022), MiniGPT4 (Zhu et al., 2023) and LLaV A (Liu et al., 2023b,a) further improves the quality of
BLIP-2 outputs and LLaMA-2 by instruction tuning with additional descriptions generated by ChatGPT.
Large Language Models as Reasoners Concurrent work has explored the idea of using large lan-
guage models to connect visual foundation models. For example, HuggingGPT (Shen et al., 2023),
CHAMELEON (Lu et al., 2023), MM-React (Yang et al., 2023), and Visual ChatGPT (Wu et al., 2023)
utilize ChatGPT as a controller, allowing it to delegate visually demanding tasks to foundational visual
models. IdealGPT (You et al., 2023) proposes an iterative approach to decompose complex visual tasks
into a series of sub-questions and answers. ChatCaptioner (Chen et al., 2023) has BLIP-2 interact conver-
sationally with ChatGPT to create more informative image descriptions. ViperGPT (Surís et al., 2023),
VisProg, (Gupta and Kembhavi, 2023), and CodeVQA (Subramanian et al., 2023) use code LLMs such
as CodeX (Chen et al., 2021) to write python programs which invoke VLMs to solve challenging visual
tasks without any task-specific training. Different from these works, we replace the ChatGPT controller
with keyword-guided detailed descriptions of the contents of an image, delegating reasoning to a more
powerful LLM instead of a VLM. We also avoid iteratively prompting our models to avoid potential error
accumulation.
B Prompt Variations
In Tables 3 and 4 we display all the different prompt variations used in our experiments.
Name VLM Prompt ( PK) ChatGPT Prompt ( Ptxt,a) Performance
KEYCOMPDescribe the image in detail
using these keywords: {keywords}Select the best caption for this image:
A:{caption_0}
B:{caption_1}
Think step-by-step and start your answer with A or B.
Even if you are unsure make a guess.30.3
Explanation PromptingDescribe the image in detail
using these keywords: {keywords}Select the best caption for this image:
A:{caption_0}
B:{caption_1}
Start your answer with A or B.
Even if you are unsure make a guess.
Briefly explain your decision in 1-2 sentences.26.3
Multiple ChoiceDescribe the image in detail
using these keywords: {keywords}Select the best caption for this image:
A:{caption_0}
B:{caption_1}
Start your answer with A or B.
Even if you are unsure make a guess.29.0
No Keyword Guidance Describe the image in detailSelect the best caption for this image:
A:{caption_0}
B:{caption_1}
Start your answer with A or B.
Even if you are unsure make a guess.
Briefly explain your decision in 1-2 sentences.21.6
MiniGPT4 End-to-EndWhich caption is more appropriate:
A: {caption_0 }
B: {caption_1 }
Answer in one sentence.
Even if you are unsure make a guess.
Briefly explain your decision in 1-2 sentences.None 20.8
Table 3: Average performance of different prompt variants for text score across 3 different runs.
Name VLM Prompt ( PK) ChatGPT Prompt ( Pimg,a) Performance
KEYCOMPDescribe the image in detail
using these keywords: { keywords }Statement: { caption }
Image A: { description_0 }
Image B: { description_1 }
Think step by step and fill in the blank:
Situation {{}} is most consistent with the statement { caption } because ...24.6
Explanation PromptingDescribe the image in detail
using these keywords: { keywords }Statement: { caption }
Image A: { description_0 }
Image B: { description_1 }
Fill in the blank:
Situation {{}} is most consistent with the statement { caption } because ...
Explain your decision in 1-2 sentences.23.9
Multiple ChoiceDescribe the image in detail
using these keywords: { keywords }Image 1: { image_0_description }
Image 2: { image_1_description }
Consider the caption: { caption }
Select the better image for this caption:
A: {image_1 }
B: {image_2 }
Start your answer with A or B.22.3
No Keyword Guidance Describe the image in detailStatement: { caption }
Image A: { description_0 }
Image B: { description_1 }
Think step by step and fill in the blank:
Situation {{}} is most consistent with the statement { caption } because ...21.8
MiniGPT4 End-to-EndGiven the following images:
A:<Img>ImageContent</Img>
B:<Img>ImageContent</Img>
Which image is more appropriate
for the caption {caption} ?
Answer with A or B.None 12.5
Table 4: Average performance of different prompt variants for image score across 3 separate runs.
C Text Score Task for Question Categories
We compile several examples of our method, with one example from each question category defined by a
recent study (Diwan et al., 2022).
Text Score T ask
A: Fire T ruck
B: Truck FireKeyword Detection
{'Fire', 'T ruck'}Description
The image shows a red fire truck
driving down a cit y street. The
truck has a long ladder on its roof
and a large w ater tank on its
back. The truck is equipped with
various tools and equipment that
firefighters use to fight fires.Prompt-Based Reasoning
Select the best caption for the image
A: Fire T ruck
B: Truck Fire
Think step b y step and start y our answer with A or B .
Decision
A: "It's a fire truck" is the best caption for this image
as it accur ately describes the v ehicle shown in the
picture. Option B , "It's a truck fire," is not an accur ate
description of the image and could be interpreted as
the truck itself being on fire
Figure 4: Non-Compositional Question. The swapped words (“Fire” and “Truck”) do not necessarily contain the
same semantic entities, so compositional reasoning may not be required to solve the question.
Text Score T ask
A: A dog sitting on a couch with a
person lying on the floor
B: A person lying on a couch with
a dog sitting on the floorKeyword Detection
{'lying', 'dog', 'person',
'couch', 'floor'}Description
The image shows a woman sitting on
a couch with a dog lying next to her
on the floor . The woman is holding a
cup of coffee and appears to be
relaxing in her living room.Prompt-Based Reasoning
Select the best caption for the image
A: A dog sitting on a couch with a
person lying on the floor
B: A person lying on a couch with a
dog sitting on the floor
Think step b y step and start y our
answer with A or B .
Decision
B: "A person lying on a couch with a dog sitting on the
floor." The caption accur ately describes the image and
correctly identifies the woman as sitting on the couch
rather than lying on the floor .
Figure 5: Ambiguously Correct Question. Note that the correct caption B describes the woman as lying on the
couch when she is sitting, but the LLM is still able to pick the ambiguously correct caption.
Text Score T ask
A: The person in the middle is
floating and some others are standing
B: The person in the middle is
standing and some others are floatingKeyword Detection
{'person', 'middle', 'floating',
'standing', 'others'}Description
The image shows a woman in a black
swimsuit standing in the w ater with her
arms outstretched. She is surrounded b y
other people in swimsuits who are
floating on their backs in the w ater. The
sky is clear and blue, and there are
mountains in the distance.Prompt-Based Reasoning
Select the best caption for the image
A: The person in the middle is floating and
some others are standing
B: The person in the middle is standing and
some others are floating
Think step b y step and start y our answer
with A or B .
Decision
B: "The person in the middle is standing and some
others are floating." The image clearly shows the woman
in the center standing with her arms outstretched while
others float on their backs around her .
Figure 6: Visually Difficult Question. To correctly answer, the model needs to identify the person floating on the
right.
Text Score T ask
A: There are three
people and two windows
B: There are three
windows and two peopleKeyword Detection
{'three', 'two', 'windows', 'people}Description
The image shows a house with three
people standing in front of it. The house
has two windows on the side and a door
in the front. The roof is sloped and there
is a chimney on top of the house. The
image is dr awn with colored pencils on a
white paper .Prompt-Based Reasoning
Select the best caption for the image
A: There are three people and two windows
B: There are three windows and two people
Think step b y step and start y our answer with A or B .
Decision
A: "There are three people and two windows" is the best
caption for this image because it accur ately describes the
number of people and windows in the image. Caption B
describes the opposite of what is shown in the image.
Figure 7: Unusual Image Question. The VLM is more likely to describe an unusual crayon drawing correctly, based
on the detected keywords “windows” and “people” in the prompt, while embedding-based methods are not feasible
to include any hints, and are sensitive to domain shifts in the image distribution.
Text Score T ask
A: Some green on the ball
B: The ball on some greenKeyword Detection
{'green', 'ball', 'on'}Description
The image shows a white golf ball sitting
on the green gr ass of a golf course. The
background is a lush, green forest with
tall trees in the distance. Prompt-Based Reasoning
Select the best caption for the image
A: Some green on the ball
B: The ball on some green
Think step b y step and start y our answer with A or B .
Decision
B: "The ball on some green". This caption accur ately
describes the main focus of the image - the golf ball
sitting on the green gr ass of a golf course.
Figure 8: Unusual Text Question. The caption “The ball on some green" is considered less common compared to an
alternative such as “the ball on some grass".
Text Score T ask
A: The green one is fast and the
one in white is slow
B: The green one is slow and the
one in white is fastKeyword Detection
{'green', 'white', 'slow', 'fast}Description
The image shows a white r abbit
and a turtle standing next to
each other on a white surface.
The turtle's shell is green, while
the rabbit's fur is white.Prompt-Based Reasoning
Select the best caption for the image
A: The green one is fast and the one in white is slow
B: The green one is slow and the one in white is fast
Think step b y step and start y our answer with A or
B.
Decision
B: The image shows a turtle and a r abbit standing next
to each other . In gener al, turtles are slower than
rabbits, so the caption "the green one is slow and the
one in white is compar atively fast" mak es more sense.
Figure 9: Complex Reasoning Question. This requires the knowledge that rabbits are typically faster than turtles.
Text Score T ask
A: A mug in some gr ass
B: Some gr ass in a mugKeyword Detection
{'mug', 'gr ass', 'in'}Description
There is a mug sitting on a table
with a small plant growing out of it.
The mug has a cartoon char acter on
it and the plant is made up of small
blades of gr ass.Prompt-Based Reasoning
Select the best caption for the image
A: A mug in some gr ass
B: Some gr ass in a mug
Think step b y step and start y our answer with A or B .
Decision
B: This caption accur ately describes the main focus of the
image, which is a small plant growing out of a mug.
Additionally , the caption pro vides additional details about
the plant being made up of small blades of gr ass
Figure 10: No Tag Question.
D Image Score Task for Question Categories
In this section we illustrate the details of our method when applied to the image score task. We include
one example from each question category, using the same image caption pairs used in Appendix C.
Image Score T ask
Which image corresponds to the
caption:  "It's a truck fire" ?Keyword Detection
{'fire', 'truck'}Description
A:The image shows a red fire truck driving
down a cit y street. The truck has a long
ladder on its roof and a large w ater tank on
its back. The truck is equipped with v arious
tools and equipment that firefighters use to
fight fires.
B: The image shows a truck on fire in the
middle of a gr assy field. The truck is
completely engulfed in flames, with smok e
billowing out of the windows and the hood. 
Decision
B: Situation B is most consistent with the
statement it's a truck fire because the image
shows a truck that is completely engulfed in
flames, with smok e billowing out of the
windows and the hood. This aligns with the
description of a truck on fire.Prompt-Based Reasoning
Statement : "It's a truck fire "
Image A: The image shows a red fire truck driving
down a cit y street. The truck  has a long ladder on
its roof and a large w ater tank on its back ...
Image B: The image shows a truck on fire in the
middle of a gr assy field. The truck is completely
engulfed in flames, with smok e billowing out of the
windows and the hood. 
Fill in the blank : Situation {{}} is most consistent
with the statement "It's a truck fire" because ...
A
B
Figure 11: Non-Compositional Question for the image score task.
Image Score T ask
Which image corresponds to the
caption:  "A dog sitting on a couch
with a person lying on the floor" ?Keyword Detection
{'lying', 'dog', 'person',
'couch', 'floor'}Description
A: The image shows a y oung girl sitting
on the floor next to a large black dog
that is lying on a couch.
B: The image shows a woman sitting on
a couch with a dog lying next to her on
the floor . The woman is holding a cup of
coffee and appears to be relaxing in her
living room.
Decision
A: Situation A is most consistent with the
statement "a dog sitting on a couch with a person
lying on the floor". In the image, there is a dog
sitting on a couch and a person (y oung girl) lying
on the floor . The position of the dog and the
person align with the statement.
A
BPrompt-Based Reasoning
Statement : "A dog sitting on a couch with a person
lying on the floor "
Image A: The image shows a y oung girl sitting on
the floor next to a large black dog that is lying on a
couch.
Image B: The image shows a woman sitting on a
couch with a dog lying next to her on the floor . The
woman is holding a cup of coffee and appears to be
relaxing in her living room.
Fill in the blank : Situation {{}} is most consistent
with the statement "A dog sitting on a couch with a
person lying on the floor" because ...
Figure 12: Ambiguously Correct Question. Refer to Appendix C for the ambiguity contained in image B.
Image Score T ask
Keyword Detection
{'person', 'middle', 'floating',
'standing', 'others'}Description
A: A group of women in swimsuits are floating
on inflatable tubes in the middle of a large
body of w ater. They are all smiling and ha ving
a good time. In the background, there are
mountains and a clear blue sky .
B: The image shows a woman in a black
swimsuit standing in the w ater with her arms
outstretched. She is surrounded b y other
people in swimsuits who are floating on their
backs in the w ater. The sky is clear and blue,
and there are mountains in the distance.
Decision
B: Situation B is most consistent with the
statement "the person in the middle is
standing and some others are floating"
because it describes a woman in a black
swimsuit standing in the w ater with her arms
outstretched. It also mentions other people in
swimsuits floating on their backs in the w ater. Prompt-Based Reasoning
Statement : "The person in the middle is standing and
some others are floating "
Image A: A group of women in swimsuits are floating on
inflatable tubes in the middle of a large body of
water. They are all smiling and ha ving a good time ...
Image B: The image shows a woman in a black swimsuit
standing in the w ater with her arms outstretched. She is
surrounded b y other people in swimsuits who are floating
on their backs in the w ater.
Fill in the blank : Situation {{}} is most consistent with
the statement " The person in the middle is standing and
some others are floating " because ...
A
B
Which image corresponds to the
caption:  "The person in the
middle is standing and some
others are floating "?Figure 13: Visually Difficult Question for the image score task.
Image Score T ask
Keyword Detection
{'three', 'two', 'windows', 'people}Description
A: The image shows a house with three people
standing in front of it. The house has two
windows on the side and a door in the front. The
roof is sloped and there is a chimney on top of
the house. The image is dr awn with colored
pencils on a white paper .
B: The image shows a child's dr awing of a
house with three windows and two people
standing in front of it. There is a tree in the
front y ard and the sun is shining in the
backgroundPrompt-Based Reasoning
Statement : "There are three windows and two people "
Image A: The image shows a house with three people
standing in front of it. The house has two windows on the
side and a door in the front. The roof is sloped and there is
a chimney on top of the house. The image is dr awn with
colored pencils on a white paper .
Image B: The image shows a child's dr awing of a house
with three windows and two people standing in front of it.
There is a tree in the front y ard and the sun is shining in
the background
Fill in the blank : Situation {{}} is most consistent with
the statement "There are three windows and two people"
because ...
Which image corresponds to the
caption: "There are three people and
two windows "?Decision
A: Situation A is most consistent with the statement,
"there are three people and two windows" because in
Image A, there are indeed three people standing in
front of the house, as stated in the statement.
Additionally , Image A specifically mentions that there
are two windows on the side of the house,
corrobor ating the statement's claim of there being
two windows.
A
B
Figure 14: Unusual Image Question. Although both images are cartoons, both descriptions match the contents of
the images accurately.
Image Score T ask
Keyword Detection
{'green', 'ball', 'on'}Description
A: The image shows a person wearing
black pants and a red shirt, holding a
green bowling ball on a wooden
bowling alley .
B: The image shows a white golf ball
sitting on the green gr ass of a golf
course. The background is a lush,
green forest with tall trees in the
distance. 
Decision
A: Situation A is most consistent with the
statement "some green on the ball"
because the image describes a person
holding a green bowling ball. This implies
that there is some green color present on
the ball.Prompt-Based Reasoning
Statement : "Some green on the ball "
Image A: The image shows a person wearing
black pants and a red shirt, holding a green
bowling ball on a wooden bowling alley ..
Image B: The image shows a white golf ball
sitting on the green gr ass of a golf course. The
background is a lush, green forest with tall
trees in the distance. 
Fill in the blank : Situation {{}} is most
consistent with the statement "Some green on
the ball" because ...
Which image corresponds to the
caption:  "Some green on the ball "?
B
AFigure 15: Unusual Text question for the image score task.
Image Score T ask
Keyword Detection
{'green', 'white', 'slow', 'fast}Description
A: The image shows a man standing next
to a green muscle car . He is wearing a
white tank top and has his arms crossed in
front of him. The car is a classic muscle car
from the 1960s or 1970s, with a large V8
engine and distinctiv e body st yling.
B: The image shows a white r abbit and a
turtle standing next to each other on a
white surface. The turtle's shell is green,
while the r abbit's fur is white.
Decision
A: Situation A is most consistent with the statement
"the green one is fast and the one in white is
compar atively slow" because in the image, the green
muscle car is t ypically associated with speed and
high performance. On the other hand, the man in the
image is wearing a white tank top , which represents
the color mentioned in the statement as being
compar atively slow .Prompt-Based Reasoning
Statement : "The green one is fast and the one in
white is compar atively slow "
Image A: The image shows a man standing next to a
green muscle car . He is wearing a white tank top and
has his arms crossed in front of him ...
Image B: The image shows a white r abbit and a turtle
standing next to each other on a white surface. The
turtle's shell is green, while the r abbit's fur is white.
Fill in the blank : Situation {{}} is most consistent
with the statement "The green one is fast and the one
in white is compar atively slow" because ...
Which image corresponds to the caption:
"The green one is fast and the one in
white is comparatively slow "?
A
B
Figure 16: Complex Reasoning question for the image score task.
Image Score T ask
Keyword Detection
{'mug', 'grass', 'in'}Description
A: The image shows a white mug sitting on
a patch of green gr ass in a park. The mug
appears to be made of porcelain and has a
handle on one side and a spout on the
other. The gr ass is tall and lush, with blades
of green that sw ay gently in the breez e.
B: There is a mug sitting on a table with a
small plant growing out of it. The mug has
a cartoon char acter on it and the plant is
made up of small blades of gr ass.
Decision
A: Situation A is most consistent with the
statement "there is a mug in some gr ass"
because the image shows a mug placed on a
patch of green gr ass in a park. The mug is
made of porcelain, has a handle and a spout,
and it is clearly depicted in the image.Prompt-Based Reasoning
Statement : "There is a mug in some gr ass"
Image A: The image shows a white mug sitting
on a patch of green gr ass in a park. The mug
appears to be made of porcelain and has a handle
on one side and a spout on the other . The gr ass is 
...
Image B: There is a mug sitting on a table with a
small plant growing out of it. The mug has a
cartoon char acter on it and the plant is made up of
small blades of gr ass.
Fill in the blank : Situation {{}} is most
consistent with the statement "There is a mug in
some gr ass" because ...
Which image corresponds to the caption:
"There is a mug in some grass "?
B
A
Figure 17: NoTag question for the image score task.
E Fine Grained Image Scores
0102030
Visually Difficult
5.00%Ambiguously Correct
7.50%Unusual Image
10.00%Unusual Text
10.75%Complex Reasoning
19.5%No Tag
43.0%LXMERT CLIP UNITER KEYCOMP (Ours)
Figure 18: Fine-grained image score performance across different question categories. We give specific examples
from each category in Appendix C. Percentages on the x-axis indicate each question type’s proportion of the dataset.
To ensure representative results, question categories comprising less than 5% of the dataset are excluded.
F Effect of VLM Size and LLM Size
VLM Text Image Group
MiniGPT-4 7b 23.3 25.5 0.1
MiniGPT-4 13b 30.3 24.6 12.4
Table 5: K EYCOMP performance while changing the size of the VLM image captioner.
LLM Text Image Group
GPT-3.5 30.3 24.6 12.4
GPT-4 44.0 25.9 15.4
Table 6: K EYCOMP performance while changing the size of the LLM used for reasoning.
Tables 5 and 6 show the effect of changing the VLM and LLM sizes respectively. Our results suggest
that changing the LLM size improves text score significantly and offers marginal improvement in image
score. This intuitively makes sense because text score requires a deeper understanding of text to distinguish
between similar captions. Upgrading the VLM also provides improvement to text score, but does not
impact the image score.
G Error Analysis
In this section, we detail what examples KEYCOMP struggles with and suggest future directions for
generative approaches. We categorize common errors as either VLM-based, LLM-based or both.
G.1 Spatial Reasoning (VLM)
For questions requiring spatial reasoning, we note that KEYCOMP sometimes produces inaccurate
descriptions of the scene. We believe this stems from the image captioning model (VLM) and may be
addressed by utilizing scene understanding models trained with object-relation level supervision.
VLM Prompt : Describe the image using these
keywords: {"behind", "bushes", "point y",
"rectangular"}
VLM Output:  The image shows a rectangular
hedge with point y bushes behind it.VLM Prompt : Describe the image using these
keywords: {"behind", "bushes", "point y",
"rectangular"}
VLM Output : The image shows a row of point y
bushes growing behind a rectangular hedge.
Figure 19: Spatial Reasoning Error. As the VLM does not explicitly model the spatial information of objects in an
image, the VLM mistakenly thinks the bushes are behind the hedges in the right image. This is particularly difficult
when two objects have similar colors.
G.2 Incorrect LLM Reasoning
Occasionally the LLM is presented with VLM descriptions that should contain enough information to
answer the question, but still selects the wrong answer. This often occurs when the VLM description has
a complicated syntax structure that involves multiple objects.
Which caption is better for this image? 
A: Circular food on heart -shaped wood
B: Heart -shaped food on circular wood
VLM Prompt : Describe the image using these
keywords: {"circular", "wood", "food", "heart"}
VLM Output:  The image shows a woman in an apron
preparing a heart -shaped dish made of circular slices of
food on a wooden table in front of a fireplace.
LLM Output: B: "heart -shaped food on circular wood"
sounds lik e the more appropriate caption because the
focus of the image is on the heart -shaped dish made of
circular slices of food on a circular wooden table.
Figure 20: LLM Interpretation Error. Although VLMs provide a sufficiently detailed description, the description
has a complicated syntax structure that involves multiple objects. The LLM response still does not match the VLM
description.
G.3 Describing Out of Focus or Occluded Objects (VLM)
Sometimes the VLM produces inaccurate descriptions of scene elements which are out of focus or missing
parts of objects. In these cases, even though the prompt requires the LLM to choose an option, the LLM
may sometimes refuse to choose, saying it is uncertain and offers a better caption. This makes it hard
to automate the evaluation and may be addressed with further fine-tuning in this instruction format or
additional prompt engineering.
Which caption is better for this image? 
A: The person closer to the camer a weightlights and the
person farther from the camer a runs
B: The person closer to the camer a runs and the person
farther from the camer a weightlifts
VLM Prompt : Describe the image using these
keywords:  {"person", "farther", "camer a", "weightlift" "run"}
VLM Output:  The image shows a man and a woman
running on a treadmill in a gym. The man is wearing a
blue shirt and black shorts, while the woman is wearing
a black shirt and blue shorts. They are both wearing
athletic shoes and headphones.
LLM Output: Neither A nor B would be an accur ate
caption for the described image. Both individuals
described are running on treadmills, not weightlifting. A
more accur ate caption could be, "A man and woman
exercising on treadmills in a busy gym".
Figure 21: LLM does not give an answer. In this case, the VLM description also does not describe the scene
accurately, likely because the man is out of focus and his head is not visible.
