from dataclasses import dataclass, field
from typing import List, Dict

@dataclass
class WeekDetail:
    week: str = ""
    files: List[str] = field(default_factory=list)
    tech_stack: List[str] = field(default_factory=list)
    learning_paragraphs: List[str] = field(default_factory=list)
    code_examples: Dict[str, str] = field(default_factory=dict)


def get_detail() -> WeekDetail:
    """05.5ì£¼ì°¨_ProductEngineer_PromptEngineering: ìƒì„¸ í•™ìŠµ ê¸°ë¡ (í•œêµ­ì–´)
    
    LLM í™œìš©, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, RAG, ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜
    """

    w = WeekDetail(week="05.5ì£¼ì°¨_ProductEngineer_PromptEngineering")

    w.files = [
        "00.ê°•ì˜ìë£Œ/LLM_ê¸°ì´ˆ_ë°_API.pdf",
        "00.ê°•ì˜ìë£Œ/í”„ë¡¬í”„íŠ¸_ì—”ì§€ë‹ˆì–´ë§_ê³ ê¸‰.pdf",
        "01.daily_mission/Day1_LLM_API_ê¸°ì´ˆ.ipynb",
        "01.daily_mission/Day2_í”„ë¡¬í”„íŠ¸_êµ¬ì¡°í™”.ipynb",
        "01.daily_mission/Day3_RAG_ì‹œìŠ¤í…œ.ipynb",
        "02.advanced_mission/Day4_ì—ì´ì „íŠ¸_ì•„í‚¤í…ì²˜.ipynb",
        "02.advanced_mission/Day5_í†µí•©_í”„ë¡œì íŠ¸.ipynb",
    ]

    w.tech_stack = [
        "LLM API: OpenAI GPT-4, Claude, Gemini",
        "ë¼ì´ë¸ŒëŸ¬ë¦¬: LangChain, LlamaIndex, Anthropic SDK",
        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤: Pinecone, Weaviate, Chroma",
        "ì„ë² ë”© ëª¨ë¸: OpenAI Embeddings, Hugging Face",
        "í”„ë¡¬í”„íŠ¸ íŒ¨í„´: Role/Instruction/Example/Chain-of-Thought",
        "ì—ì´ì „íŠ¸ íŒ¨í„´: ReAct, Tool Use, Memory Management",
        "ëª¨ë‹ˆí„°ë§: LangSmith, Weights & Biases",
    ]

    w.learning_paragraphs = [
        (
            "ğŸ“… Day 1: LLM APIì™€ ê¸°ë³¸ êµ¬ì¡°\n"
            "- OpenAI API êµ¬ì¡° ë° ëª¨ë¸ ì„ íƒ (gpt-4, gpt-3.5-turbo)\n"
            "- Chat Completion vs Text Completion\n"
            "- í† í° ê³„ì‚° ë° ë¹„ìš© ì¶”ì •\n"
            "- API ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§\n"
            "- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"
        ),

        (
            "ğŸ“… Day 2: ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\n"
            "- í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ìµœì í™” (System/User/Assistant roles)\n"
            "- Few-shot learning íŒ¨í„´\n"
            "- Chain-of-Thought í”„ë¡¬í”„íŒ…\n"
            "- Temperatureì™€ Top-P íŒŒë¼ë¯¸í„° íŠœë‹\n"
            "- í”„ë¡¬í”„íŠ¸ ë²„ì „ ê´€ë¦¬ ë° A/B í…ŒìŠ¤íŠ¸"
        ),

        (
            "ğŸ“… Day 3: RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ\n"
            "- ë²¡í„° ì„ë² ë”©ì˜ ê°œë…\n"
            "- ë¬¸ì„œ ì²­í‚¹ ë° ì „ì²˜ë¦¬\n"
            "- ìœ ì‚¬ë„ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜\n"
            "- ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMê³¼ ê²°í•©\n"
            "- Pinecone/Weaviate í†µí•©"
        ),

        (
            "ğŸ“… Day 4: ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ ì„¤ê³„\n"
            "- ì—ì´ì „íŠ¸ì˜ ê°œë… (ëª©í‘œ, ë„êµ¬, ë©”ëª¨ë¦¬)\n"
            "- Tool/Function Calling êµ¬í˜„\n"
            "- ReAct íŒ¨í„´: Reasoning + Acting\n"
            "- ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬\n"
            "- ë°˜ë³µ ì œí•œ ë° ì—ëŸ¬ ë³µêµ¬"
        ),

        (
            "ğŸ“… Day 5: í†µí•© í”„ë¡œì íŠ¸ ë° ë°°í¬\n"
            "- ì „ì²´ AI íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n"
            "- ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ í†µí•©\n"
            "- ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì¶”ì \n"
            "- ë¹„ìš© ìµœì í™” ì „ëµ\n"
            "- í”„ë¡œë•ì…˜ ë°°í¬ ë° ìŠ¤ì¼€ì¼ë§"
        ),
    ]

    w.code_examples = {}

    w.code_examples['01_basic_llm_api.py'] = '''# Day 1: OpenAI API ê¸°ì´ˆ

import openai
import os
from typing import Optional

# API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

def basic_completion(prompt: str, model: str = "gpt-4") -> str:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±"""
    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=500
    )
    return response['choices'][0]['message']['content']

def streaming_completion(prompt: str) -> None:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì‹¤ì‹œê°„ ì¶œë ¥)"""
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        stream=True
    )
    
    for chunk in response:
        if 'choices' in chunk:
            delta = chunk['choices'][0]['delta']
            if 'content' in delta:
                print(delta['content'], end='', flush=True)
    print()

def count_tokens(text: str) -> int:
    """í† í° ìˆ˜ ê³„ì‚° (ëŒ€ëµì )"""
    import tiktoken
    encoding = tiktoken.get_encoding("cl100k_base")
    tokens = encoding.encode(text)
    return len(tokens)

def estimate_cost(prompt_tokens: int, completion_tokens: int) -> float:
    """API ë¹„ìš© ì¶”ì •"""
    # GPT-4 ê°€ê²© (2024 ê¸°ì¤€)
    prompt_price = 0.00003  # $0.03 / 1K tokens
    completion_price = 0.00006  # $0.06 / 1K tokens
    
    cost = (prompt_tokens * prompt_price + completion_tokens * completion_price)
    return cost

def api_with_retry(prompt: str, max_retries: int = 3) -> Optional[str]:
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” API í˜¸ì¶œ"""
    import time
    
    for attempt in range(max_retries):
        try:
            return basic_completion(prompt)
        except openai.error.RateLimitError:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt
            print(f"Rate limit. {wait_time}ì´ˆ ëŒ€ê¸°...")
            time.sleep(wait_time)
        except openai.error.APIError as e:
            if attempt == max_retries - 1:
                raise
            print(f"API ì˜¤ë¥˜: {e}. ì¬ì‹œë„...")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == '__main__':
    prompt = "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    
    print("=== ê¸°ë³¸ ì™„ì„± ===")
    result = basic_completion(prompt)
    print(result)
    
    print("\\n=== ìŠ¤íŠ¸ë¦¬ë° ===")
    streaming_completion(prompt)
    
    print("\\n=== í† í° ìˆ˜ ===")
    tokens = count_tokens(prompt)
    print(f"í”„ë¡¬í”„íŠ¸ í† í°: {tokens}")
'''

    w.code_examples['02_prompt_engineering.py'] = '''# Day 2: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê³ ê¸‰

import openai
from typing import List, Dict

class PromptEngineer:
    """í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = "gpt-4"
        self.temperature = 0.7
    
    def system_prompt(self, role: str) -> Dict:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜"""
        return {"role": "system", "content": role}
    
    def few_shot_example(self, examples: List[Dict]) -> str:
        """Few-shot ì˜ˆì œ ìƒì„±"""
        prompt = "ë‹¤ìŒì€ ì˜ˆì œì…ë‹ˆë‹¤:\\n"
        for i, example in enumerate(examples, 1):
            prompt += f"ì˜ˆì œ {i}:\\n"
            prompt += f"ì…ë ¥: {example['input']}\\n"
            prompt += f"ì¶œë ¥: {example['output']}\\n\\n"
        return prompt
    
    def chain_of_thought(self, problem: str) -> str:
        """Chain-of-Thought í”„ë¡¬í”„íŒ…"""
        return f"""ë‹¤ìŒ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”.
ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ê³ , ìµœì¢… ë‹µë³€ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ë¬¸ì œ: {problem}

ë‹¨ê³„:
1. ë¬¸ì œ ì´í•´
2. í•„ìš”í•œ ì •ë³´ íŒŒì•…
3. ë‹¨ê³„ë³„ í’€ì´
4. ìµœì¢… ë‹µë³€"""
    
    def structured_output(self, task: str, format_type: str = "json") -> str:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ ìš”ì²­"""
        if format_type == "json":
            return f"""{task}

ì‘ë‹µì€ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "answer": "ë‹µë³€",
    "confidence": 0.0-1.0,
    "reasoning": "ì´ìœ "
}}"""
        elif format_type == "markdown":
            return f"""{task}

ì‘ë‹µì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
# ì œëª©
## ë¶€ì œ
- í•­ëª© 1
- í•­ëª© 2"""
    
    def multilingual_prompt(self, text: str, target_lang: str) -> str:
        """ë‹¤êµ­ì–´ í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_lang}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
ì›ë¬¸ì˜ ì˜ë¯¸ì™€ í†¤ì„ ìµœëŒ€í•œ ë³´ì¡´í•´ì£¼ì„¸ìš”.

ì›ë¬¸:
{text}

ë²ˆì—­:"""
    
    def prompt_variations(self, base_prompt: str, temperatures: List[float]) -> List[str]:
        """ë‹¤ì–‘í•œ ì˜¨ë„ ì„¤ì •ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        variations = []
        for temp in temperatures:
            self.temperature = temp
            # API í˜¸ì¶œ ë¡œì§
            variations.append(f"ì˜¨ë„ {temp}: [ì‘ë‹µ]")
        return variations

# ì‚¬ìš© ì˜ˆì œ
engineer = PromptEngineer()

# Few-shot í•™ìŠµ
examples = [
    {"input": "Cat", "output": "ë™ë¬¼"},
    {"input": "Apple", "output": "ê³¼ì¼"}
]
few_shot_prompt = engineer.few_shot_example(examples)

# Chain-of-Thought
cot_prompt = engineer.chain_of_thought("12 Ã— 5 + 8 Ã· 2 = ?")

# êµ¬ì¡°í™”ëœ ì¶œë ¥
structured = engineer.structured_output(
    "ê°ì • ë¶„ì„: 'ì´ ì œí’ˆì€ ì •ë§ í›Œë¥­í•©ë‹ˆë‹¤!'",
    format_type="json"
)

# ë²ˆì—­
translation = engineer.multilingual_prompt(
    "The quick brown fox jumps over the lazy dog",
    "í•œêµ­ì–´"
)

print("Few-shot í”„ë¡¬í”„íŠ¸:")
print(few_shot_prompt)
print("\\nChain-of-Thought:")
print(cot_prompt)
'''

    w.code_examples['03_rag_pipeline.py'] = '''# Day 3: RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ

from typing import List, Dict, Tuple
import numpy as np
from dataclasses import dataclass

@dataclass
class Document:
    """ë¬¸ì„œ ê°ì²´"""
    id: str
    content: str
    embedding: List[float] = None
    metadata: Dict = None

class RAGPipeline:
    """RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, embedding_model="text-embedding-3-small"):
        self.embedding_model = embedding_model
        self.documents: List[Document] = []
        self.embeddings: np.ndarray = None
    
    def chunk_document(self, text: str, chunk_size: int = 500) -> List[str]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i:i + chunk_size])
        return chunks
    
    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        import openai
        
        response = openai.Embedding.create(
            input=text,
            model=self.embedding_model
        )
        return response['data'][0]['embedding']
    
    def add_documents(self, documents: List[Dict]):
        """ë¬¸ì„œ ì¶”ê°€"""
        for doc in documents:
            chunks = self.chunk_document(doc['content'])
            
            for i, chunk in enumerate(chunks):
                embedding = self.embed_text(chunk)
                
                doc_obj = Document(
                    id=f"{doc['id']}_chunk_{i}",
                    content=chunk,
                    embedding=embedding,
                    metadata={"source": doc.get('source', '')}
                )
                self.documents.append(doc_obj)
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        magnitude1 = np.linalg.norm(vec1)
        magnitude2 = np.linalg.norm(vec2)
        
        return dot_product / (magnitude1 * magnitude2)
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Document]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.embed_text(query)
        
        similarities = []
        for doc in self.documents:
            similarity = self.cosine_similarity(query_embedding, doc.embedding)
            similarities.append((doc, similarity))
        
        # ìƒìœ„ Kê°œ ë°˜í™˜
        similarities.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in similarities[:top_k]]
    
    def generate_with_context(self, query: str, llm_model: str = "gpt-4") -> str:
        """RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        import openai
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.retrieve(query)
        
        # ë¬¸ë§¥ ìƒì„±
        context = "\\n".join([doc.content for doc in relevant_docs])
        
        # LLMì— ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì§ˆë¬¸
        response = openai.ChatCompletion.create(
            model=llm_model,
            messages=[
                {
                    "role": "system",
                    "content": f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}"""
                },
                {"role": "user", "content": query}
            ]
        )
        
        return response['choices'][0]['message']['content']

# ì‚¬ìš© ì˜ˆì œ
rag = RAGPipeline()

# ë¬¸ì„œ ì¶”ê°€
documents = [
    {
        "id": "doc1",
        "content": "íŒŒì´ì¬ì€ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ë°°ìš°ê¸° ì‰½ê³  ê°•ë ¥í•©ë‹ˆë‹¤.",
        "source": "python_guide"
    },
    {
        "id": "doc2",
        "content": "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
        "source": "ml_guide"
    }
]

# rag.add_documents(documents)
# answer = rag.generate_with_context("íŒŒì´ì¬ì˜ íŠ¹ì§•ì€?")
# print(f"ë‹µë³€: {answer}")
'''

    w.code_examples['04_agent_architecture.py'] = '''# Day 4: ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ (ReAct íŒ¨í„´)

from typing import List, Dict, Any, Callable
from dataclasses import dataclass
import json

@dataclass
class Tool:
    """ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬"""
    name: str
    description: str
    func: Callable
    parameters: Dict[str, str]

class ReActAgent:
    """ReAct (Reasoning + Acting) íŒ¨í„´ì˜ ì—ì´ì „íŠ¸"""
    
    def __init__(self, model: str = "gpt-4"):
        self.model = model
        self.tools: Dict[str, Tool] = {}
        self.memory: List[Dict] = []
        self.max_iterations = 10
    
    def register_tool(self, tool: Tool):
        """ë„êµ¬ ë“±ë¡"""
        self.tools[tool.name] = tool
    
    def get_tools_prompt(self) -> str:
        """ë„êµ¬ ì„¤ëª… í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        tools_text = "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:\\n"
        for name, tool in self.tools.items():
            tools_text += f"- {name}: {tool.description}\\n"
        return tools_text
    
    def think(self, observation: str) -> str:
        """ì‚¬ê³  ë‹¨ê³„: ë‹¤ìŒ í–‰ë™ ê²°ì •"""
        import openai
        
        history = json.dumps(self.memory[-3:], ensure_ascii=False)
        
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": f"""ë‹¹ì‹ ì€ ReAct ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ë§¤ í„´ë§ˆë‹¤ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. Thought: í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì•¡ì…˜ ê²°ì •
2. Action: ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ìµœì¢… ë‹µë³€ ì œì‹œ
3. Observation: ì•¡ì…˜ ê²°ê³¼ ê´€ì°°

{self.get_tools_prompt()}"""
                },
                {
                    "role": "user",
                    "content": f"""í˜„ì¬ ê´€ì°°:
{observation}

ê³¼ê±° ë©”ëª¨ë¦¬:
{history}

ë‹¤ìŒì€ Thought, Action, Observation í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
                }
            ],
            temperature=0
        )
        
        return response['choices'][0]['message']['content']
    
    def act(self, action: str) -> str:
        """ì•¡ì…˜ ì‹¤í–‰"""
        try:
            # ì•¡ì…˜ íŒŒì‹± (ì˜ˆ: "calculator(12 + 5)")
            if "(" in action and ")" in action:
                tool_name = action.split("(")[0].strip()
                params_str = action.split("(")[1].split(")")[0]
                
                if tool_name in self.tools:
                    tool = self.tools[tool_name]
                    result = tool.func(params_str)
                    return f"Action Result: {result}"
            
            return f"ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {action}"
        
        except Exception as e:
            return f"ì•¡ì…˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
    
    def run(self, query: str) -> str:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        self.memory = []
        observation = f"ì§ˆë¬¸: {query}"
        
        for i in range(self.max_iterations):
            # ì‚¬ê³  + ì•¡ì…˜
            response = self.think(observation)
            self.memory.append({"step": i, "response": response})
            
            # ìµœì¢… ë‹µë³€ í™•ì¸
            if "Final Answer:" in response:
                answer = response.split("Final Answer:")[1].strip()
                return answer
            
            # ë‹¤ìŒ ì•¡ì…˜ ì‹¤í–‰
            if "Action:" in response:
                action = response.split("Action:")[1].split("\\n")[0].strip()
                observation = self.act(action)
        
        return "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬"

# ë„êµ¬ ì •ì˜
def calculator(expression: str) -> str:
    """ê³„ì‚°ê¸° ë„êµ¬"""
    try:
        result = eval(expression)
        return str(result)
    except:
        return "ê³„ì‚° ì˜¤ë¥˜"

def search_knowledge(query: str) -> str:
    """ì§€ì‹ ê²€ìƒ‰ ë„êµ¬"""
    knowledge = {
        "íŒŒì´ì¬": "ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´",
        "ë¨¸ì‹ ëŸ¬ë‹": "ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ í•™ìŠµ"
    }
    return knowledge.get(query, "ì •ë³´ ì—†ìŒ")

# ì—ì´ì „íŠ¸ ì„¤ì •
agent = ReActAgent()
agent.register_tool(Tool(
    name="calculator",
    description="ìˆ˜í•™ ê³„ì‚° ìˆ˜í–‰",
    func=calculator,
    parameters={"expression": "ìˆ˜ì‹"}
))
agent.register_tool(Tool(
    name="search_knowledge",
    description="ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰",
    func=search_knowledge,
    parameters={"query": "ê²€ìƒ‰ì–´"}
))

# ì‹¤í–‰ ì˜ˆì œ
# result = agent.run("10 + 20ì€ ì–¼ë§ˆì…ë‹ˆê¹Œ?")
# print(result)
'''

    w.code_examples['05_end_to_end_project.py'] = '''# Day 5: í†µí•© AI í”„ë¡œì íŠ¸

from typing import List, Optional, Dict
import openai
import os

class AIAssistant:
    """í†µí•© AI ì–´ì‹œìŠ¤í„´íŠ¸"""
    
    def __init__(self, name: str = "AI Assistant"):
        self.name = name
        self.conversation_history: List[Dict] = []
        self.max_memory = 10
        self.model = "gpt-4"
    
    def add_message(self, role: str, content: str):
        """ëŒ€í™” ê¸°ë¡ì— ë©”ì‹œì§€ ì¶”ê°€"""
        self.conversation_history.append({
            "role": role,
            "content": content
        })
        
        # ë©”ëª¨ë¦¬ ì œí•œ
        if len(self.conversation_history) > self.max_memory:
            self.conversation_history.pop(0)
    
    def generate_response(self, user_input: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        self.add_message("user", user_input)
        
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
                    }
                ] + self.conversation_history,
                temperature=0.7,
                max_tokens=1000
            )
            
            assistant_message = response['choices'][0]['message']['content']
            self.add_message("assistant", assistant_message)
            
            return assistant_message
        
        except Exception as e:
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def get_summary(self) -> str:
        """ëŒ€í™” ìš”ì•½"""
        if not self.conversation_history:
            return "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        summary_prompt = "ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:\\n"
        summary_prompt += "\\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in self.conversation_history
        ])
        
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": summary_prompt}],
            max_tokens=500
        )
        
        return response['choices'][0]['message']['content']
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""
        print(f"\\n=== {self.name} ===")
        print("'exit'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.\\n")
        
        while True:
            user_input = input("ë‹¹ì‹ : ").strip()
            
            if user_input.lower() == 'exit':
                print(f"{self.name}: ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            
            if not user_input:
                continue
            
            response = self.generate_response(user_input)
            print(f"\\n{self.name}: {response}\\n")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == '__main__':
    assistant = AIAssistant(name="Python Tutor")
    
    # API í‚¤ ì„¤ì •
    openai.api_key = os.getenv("OPENAI_API_KEY")
    
    # ëŒ€í™”í˜• ì±„íŒ…
    # assistant.interactive_chat()
    
    # ë˜ëŠ” ë‹¨ì¼ ì§ˆë¬¸
    # response = assistant.generate_response("íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ëŠ”?")
    # print(f"ì‘ë‹µ: {response}")
    
    # ëŒ€í™” ìš”ì•½
    # summary = assistant.get_summary()
    # print(f"\\nìš”ì•½:\\n{summary}")
'''

    return w


def print_detail():
    d = get_detail()
    print(f"Week: {d.week}")
    print(f"Files: {len(d.files)} files")
    print(f"Tech Stack: {len(d.tech_stack)} technologies")
    print(f"Learning Content: {len(d.learning_paragraphs)} days")
    print(f"Code Examples: {len(d.code_examples)} examples")
