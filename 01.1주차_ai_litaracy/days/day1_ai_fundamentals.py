"""
Day 1: AIì™€ LLM ê¸°ì´ˆ ì´í•´
========================

AI(Artificial Intelligence), Machine Learning, Deep Learningì˜ ê°œë…ê³¼ ì°¨ì´, 
LLMì˜ ë™ì‘ ì›ë¦¬, í† í°ê³¼ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´í•´
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any

@dataclass
class LearningContent:
    """í•™ìŠµ ë‚´ìš© êµ¬ì¡°"""
    title: str
    description: str
    concepts: List[str] = field(default_factory=list)
    examples: List[str] = field(default_factory=list)
    key_takeaways: List[str] = field(default_factory=list)


class Day1Learning:
    """Day 1: AIì™€ LLM ê¸°ì´ˆ ì´í•´"""
    
    # ============ Section 1: AIì˜ ê°œë… ============
    ai_concepts = LearningContent(
        title="1.1 AI vs Machine Learning vs Deep Learning",
        description="""
        AI(ì¸ê³µì§€ëŠ¥): ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” í”„ë¡œê·¸ë¨
        - ê´‘ì˜: ê·œì¹™ ê¸°ë°˜ ì „ë¬¸ê°€ ì‹œìŠ¤í…œë„ AI
        - í˜‘ì˜: ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œ
        
        Machine Learning: ëª…ì‹œì  í”„ë¡œê·¸ë˜ë° ì—†ì´ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµ
        - Supervised Learning (ì§€ë„í•™ìŠµ): ë ˆì´ë¸”ëœ ë°ì´í„°ë¡œ í•™ìŠµ
        - Unsupervised Learning (ë¹„ì§€ë„í•™ìŠµ): íŒ¨í„´ ë°œê²¬
        - Reinforcement Learning (ê°•í™”í•™ìŠµ): ë³´ìƒìœ¼ë¡œë¶€í„° í•™ìŠµ
        
        Deep Learning: ë‹¤ì¸µ ì‹ ê²½ë§ì„ ì´ìš©í•œ ê¸°ê³„í•™ìŠµ
        - íŠ¹ì§• ì¶”ì¶œì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰
        - ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ìŒì„± ì²˜ë¦¬ì— ê°•í•¨
        """,
        concepts=[
            "AIì˜ ì •ì˜ì™€ ì—­ì‚¬",
            "Machine Learningì˜ 3ê°€ì§€ í•™ìŠµ ë°©ì‹",
            "Neural Networksì˜ ê¸°ì´ˆ",
            "Deep Learningì˜ ë“±ì¥ ë°°ê²½",
            "LLM (Large Language Models)ì˜ ê°œë…"
        ],
        examples=[
            "ì˜ˆ1: ì´ë©”ì¼ ìŠ¤íŒ¸ í•„í„° (Supervised ML)",
            "ì˜ˆ2: ì¶”ì²œ ì‹œìŠ¤í…œ (Collaborative Filtering)",
            "ì˜ˆ3: ì´ë¯¸ì§€ ë¶„ë¥˜ (Deep Learning)",
            "ì˜ˆ4: ChatGPT (LLM)"
        ],
        key_takeaways=[
            "AIëŠ” ê´‘ì˜ì˜ ê°œë…, MLì€ AIì˜ ë¶€ë¶„ì§‘í•©",
            "DLì€ MLì˜ íŠ¹ìˆ˜í•œ í˜•íƒœ",
            "LLMì€ transformer ì•„í‚¤í…ì²˜ ê¸°ë°˜ì˜ DL ëª¨ë¸"
        ]
    )
    
    # ============ Section 2: LLMì˜ ë™ì‘ ì›ë¦¬ ============
    llm_architecture = LearningContent(
        title="1.2 LLMì˜ ë™ì‘ ì›ë¦¬",
        description="""
        LLM (Large Language Model): ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ í•™ìŠµëœ ì‹ ê²½ë§
        
        í•™ìŠµ ê³¼ì •:
        1. Pre-training: ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¡œ ë¯¸ë¦¬ í•™ìŠµ
           - Unsupervised Learning
           - ë‹¤ìŒ í† í° ì˜ˆì¸¡ (Causal Language Modeling)
        
        2. Fine-tuning: íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¶”ê°€ í•™ìŠµ
           - Supervised Learning
           - Instruction tuning, RLHF (Reinforcement Learning from Human Feedback)
        
        3. In-context Learning: í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•œ ì ì‘
           - Few-shot examples
           - ì¬í•™ìŠµ ì—†ì´ ìƒˆë¡œìš´ ì‘ì—… ìˆ˜í–‰
        
        Transformer ì•„í‚¤í…ì²˜:
        - Attention ë©”ì»¤ë‹ˆì¦˜: ì…ë ¥ ê°„ì˜ ìƒê´€ê´€ê³„ í•™ìŠµ
        - Self-Attention: ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ë¶€ì˜ ê´€ê³„ íŒŒì•…
        - Multi-head Attention: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— í•™ìŠµ
        """,
        concepts=[
            "Transformer ì•„í‚¤í…ì²˜ì˜ êµ¬ì¡°",
            "Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì›ë¦¬",
            "Self-Attentionê³¼ Cross-Attention",
            "Positional Encoding (ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©)",
            "Feed-forward Networks",
            "Normalizationê³¼ Residual Connections"
        ],
        examples=[
            "ì˜ˆ1: GPT ì‹œë¦¬ì¦ˆì˜ ì§„í™” (GPT-2 â†’ GPT-3 â†’ GPT-4)",
            "ì˜ˆ2: BERT vs GPTì˜ ì°¨ì´ì ",
            "ì˜ˆ3: Attention ì‹œê°í™” ì˜ˆì œ",
            "ì˜ˆ4: í† í° ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜"
        ],
        key_takeaways=[
            "LLMì˜ í•µì‹¬ì€ Attention ë©”ì»¤ë‹ˆì¦˜",
            "Pre-trainingê³¼ Fine-tuningì˜ ì¤‘ìš”ì„±",
            "í”„ë¡¬í”„íŠ¸ê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢Œìš°",
            "ë” í° ëª¨ë¸, ë” ë§ì€ ë°ì´í„° = ë” ë‚˜ì€ ì„±ëŠ¥"
        ]
    )
    
    # ============ Section 3: í† í°ê³¼ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ============
    tokenization = LearningContent(
        title="1.3 í† í°(Token)ê³¼ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°",
        description="""
        í† í° (Token): LLMì´ ì´í•´í•˜ëŠ” ìµœì†Œ ë‹¨ìœ„
        - ë‹¨ì–´, ë¶€ë¶„ ë‹¨ì–´, ì‹¬ë³¼ ë“±ìœ¼ë¡œ ë¶„í• 
        - ê° í† í°ì€ ê³ ìœ í•œ ì •ìˆ˜ IDë¡œ ë§¤í•‘
        - 1 í† í° â‰ˆ 0.75 ë‹¨ì–´ (í‰ê· )
        
        í† í°í™” ë°©ì‹:
        1. Word Tokenization: ì „ì²´ ë‹¨ì–´ ë‹¨ìœ„ (ì–´íœ˜ í¬ê¸° í¼)
        2. Subword Tokenization: BPE, WordPiece (íš¨ìœ¨ì )
        3. Character Tokenization: ë¬¸ì ë‹¨ìœ„ (ë“œë¬¼ê²Œ ì‚¬ìš©)
        
        ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (Context Window):
        - ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜
        - GPT-4: 8K, 32K, 128K ë²„ì „
        - Claude: 100K, 200K í† í°
        - ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ = ë” ë§ì€ ì •ë³´ ì²˜ë¦¬ ê°€ëŠ¥
        
        í† í° ê³„ì‚°:
        - Input tokens: ì…ë ¥ í”„ë¡¬í”„íŠ¸ì˜ í† í° ìˆ˜
        - Output tokens: ìƒì„±ëœ ì‘ë‹µì˜ í† í° ìˆ˜
        - API ë¹„ìš© = (input_tokens Ã— input_price) + (output_tokens Ã— output_price)
        """,
        concepts=[
            "í† í°í™” ì•Œê³ ë¦¬ì¦˜ (BPE, WordPiece, SentencePiece)",
            "í† í°ê³¼ ë‹¨ì–´ì˜ ê´€ê³„",
            "ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì˜ ì œí•œ",
            "ë¡± ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ìˆ ",
            "í† í° íš¨ìœ¨ì„± ìµœì í™”"
        ],
        examples=[
            "ì˜ˆ1: ê°™ì€ ë¬¸ì¥ì˜ ë‹¤ì–‘í•œ í† í°í™” ê²°ê³¼",
            "ì˜ˆ2: ë‹¤êµ­ì–´ í† í°í™”ì˜ ì°¨ì´",
            "ì˜ˆ3: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ ì‚¬ë¡€",
            "ì˜ˆ4: í† í° ìˆ˜ ì¶”ì • ê³„ì‚°"
        ],
        key_takeaways=[
            "í”„ë¡¬í”„íŠ¸ ìµœì í™”ëŠ” í† í° íš¨ìœ¨ì„± ê³ ë ¤ í•„ìˆ˜",
            "ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ëŠ” ì¥ë¬¸ ì²˜ë¦¬ì˜ í•µì‹¬ ì œì•½",
            "í† í° ìˆ˜ë¥¼ ì •í™•íˆ ê³„ì‚°í•˜ë©´ ë¹„ìš© ì˜ˆì¸¡ ê°€ëŠ¥",
            "ì–¸ì–´ë§ˆë‹¤ í† í°í™” íš¨ìœ¨ì´ ë‹¤ë¦„"
        ]
    )
    
    # ============ Section 4: OpenAI API ì„¤ì • ============
    api_setup = LearningContent(
        title="1.4 OpenAI API ê°€ì… ë° ì„¤ì •",
        description="""
        OpenAI API ì‹œì‘í•˜ê¸°:
        
        1ë‹¨ê³„: ê³„ì • ìƒì„±
        - https://platform.openai.com ë°©ë¬¸
        - ì´ë©”ì¼ë¡œ ê°€ì…
        - ì´ë©”ì¼ ì¸ì¦
        
        2ë‹¨ê³„: API í‚¤ ìƒì„±
        - Settings â†’ API keys ì´ë™
        - "Create new secret key" í´ë¦­
        - í‚¤ ë³µì‚¬ ë° ì•ˆì „í•˜ê²Œ ë³´ê´€
        
        3ë‹¨ê³„: ì´ˆê¸° í¬ë ˆë”§ í™•ì¸
        - ì‹ ê·œ ê°€ì…ì: $5 ë¬´ë£Œ í¬ë ˆë”§ (3ê°œì›” ìœ íš¨)
        - Usage í˜ì´ì§€ì—ì„œ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰ í™•ì¸
        
        4ë‹¨ê³„: API í‚¤ ì„¤ì •
        - í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •: OPENAI_API_KEY
        - Pythonì—ì„œ ë¡œë“œ: os.getenv("OPENAI_API_KEY")
        - ì ˆëŒ€ ì½”ë“œì— ì§ì ‘ ì‚½ì…í•˜ì§€ ë§ ê²ƒ!
        
        5ë‹¨ê³„: ìš”ê¸ˆ ì œí•œ ì„¤ì •
        - Usage limits ì„¤ì •ìœ¼ë¡œ ì˜ˆìƒì¹˜ ëª»í•œ ë¹„ìš© ë°©ì§€
        - ì¡°ì§ ë ˆë²¨ì˜ ë¹„ìš© ê´€ë¦¬
        """,
        concepts=[
            "OpenAI ê³„ì • ë° ì¡°ì§ ê´€ë¦¬",
            "API í‚¤ ë³´ì•ˆ ê´€ë¦¬",
            "ì‚¬ìš©ë£Œ ëª¨ë‹ˆí„°ë§",
            "ìš”ê¸ˆ ì œí•œ ì„¤ì •",
            "API ì—ëŸ¬ ì²˜ë¦¬"
        ],
        examples=[
            "ì˜ˆ1: .env íŒŒì¼ì„ ì´ìš©í•œ ì•ˆì „í•œ í‚¤ ê´€ë¦¬",
            "ì˜ˆ2: í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë°©ë²•",
            "ì˜ˆ3: ì²« API í˜¸ì¶œ ì½”ë“œ",
            "ì˜ˆ4: ë¹„ìš© ì¶”ì  ëŒ€ì‹œë³´ë“œ"
        ],
        key_takeaways=[
            "API í‚¤ëŠ” ì ˆëŒ€ ê³µê°œí•˜ë©´ ì•ˆ ë¨",
            "í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•œ í‚¤ ê´€ë¦¬ê°€ í•„ìˆ˜",
            "ì‚¬ìš©ëŸ‰ì„ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•  ê²ƒ",
            "ìš”ê¸ˆ ì œí•œì„ ì„¤ì •í•˜ì—¬ ê³¼ë„í•œ ë¹„ìš© ë°©ì§€"
        ]
    )
    
    # ============ Section 5: ì²« API í˜¸ì¶œ ============
    first_call = LearningContent(
        title="1.5 ì²« ë²ˆì§¸ ChatGPT API í˜¸ì¶œ",
        description="""
        ê¸°ë³¸ API í˜¸ì¶œ êµ¬ì¡°:
        
        1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
        2. API í‚¤ ì„¤ì •
        3. ë©”ì‹œì§€ êµ¬ì„± (system, user)
        4. API í˜¸ì¶œ
        5. ì‘ë‹µ íŒŒì‹± ë° ì¶œë ¥
        
        Chat Completion API:
        - Model: ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ (gpt-4, gpt-3.5-turbo ë“±)
        - Messages: ëŒ€í™” ì´ë ¥
        - Temperature: ì‘ë‹µì˜ ì°½ì˜ì„± (0~2)
        - Max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        
        ì‘ë‹µ êµ¬ì¡°:
        - choices[0]['message']['content']: ì‹¤ì œ ì‘ë‹µ í…ìŠ¤íŠ¸
        - usage: ì‚¬ìš©í•œ í† í° ìˆ˜
        - model: ì‹¤ì œ ì‚¬ìš©ëœ ëª¨ë¸
        - finish_reason: ìƒì„± ì¢…ë£Œ ì´ìœ 
        """,
        concepts=[
            "Chat Completion APIì˜ ê¸°ë³¸ êµ¬ì¡°",
            "ë©”ì‹œì§€ ì—­í•  (system, user, assistant)",
            "ì£¼ìš” íŒŒë¼ë¯¸í„°ì™€ ì˜ë¯¸",
            "ì‘ë‹µ êµ¬ì¡° í•´ì„",
            "ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„"
        ],
        examples=[
            "ì˜ˆ1: ê°„ë‹¨í•œ ì§ˆë¬¸-ë‹µë³€",
            "ì˜ˆ2: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í™œìš©",
            "ì˜ˆ3: ë‹¤ì¤‘ í„´ ëŒ€í™”",
            "ì˜ˆ4: í† í° ì‚¬ìš©ëŸ‰ ì¶”ì "
        ],
        key_takeaways=[
            "system ì—­í• ì´ ëª¨ë¸ì˜ ë™ì‘ ë°©ì‹ì„ ê²°ì •",
            "TemperatureëŠ” ì‘ë‹µì˜ ì¼ê´€ì„±ê³¼ ì°½ì˜ì„±ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„",
            "API ì‘ë‹µì—ëŠ” ì‚¬ìš©ëŸ‰ ì •ë³´ í¬í•¨",
            "ì—ëŸ¬ ì²˜ë¦¬ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ í•„ìˆ˜"
        ]
    )
    
    @staticmethod
    def get_all_content() -> Dict[str, LearningContent]:
        """ëª¨ë“  í•™ìŠµ ë‚´ìš© ë°˜í™˜"""
        return {
            "ai_concepts": Day1Learning.ai_concepts,
            "llm_architecture": Day1Learning.llm_architecture,
            "tokenization": Day1Learning.tokenization,
            "api_setup": Day1Learning.api_setup,
            "first_call": Day1Learning.first_call,
        }
    
    @staticmethod
    def print_summary():
        """í•™ìŠµ ë‚´ìš© ìš”ì•½ ì¶œë ¥"""
        print("=" * 60)
        print("Day 1: AIì™€ LLM ê¸°ì´ˆ ì´í•´")
        print("=" * 60)
        
        for section_name, content in Day1Learning.get_all_content().items():
            print(f"\nğŸ“Œ {content.title}")
            print(f"ê°œë…: {', '.join(content.concepts[:3])}...")
            print(f"ì£¼ìš” í•™ìŠµ: {', '.join(content.key_takeaways[:2])}...")
