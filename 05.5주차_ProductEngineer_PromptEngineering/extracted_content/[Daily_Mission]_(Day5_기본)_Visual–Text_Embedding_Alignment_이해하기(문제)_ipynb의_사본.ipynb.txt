# [Daily Mission] Day 5(ê¸°ë³¸) â€” Visualâ€“Text Embedding Alignment ì´í•´í•˜ê¸°
ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ëŠ” ì–´ë–»ê²Œ ê°™ì€ ì˜ë¯¸ ê³µê°„ì— ë†“ì´ëŠ”ê°€?

## ë¯¸ì…˜ì†Œê°œ

ì´ë²ˆ Daily Missionì—ì„œëŠ”  
Visual Prompt Engineeringì˜ ê·¼ë³¸ ì›ë¦¬ì¸ ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ì„ë² ë”© ì •ë ¬(Embedding Alignment)ì„ ì§ì ‘ ì‹¤í—˜ì„ í†µí•´ ì´í•´í•©ë‹ˆë‹¤.

ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì€ ì´ë¯¸ì§€ë¥¼ ë‹¨ìˆœíˆ â€œí…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œâ€ ì´í•´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
ëŒ€ì‹  ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ë²¡í„°(embedding)ë¡œ ë³€í™˜í•œ ë’¤,  
ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ **ê°™ì€ ì„ë² ë”© ê³µê°„ì—ì„œ ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ë„ë¡ í•™ìŠµ**ë©ë‹ˆë‹¤.

ì¦‰,  
> ìš°ë¦¬ê°€ ì´ë¯¸ì§€ë¥¼ ì˜ ì´í•´í•˜ê²Œ ë§Œë“œëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•œë‹¤ëŠ” ê²ƒì€  
> ê²°êµ­ **ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ ì˜ ì •ë ¬ë˜ë„ë¡ ë•ëŠ” ê³¼ì •**ì…ë‹ˆë‹¤.

---

## ì´ë²ˆ ë¯¸ì…˜ì˜ ëª©í‘œ

ì´ë²ˆ ë¯¸ì…˜ì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ê°ê° embeddingìœ¼ë¡œ ë³€í™˜ëœë‹¤ëŠ” ê°œë…ì„ ì´í•´í•œë‹¤
- ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°(image / text)ê°€ **ê°™ì€ ì˜ë¯¸ ê³µê°„ì— ì •ë ¬ë  ìˆ˜ ìˆìŒ**ì„ ì‹¤í—˜ìœ¼ë¡œ í™•ì¸í•œë‹¤
- â€œì˜ ì“´ ì„¤ëª…â€ê³¼ â€œì˜ëª»ëœ ì„¤ëª…â€ì´ ì´ë¯¸ì§€ embeddingê³¼ì˜ ê±°ë¦¬ì—ì„œ ì–´ë–»ê²Œ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ê´€ì°°í•œë‹¤
- Visual Prompt Engineeringì„ **ë¬¸ì¥ ì‘ì„± ê¸°ìˆ ì´ ì•„ë‹Œ, ì˜ë¯¸ ê³µê°„ ì •ë ¬ ë¬¸ì œ**ë¡œ ì´í•´í•œë‹¤

---

## ì´ë²ˆ ë¯¸ì…˜ì—ì„œ ë‹¤ë£° í•µì‹¬ ì§ˆë¬¸

ì´ ë¯¸ì…˜ì€ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê³¼ì •ì„ í†µí•´ ì§„í–‰ë©ë‹ˆë‹¤.

- í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë§Œë“¤ì—ˆì„ ë•Œ,
  ì–´ë–¤ ì„¤ëª…ì´ ì´ë¯¸ì§€ì™€ ê°€ì¥ ê°€ê¹ê²Œ ì •ë ¬ë ê¹Œ?
- í…ìŠ¤íŠ¸ ì„¤ëª…ì˜ ê¸¸ì´ë³´ë‹¤ **ì˜ë¯¸ì˜ ì •í™•ì„±**ì´ ë” ì¤‘ìš”í• ê¹Œ?
- ì–´ë–¤ ì •ë³´(ê°ì²´, ê´€ê³„, ë§¥ë½)ê°€ ë¹ ì§€ë©´
  ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ê±°ë¦¬ëŠ” ë©€ì–´ì§ˆê¹Œ?

ì´ ì§ˆë¬¸ë“¤ì— ëŒ€í•œ ë‹µì„ **ì •ì„±ì ì¸ ëŠë‚Œì´ ì•„ë‹ˆë¼, ì„ë² ë”© ìœ ì‚¬ë„ë¼ëŠ” ì •ëŸ‰ì  ì§€í‘œ**ë¡œ í™•ì¸í•´ë´…ë‹ˆë‹¤.

---

## ë¯¸ì…˜ ì§„í–‰ ë°©ì‹

ì´ë²ˆ ë¯¸ì…˜ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ íë¦„ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤.

1. í•˜ë‚˜ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•œë‹¤
2. ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë§Œë“ ë‹¤
3. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°ê° embeddingìœ¼ë¡œ ë³€í™˜í•œë‹¤
4. ì´ë¯¸ì§€ embeddingê³¼ í…ìŠ¤íŠ¸ embedding ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œë‹¤
5. ì„¤ëª… í’ˆì§ˆì— ë”°ë¼ ìœ ì‚¬ë„ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ë¹„êµÂ·í•´ì„í•œë‹¤

ì´ë¥¼ í†µí•´ **ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ â€œì´í•´í•œë‹¤â€ëŠ” ê²ƒì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€**ë¥¼ ì§ì ‘ í™•ì¸í•˜ê²Œ ë©ë‹ˆë‹¤.

---

ì´ë²ˆ ë¯¸ì…˜ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€  
> **ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì–´ë–»ê²Œ ê°™ì€ ì˜ë¯¸ ê³µê°„ì—ì„œ ë§Œë‚˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.ğŸš€

## ë¯¸ì…˜ ê°œìš”

ì´ ë¯¸ì…˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

1. **í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„**  
   í•˜ë‚˜ì˜ ê¸°ì¤€ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ê³ , ì´í›„ ëª¨ë“  ì‹¤í—˜ì—ì„œ ë™ì¼í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

2. **í…ìŠ¤íŠ¸ ì„¤ëª… í›„ë³´ ì‘ì„±**  
   ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•´  
   - ì˜ë¯¸ê°€ ì˜ ë§ëŠ” ì„¤ëª…  
   - ë¶€ë¶„ì ìœ¼ë¡œ ë§ëŠ” ì„¤ëª…  
   - ì˜ë¯¸ê°€ ê±°ì˜ ë§ì§€ ì•ŠëŠ” ì„¤ëª…  
   ë“± ì—¬ëŸ¬ ìˆ˜ì¤€ì˜ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

3. **ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±**  
   ì„ íƒí•œ ì´ë¯¸ì§€ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì´ë¯¸ì§€ê°€ ì˜ë¯¸ ê³µê°„ì—ì„œ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

4. **í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±**  
   ê° í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë™ì¼í•œ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

5. **ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°**  
   ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ì„¤ëª…ì˜ í’ˆì§ˆì— ë”°ë¼ ê±°ë¦¬(ìœ ì‚¬ë„)ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ë¹„êµí•©ë‹ˆë‹¤.

6. **ê²°ê³¼ í•´ì„ ë° ì •ë¦¬**  
   ì–´ë–¤ ì„¤ëª…ì´ ì´ë¯¸ì§€ì™€ ê°€ì¥ ê°€ê¹ê²Œ ì •ë ¬ë˜ì—ˆëŠ”ì§€ ë¶„ì„í•˜ê³ , ê·¸ ì´ìœ ë¥¼ í…ìŠ¤íŠ¸ ë‚´ìš©ê³¼ ì—°ê²°í•´ í•´ì„í•©ë‹ˆë‹¤.

## í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„

import matplotlib.pyplot as plt

# ë°ì´í„° ì •ì˜
years = ["2020", "2021", "2022", "2023"]
category_a = [100, 150, 200, 250]
category_b = [120, 130, 140, 160]
category_c = [200, 180, 160, 140]

# ì°¨íŠ¸ ìƒì„±
plt.figure(figsize=(6, 4))
plt.plot(years, category_a, marker="o", label="Category A")
plt.plot(years, category_b, marker="o", label="Category B")
plt.plot(years, category_c, marker="o", label="Category C")

plt.title("Annual Sales by Category")
plt.xlabel("Year")
plt.ylabel("Sales")
plt.legend()
plt.grid(True)

# ì´ë¯¸ì§€ ì €ì¥
image_path = "/content/test_chart.png"
plt.savefig(image_path, bbox_inches="tight")
plt.show()

print(f"Test image saved to: {image_path}")


from PIL import Image
import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
image_path = "/content/test_chart.png"  # ì¤€ë¹„í•œ ì´ë¯¸ì§€ ê²½ë¡œë¡œ ìˆ˜ì •

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open(image_path)

# ì´ë¯¸ì§€ ì¶œë ¥
plt.imshow(image)
plt.axis("off")


## í…ŒìŠ¤íŠ¸ ì„¤ëª… í›„ë³´ ë§Œë“¤ê¸°

ì´ì œ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì— ëŒ€í•´  
**ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ ì„¤ëª… í›„ë³´**ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.

ì´ ë‹¨ê³„ì˜ ëª©ì ì€ â€œì •ë‹µ ì„¤ëª…ì„ í•˜ë‚˜ ê³ ë¥´ëŠ” ê²ƒâ€ì´ ì•„ë‹ˆë¼, **ì„¤ëª…ì˜ í’ˆì§ˆ ì°¨ì´ê°€ ì„ë² ë”© ì •ë ¬ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€**ë¥¼ ì‹¤í—˜í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

---

### ì„¤ëª… í›„ë³´ êµ¬ì„± ë°©ì‹

ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•´ ë‹¤ìŒ ì„¸ ê°€ì§€ ìˆ˜ì¤€ì˜ ì„¤ëª…ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

1. **ì˜ë¯¸ê°€ ì˜ ë§ëŠ” ì„¤ëª… (Good)**  
   - ì°¨íŠ¸ì˜ ìœ í˜•ì„ ì •í™•íˆ ì–¸ê¸‰
   - ì¶•ê³¼ ë²”ì£¼ë¥¼ ëª…í™•íˆ ì„¤ëª…
   - ì£¼ìš” ì¶”ì„¸(ì¦ê°€, ê°ì†Œ, ë¹„êµ)ë¥¼ í¬í•¨

2. **ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë§ëŠ” ì„¤ëª… (Medium)**  
   - ì°¨íŠ¸ë¼ëŠ” ì ì€ ì–¸ê¸‰í•˜ì§€ë§Œ
   - êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì¶”ì„¸ ì •ë³´ëŠ” ë¶€ì¡±

3. **ì˜ë¯¸ê°€ ê±°ì˜ ë§ì§€ ì•ŠëŠ” ì„¤ëª… (Bad)**  
   - ì°¨íŠ¸ ìœ í˜•ì´ë‚˜ ë‚´ìš©ì´ ì‹¤ì œ ì´ë¯¸ì§€ì™€ ë‹¤ë¦„
   - ì´ë¯¸ì§€ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë°˜ì˜í•˜ì§€ ì•ŠìŒ

ì•„ë˜ ì…€ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì„¤ëª… í›„ë³´ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.


### (Todo1) í…ìŠ¤íŠ¸ ì„¤ëª… í›„ë³´ ì •ì˜

text_descriptions = {
    # Good: ì´ë¯¸ì§€ì˜ ëª¨ë“  ì •ë³´(ì°¨íŠ¸ íƒ€ì…, ê¸°ê°„, ë²”ë¡€, ìˆ˜ì¹˜ì  ì¶”ì„¸)ë¥¼ ì™„ë²½íˆ ë¬˜ì‚¬ -> ê°„ê²°í•˜ê²Œ ìˆ˜ì •
    "good": [
        "A line chart titled 'Annual Sales by Category' showing trends from 2020 to 2023. Category A grows, Category B shows slight growth, and Category C declines."
    ],

    # Medium: ì •ë³´ëŠ” ë§ì§€ë§Œ êµ¬ì²´ì„±ì´ ë–¨ì–´ì§ (ì„ë² ë”© ê³µê°„ì—ì„œ ê·¼ì²˜ì—ëŠ” ìˆì§€ë§Œ ì •í™•ë„ëŠ” ë‚®ìŒ)
    "medium": [
        "A line graph showing sales data for three different categories over a four-year period. There are multiple lines with markers indicating data points for each year."
    ],

    # Bad: ì™„ì „íˆ ë‹¤ë¥¸ ì •ë³´ (Negative Sampling)
    # ë°ì´í„°ëŠ” 'Sales'ì¸ë° 'Temperature'ë¡œ ë¬˜ì‚¬í•˜ê±°ë‚˜, 'Line'ì¸ë° 'Bar'ë¡œ ë¬˜ì‚¬í•˜ì—¬ ëŒ€ì¡°
    "bad": [
        "A bar chart displaying monthly temperature fluctuations in three different cities. The horizontal axis represents months, and the vertical axis shows Celsius degrees."
    ]
}


# í™•ì¸
for level, texts in text_descriptions.items():
    print(f"[{level.upper()}]")
    for t in texts:
        print("- ", t)
    print()


## (Todo2) ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±

ì´ì œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì˜ë¯¸ë¥¼ ë‹´ì€ ë²¡í„°(embedding)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ í”½ì…€ ê·¸ëŒ€ë¡œ ë‹¤ë£¨ì§€ ì•Šê³ , ì´ë¯¸ì§€ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ìš”ì•½í•œ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

ì´ ë²¡í„°ëŠ” ì´í›„ ë‹¨ê³„ì—ì„œ:
- í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ë¹„êµë˜ê³ 
- ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.

ì´ë²ˆ ë¯¸ì…˜ì—ì„œëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°™ì€ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ì •ë ¬í•˜ë„ë¡ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì•„ë˜ ì…€ì—ì„œ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , ê·¸ ê²°ê³¼ê°€ ì–´ë–¤ í˜•íƒœì¸ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.

# ì´ˆë°˜ í•œ ë²ˆë§Œ ì‹¤í–‰
!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import torch
import clip
from PIL import Image

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP ëª¨ë¸ ë¡œë“œ
model, preprocess = clip.load("ViT-B/32", device=device)


"""
TODO:
ì´ ì½”ë“œëŠ” ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì— ë„£ê¸° ì „ì— í•„ìš”í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê³ ,
ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•œ ë’¤(deviceë¡œ ì´ë™),
ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì´ë‹¤.

íŒíŠ¸:
- preprocess(...) : PIL Imageë¥¼ ëª¨ë¸ ì…ë ¥ìš© Tensorë¡œ ë³€í™˜
- unsqueeze(0)    : ë°°ì¹˜ ì°¨ì›(batch dimension) ì¶”ê°€ (1, C, H, W)
- to(device)      : CPU ë˜ëŠ” GPUë¡œ Tensor ì´ë™
"""

# TODO 2-1: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜(preprocess)ë¥¼ ì ìš©í•˜ê³ ,
#         ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•œ ë’¤ deviceë¡œ ì´ë™í•˜ì‹œì˜¤
image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
# TODO 2-1-1: ì´ë¯¸ì§€ â†’ Tensor ë³€í™˜
# TODO 2-1-2: ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (ì™œ 0ì¼ê¹Œ?)
# ì—°ì‚° ì¥ì¹˜ë¡œ ì´ë™


with torch.no_grad():
    image_embedding = model.encode_image(image_input)


print("Image embedding shape:", image_embedding.shape)


## ì™œ ì—¬ê¸°ì„œ `unsqueeze(0)`ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

---

### 1ï¸âƒ£ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ í•­ìƒ ë°°ì¹˜(batch) ë‹¨ìœ„ ì…ë ¥ì„ ê¸°ëŒ€í•œë‹¤

CLIPê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ **ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„**ë˜ì–´ ìˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ì˜ ê¸°ë³¸ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

(batch_size, channels, height, width)

---

### 2ï¸âƒ£ ì „ì²˜ë¦¬ ì§í›„ ì´ë¯¸ì§€ì˜ í˜•íƒœ

ì´ë¯¸ì§€ì— ì „ì²˜ë¦¬ë§Œ ì ìš©í•˜ë©´ Tensorì˜ í˜•íƒœëŠ” ë³´í†µ ë‹¤ìŒê³¼ ê°™ë‹¤. (channels, height, width) ì¦‰, **ë°°ì¹˜ ì°¨ì›ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.**

ì´ ìƒíƒœë¡œ ëª¨ë¸ì— ì…ë ¥í•˜ë©´  
- ì…ë ¥ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê±°ë‚˜  
- ëª¨ë¸ì´ ì…ë ¥ì„ ì˜¬ë°”ë¥´ê²Œ í•´ì„í•˜ì§€ ëª»í•œë‹¤.

---

### 3ï¸âƒ£ `unsqueeze(0)`ì˜ ì—­í• 

`unsqueeze(0)`ì€ **Tensorì˜ ë§¨ ì•ì— ì°¨ì›ì„ í•˜ë‚˜ ì¶”ê°€**í•œë‹¤. ê·¸ ê²°ê³¼ Tensorì˜ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë°”ë€ë‹¤.

(1, channels, height, width)

ì—¬ê¸°ì„œ `1`ì€ ë‹¤ìŒì„ ì˜ë¯¸í•œë‹¤. ì´ ë°°ì¹˜ì—ëŠ” ì´ë¯¸ì§€ê°€ 1ì¥ ë“¤ì–´ ìˆë‹¤.

ì¦‰, ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ **ë°°ì¹˜ í¬ê¸° 1ì§œë¦¬ ì…ë ¥**ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤.

---

### 4ï¸âƒ£ ì™œ 0ë²ˆ ìœ„ì¹˜ì— ì°¨ì›ì„ ì¶”ê°€í•˜ëŠ”ê°€?

`unsqueeze(0)`ì—ì„œ `0`ì€ **ì°¨ì›ì„ ì¶”ê°€í•  ìœ„ì¹˜**ë¥¼ ì˜ë¯¸í•œë‹¤.

- 0ë²ˆ ìœ„ì¹˜ëŠ” ê°€ì¥ ì•ìª½
- ì´ ìœ„ì¹˜ê°€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” **batch ì°¨ì›**ì´ë‹¤

ë‹¤ë¥¸ ìœ„ì¹˜ì— ì°¨ì›ì„ ì¶”ê°€í•˜ë©´ ì…ë ¥ êµ¬ì¡°ê°€ ê¹¨ì ¸ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤.


ë”°ë¼ì„œ `unsqueeze(0)`ëŠ” ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì…ë ¥ ê·œê²©ì— ë§ì¶”ê¸° ìœ„í•œ í•„ìˆ˜ ë‹¨ê³„ì´ë‹¤.

print) Image embedding shape: torch.Size([1, 512])
- ì´ ì´ë¯¸ì§€ëŠ” 512ì°¨ì›ì˜ ì˜ë¯¸ ë²¡í„° í•˜ë‚˜ë¡œ í‘œí˜„ë¨

## (Todo3) í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

ì´ì œ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¤€ë¹„í•œ **í…ìŠ¤íŠ¸ ì„¤ëª… í›„ë³´ë“¤ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜**í•©ë‹ˆë‹¤.

ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì—ì„œëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì„œë¡œ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì…ë ¥ë˜ì§€ë§Œ, ìµœì¢…ì ìœ¼ë¡œëŠ” **ê°™ì€ ì„ë² ë”© ê³µê°„**ì— ì •ë ¬ë˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.

ì¦‰,
- ì´ë¯¸ì§€ ì„ë² ë”©ê³¼
- í…ìŠ¤íŠ¸ ì„ë² ë”©ì€

ğŸ‘‰ **ì„œë¡œ ì§ì ‘ ë¹„êµí•  ìˆ˜ ìˆëŠ” ê°™ì€ ì°¨ì›ì˜ ë²¡í„°**ì…ë‹ˆë‹¤.

ì•„ë˜ ì…€ì—ì„œëŠ” ì•ì„œ ì •ì˜í•œ í…ìŠ¤íŠ¸ ì„¤ëª… í›„ë³´ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ê·¸ ê²°ê³¼ì˜ í˜•íƒœë¥¼ í™•ì¸í•´ë´…ë‹ˆë‹¤.

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

text_embeddings = {}

with torch.no_grad():
    # ì²« ë²ˆì§¸ ë°˜ë³µë¬¸ë§Œ í•„ìš”í•˜ë©°, ì´ ì•ˆì—ì„œ í† í°í™” ë° ì„ë² ë”© ìƒì„±
    for level, texts in text_descriptions.items():
        """
        TODO 3 ì„¤ëª…:
        ì´ ì½”ë“œëŠ” í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤.
        ì „ì²´ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

        1. í…ìŠ¤íŠ¸ í† í°í™”(Tokenization)
        2. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±(Encoding)

        ê° ë‹¨ê³„ì˜ ëª©ì ê³¼ ì—­í• ì„ ì´í•´í•˜ê³ , TODOë¥¼ ì±„ì›Œë³´ì.
        """

        # TODO 3-1: í…ìŠ¤íŠ¸ í† í°í™”
        # ì„¤ëª…:
        # - CLIPê³¼ ê°™ì€ ëª¨ë¸ì€ ë¬¸ìì—´ì„ ë°”ë¡œ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ì—†ë‹¤.
        # - ë”°ë¼ì„œ í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ í† í°(token) ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.
        # - clip.tokenize í•¨ìˆ˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í† í°í™”í•´ì¤€ë‹¤.
        # 'texts'ëŠ” í˜„ì¬ ë ˆë²¨ì˜ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì´ë¥¼ ì§ì ‘ í† í°í™”í•œë‹¤.
        text_tokens = clip.tokenize(texts).to(device)

        # TODO 3-2: í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        # ì„¤ëª…:
        # - í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ê³ ì • ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.
        # - ìœ„ì˜ image embedding í•¨ìˆ˜ë¥¼ ì°¸ê³ í•´ì„œ ì„ë² ë”© ìƒì„± í•¨ìˆ˜ë¥¼ ê¸°ì¬í•œë‹¤.
        embeddings = model.encode_text(text_tokens)

        # ìƒì„±ëœ ì„ë² ë”©ì„ ì €ì¥í•œë‹¤.
        text_embeddings[level] = embeddings

# í™•ì¸
for level, emb in text_embeddings.items():
    print(f"{level.upper()} embedding shape:", emb.shape)


## ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°


ì•ì„  ì…€ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.

- ì´ë¯¸ì§€ ì„ë² ë”© 1ê°œ
- ì„œë¡œ ë‹¤ë¥¸ ìˆ˜ì¤€ì˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ì—¬ëŸ¬ ê°œ (Good / Medium / Bad)
- ëª¨ë“  ì„ë² ë”©ì€ ë™ì¼í•œ ì°¨ì›ê³¼ ì˜ë¯¸ ê³µê°„ì„ ê³µìœ 

ì´ì œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ì—ì„œ

> **ì–´ë–¤ í…ìŠ¤íŠ¸ ì„¤ëª…ì´  
> ì´ë¯¸ì§€ì™€ ê°€ì¥ ê°€ê¹ê²Œ ì •ë ¬ë˜ì–´ ìˆì„ê¹Œ?**

ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„(similarity)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ì´ë²ˆ ë¯¸ì…˜ì—ì„œëŠ” ì„ë² ë”© ê°„ ê°ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” **Cosine Similarity**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Cosine Similarityì˜ ì§ê´€ì ì¸ ì˜ë¯¸

Cosine SimilarityëŠ” ë‘ ë²¡í„° ì‚¬ì´ì˜ **ê°ë„(angle)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

- ë‘ ë²¡í„°ê°€ ê°™ì€ ë°©í–¥ì„ ê°€ë¦¬í‚¬ìˆ˜ë¡ â†’ ê°’ì´ ì»¤ì§
- ë‘ ë²¡í„°ê°€ ë‹¤ë¥¸ ë°©í–¥ì„ ê°€ë¦¬í‚¬ìˆ˜ë¡ â†’ ê°’ì´ ì‘ì•„ì§

ì¦‰,
> **ë²¡í„°ì˜ í¬ê¸°(length)ê°€ ì•„ë‹ˆë¼  
> ë°©í–¥(direction)ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€**ë¥¼ ë³´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.

---

### ì™œ ì„ë² ë”© ë¹„êµì— Cosine Similarityë¥¼ ì‚¬ìš©í• ê¹Œ?

ì„ë² ë”© ë²¡í„°ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ â€œì–¼ë§ˆë‚˜ í° ê°’ì¸ê°€â€ê°€ ì•„ë‹ˆë¼ â€œì–´ë–¤ ì˜ë¯¸ ë°©í–¥ì„ ê°€ë¦¬í‚¤ëŠ”ê°€â€ì…ë‹ˆë‹¤.

ê·¸ë˜ì„œ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì—ì„œëŠ” ë³´í†µ:
- ì„ë² ë”© ë²¡í„°ë¥¼ ì •ê·œí™”(normalization)í•œ ë’¤
- Cosine Similarityë¡œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´:
- ë¬¸ì¥ì´ ê¸¸ì–´ì ¸ë„
- ì´ë¯¸ì§€ í•´ìƒë„ê°€ ë‹¬ë¼ë„  
ğŸ‘‰ **ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë©´ ë†’ì€ ì ìˆ˜**ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.

---

### ì´ ë¯¸ì…˜ì—ì„œì˜ Cosine Similarity í•´ì„

ì´ë²ˆ ë¯¸ì…˜ì—ì„œ Cosine Similarity ê°’ì€ ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

- ê°’ì´ í´ìˆ˜ë¡ â†’ í•´ë‹¹ í…ìŠ¤íŠ¸ ì„¤ëª…ì´ ì´ë¯¸ì§€ì˜ ì˜ë¯¸ë¥¼ ì˜ ë‹´ê³  ìˆë‹¤
- ê°’ì´ ì‘ì„ìˆ˜ë¡ â†’ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ê°€ ì˜ ë§ì§€ ì•ŠëŠ”ë‹¤

ì¦‰, **Cosine SimilarityëŠ” â€œì´ ì„¤ëª…ì´ ì´ë¯¸ì§€ì™€ ì–¼ë§ˆë‚˜ ì˜ ì •ë ¬ë˜ì–´ ìˆëŠ”ê°€â€ë¥¼ ìˆ«ìë¡œ ë³´ì—¬ì£¼ëŠ” ì§€í‘œ**ì…ë‹ˆë‹¤.


## (Todo4, 5) Cosine similarity ì„¤ê³„í•˜ê¸°

import torch

# ============================================
# TODO: Cosine Similarity í•¨ìˆ˜ ì •ì˜
# ============================================

def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    ë‘ ë²¡í„° a, b ì‚¬ì´ì˜ cosine similarityë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

    Parameters
    ----------
    a : torch.Tensor
        shape: [1, D] ë˜ëŠ” [D]
    b : torch.Tensor
        shape: [N, D] ë˜ëŠ” [D]

    Returns
    -------
    torch.Tensor
        cosine similarity ê°’
    """

    # TODO 4-1:
    # aì™€ bì˜ ë‚´ì (dot product)ì„ ê³„ì‚°í•˜ì„¸ìš”.
    # (íŒíŠ¸: torch.matmul ë˜ëŠ” @ ì—°ì‚°ì ì‚¬ìš©)
    dot_product = torch.matmul(a, b.T)

    # TODO 4-2:
    # aì˜ L2 normì„ ê³„ì‚°í•˜ì„¸ìš”.
    # (íŒíŠ¸: torch.norm)
    norm_a = torch.norm(a)

    # TODO 4-3:
    # bì˜ L2 normì„ ê³„ì‚°í•˜ì„¸ìš”.
    # (íŒíŠ¸: torch.norm, dim ì˜µì…˜ ì£¼ì˜)
    norm_b = torch.norm(b, dim=1).T

    # TODO 4-4:
    # cosine similarityë¥¼ ê³„ì‚°í•˜ì„¸ìš”.
    # (dot_product / (norm_a * norm_b))
    cosine_sim = dot_product / (norm_a * norm_b)

    return cosine_sim

# ============================================
# TODO: ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (Cosine Similarity ì‚¬ìš©)
# ============================================

similarity_results = {}

for level, embeddings in text_embeddings.items():
    # image_embedding: [1, D]
    # embeddings: [N, D]

    # cosine similarity ê³„ì‚°
    similarity = cosine_similarity(image_embedding, embeddings)

    similarity_results[level] = similarity.cpu().numpy()

# ê²°ê³¼ ì¶œë ¥
print("Imageâ€“Text Similarity Results (Cosine Similarity):\n")

for level, score in similarity_results.items():
    # scoreëŠ” í˜„ì¬ [1, N] í˜•íƒœì˜ numpy ë°°ì—´ì´ë¯€ë¡œ, float()ìœ¼ë¡œ ë°”ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì²« ë²ˆì§¸ ê°’ì„ ê°€ì ¸ì™€ì„œ ì‚¬ìš© (ì›ë˜ ì˜ë„ëŠ” N=1 ì´ì—ˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •)
    score = score.flatten()[0].item()  # ë°°ì—´ì—ì„œ ìŠ¤ì¹¼ë¼ ê°’ ì¶”ì¶œ
    print(f"{level.upper()}: similarity = {score:.4f}")


import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
data = {'Description Type': [], 'Cosine Similarity': []}
for level, score in similarity_results.items():
    data['Description Type'].append(level.upper())
    # NumPy ë°°ì—´ì—ì„œ ìŠ¤ì¹¼ë¼ ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ì¶”ê°€
    data['Cosine Similarity'].append(score.item())

df_similarity = pd.DataFrame(data)

# ìˆœì„œ ì§€ì •ì„ ìœ„í•´ ì¹´í…Œê³ ë¦¬ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
df_similarity['Description Type'] = pd.Categorical(df_similarity['Description Type'],
                                                  categories=['GOOD', 'MEDIUM', 'BAD'],
                                                  ordered=True)

# ì‹œê°í™”
plt.figure(figsize=(8, 5))
sns.barplot(x='Description Type', y='Cosine Similarity', data=df_similarity, palette='viridis')
plt.title('Cosine Similarity between Image and Text Descriptions')
plt.xlabel('Description Type')
plt.ylabel('Cosine Similarity Score')
plt.ylim(0, 1) # Cosine SimilarityëŠ” -1ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ 0ì—ì„œ 1 ì‚¬ì´ë¡œ ë³´ì—¬ì¤Œ

# ê° ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
for index, row in df_similarity.iterrows():
    plt.text(index, row['Cosine Similarity'] + 0.02, round(row['Cosine Similarity'], 4),
             color='black', ha="center")

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()


### ê²°ê³¼ í•´ì„

ì¶œë ¥ëœ ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ í™•ì¸í–ˆì„ ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ê²½í–¥ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸

- Good ì„¤ëª…ì˜ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ë‹¤
- Medium ì„¤ëª…ì€ ê·¸ë³´ë‹¤ ë‚®ë‹¤
- Bad ì„¤ëª…ì€ ê°€ì¥ ë‚®ë‹¤

ë§Œì•½ ê·¸ë ‡ë‹¤ë©´, ì´ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ ë‹¨ìˆœíˆ ë¬¸ì¥ì„ ì½ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°™ì€ ì˜ë¯¸ ê³µê°„ì—ì„œ ë¹„êµí•˜ê³  ìˆë‹¤ëŠ” ì¦ê±°**


## (Additional) í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¢‹ì€ í”„ë¡¬í”„íŠ¸, ë‚˜ìœ í”„ë¡¬í”„íŠ¸ ì„¤ê³„í•´ë³´ê¸°

ì½”ë“œê°€ ì™„ì„±ë˜ë©´ ìœ„ ì´ë¯¸ì§€ë¥¼ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ë³€ê²½í•˜ê³ , ê·¸ì— ê±¸ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ë©° ìœ ì‚¬ë„ë¥¼ ìµœëŒ€í•œ ë†’í˜€ë³´ê¸°.

---

### 1ï¸âƒ£ ì‹¤ìŠµ ê°œìš”

ì´ë²ˆ ë‹¨ê³„ì—ì„œëŠ” ë‹¤ìŒ ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.

1. ì„ì˜ì˜ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ ì„ íƒí•œë‹¤.
2. í•´ë‹¹ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” **ì¢‹ì€ í”„ë¡¬í”„íŠ¸(good prompt)** ì™€ **ë‚˜ìœ í”„ë¡¬í”„íŠ¸(bad prompt)** ë¥¼ ê°ê° ì„¤ê³„í•œë‹¤.
3. ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.
4. cosine similarityë¥¼ ê³„ì‚°í•˜ì—¬ ê²°ê³¼ë¥¼ ë¹„êµí•œë‹¤.

---

### 2ï¸âƒ£ ì¢‹ì€ í”„ë¡¬í”„íŠ¸ë€ ë¬´ì—‡ì¸ê°€?

ì¢‹ì€ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§„ë‹¤.

- ì´ë¯¸ì§€ì˜ **ì‹œê°ì  ìš”ì†Œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…**í•œë‹¤ (ì‚¬ë¬¼, ìƒ‰ìƒ, ë°°ê²½, í–‰ë™ ë“±)
- ëª¨í˜¸í•˜ì§€ ì•Šê³  **ëª…í™•í•œ ë¬¸ì¥**ìœ¼ë¡œ êµ¬ì„±ëœë‹¤
- ëª¨ë¸ì´ ì¶”ì¸¡í•˜ì§€ ì•Šì•„ë„ ë˜ë„ë¡ **ê´€ì°° ê°€ëŠ¥í•œ ì •ë³´**ë¥¼ ì œê³µí•œë‹¤

ì˜ˆì‹œ:
- â€œA brown dog running on a grassy fieldâ€
- â€œA person riding a bicycle on a city street during the dayâ€

---

### 3ï¸âƒ£ ë‚˜ìœ í”„ë¡¬í”„íŠ¸ë€ ë¬´ì—‡ì¸ê°€?

ë‚˜ìœ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§„ë‹¤.

- ì´ë¯¸ì§€ì™€ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì—†ëŠ” ë‚´ìš©
- ì§€ë‚˜ì¹˜ê²Œ ì¶”ìƒì ì´ê±°ë‚˜ ëª¨í˜¸í•œ í‘œí˜„
- ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ì •ë³´ì— ì˜ì¡´

ì˜ˆì‹œ:
- â€œA happy momentâ€
- â€œSomething interestingâ€
- â€œA picture related to successâ€

###**ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤**
<font color='red'><b>**(ì£¼)ì—…ìŠ¤í…Œì´ì§€ê°€ ì œê³µí•˜ëŠ” ëª¨ë“  êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€
ìš´ì˜ ì£¼ì²´ì¸ (ì£¼)ì—…ìŠ¤í…Œì´ì§€ ë˜ëŠ” í•´ë‹¹ ì €ì‘ë¬¼ì˜ ì ë²•í•œ ê´€ë¦¬ìì—ê²Œ ê·€ì†ë˜ì–´ ìˆìŠµë‹ˆë‹¤.**</b></font>

ì½˜í…ì¸  ì¼ë¶€ ë˜ëŠ” ì „ë¶€ë¥¼ **ë³µì‚¬, ë³µì œ, íŒë§¤, ì¬íŒë§¤ ê³µê°œ, ê³µìœ ** ë“±ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìœ ì¶œë  ê²½ìš° ì§€ì‹ì¬ì‚°ê¶Œ ì¹¨í•´ì— ëŒ€í•œ ì±…ì„ì„ ë¶€ë‹´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìœ ì¶œì— í•´ë‹¹í•˜ì—¬ ê¸ˆì§€ë˜ëŠ” í–‰ìœ„ì˜ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
* ì½˜í…ì¸ ë¥¼ ì¬ê°€ê³µí•˜ì—¬ ì˜¨/ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ê³µê°œí•˜ëŠ” í–‰ìœ„
* ì½˜í…ì¸ ì˜ ì¼ë¶€ ë˜ëŠ” ì „ë¶€ë¥¼ ì´ìš©í•˜ì—¬ ì¸ì‡„ë¬¼ì„ ë§Œë“œëŠ” í–‰ìœ„
* ì½˜í…ì¸ ì˜ ì „ë¶€ ë˜ëŠ” ì¼ë¶€ë¥¼ ë…¹ì·¨ ë˜ëŠ” ë…¹í™”í•˜ê±°ë‚˜ ë…¹ì·¨ë¡ì„ ì‘ì„±í•˜ëŠ” í–‰ìœ„
* ì½˜í…ì¸ ì˜ ì „ë¶€ ë˜ëŠ” ì¼ë¶€ë¥¼ ìŠ¤í¬ë¦° ìº¡ì³í•˜ê±°ë‚˜ ì¹´ë©”ë¼ë¡œ ì´¬ì˜í•˜ëŠ” í–‰ìœ„
* ì§€ì¸ì„ í¬í•¨í•œ ì œ3ìì—ê²Œ ì½˜í…ì¸ ì˜ ì¼ë¶€ ë˜ëŠ” ì „ë¶€ë¥¼ ê³µìœ í•˜ëŠ” í–‰ìœ„
* ë‹¤ë¥¸ ì •ë³´ì™€ ê²°í•©í•˜ì—¬ Upstage Educationì˜ ì½˜í…ì¸ ì„ì„ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” ì €ì‘ë¬¼ì„ ì‘ì„±, ê³µê°œí•˜ëŠ” í–‰ìœ„
* ì œê³µëœ ë°ì´í„°ì˜ ì¼ë¶€ í˜¹ì€ ì „ë¶€ë¥¼ Upstage Education í”„ë¡œì íŠ¸/ì‹¤ìŠµ ìˆ˜í–‰ ì´ì™¸ì˜ ëª©ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í–‰ìœ„

# Task
**Task**: Perform PCA on the image and text embeddings to reduce their dimensionality to 2, then visualize these 2D embeddings in a scatter plot. The plot should clearly distinguish between the image embedding and the 'good', 'medium', and 'bad' text embeddings using different colors or markers, and include a legend. Finally, summarize how the spatial relationships in the visualization demonstrate the semantic alignment between the image and the different text descriptions.

## Prepare Embeddings for PCA

### Subtask:
Combine the single image embedding and the various text embeddings into a single dataset, ensuring appropriate labeling for visualization.


**Reasoning**:
First, I will convert the `image_embedding` tensor to a NumPy array to prepare it for concatenation with text embeddings.



image_embedding_np = image_embedding.cpu().numpy()

**Reasoning**:
The previous cell failed because `image_embedding` was not defined. This step will ensure all necessary embeddings are converted to NumPy arrays and combined into a single dataset with corresponding labels, assuming `image_embedding` and `text_embeddings` have been successfully generated by preceding cells. I will also add the `numpy` import.



import numpy as np

# 1. Convert the `image_embedding` tensor to a NumPy array
# The .cpu() ensures the tensor is on the CPU before converting to NumPy
image_embedding_np = image_embedding.cpu().numpy()

# 2. Convert the 'good', 'medium', and 'bad' text embedding tensors to NumPy arrays
text_embedding_good_np = text_embeddings["good"].cpu().numpy()
text_embedding_medium_np = text_embeddings["medium"].cpu().numpy()
text_embedding_bad_np = text_embeddings["bad"].cpu().numpy()

# 3. Concatenate these NumPy arrays into a single array
all_embeddings_np = np.vstack([
    image_embedding_np,
    text_embedding_good_np,
    text_embedding_medium_np,
    text_embedding_bad_np
])

# 4. Create a list of labels corresponding to each embedding
labels = ['Image', 'Good Text', 'Medium Text', 'Bad Text']

print("Shape of all_embeddings_np:", all_embeddings_np.shape)
print("Labels:", labels)

**Reasoning**:
The previous code failed with a `NameError` because `image_embedding` was not defined. This indicates that the preceding cells responsible for loading the CLIP model and generating both image and text embeddings were not executed or their state was lost. To fix this specific code block and make it runnable for its intended purpose, I must ensure all necessary variables (`image_path`, `device`, `model`, `preprocess`, `image_embedding`, `text_descriptions`, `text_embeddings`) are defined within this block. This will involve re-including the model loading and embedding generation logic, along with the required imports.



import torch
import clip
from PIL import Image
import numpy as np

# Re-define device, model, and preprocess (from cell 4KQUuYo8SmL1)
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Re-define image_path (from cell ynk1Kk6KRoZ6)
image_path = "/content/test_chart.png"

# Re-generate image_embedding (from cell 4KQUuYo8SmL1)
image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
with torch.no_grad():
    image_embedding = model.encode_image(image_input)

# Re-define text_descriptions (from cell 7-sc_hWwSMGU)
text_descriptions = {
    "good": [
        "A line chart titled 'Annual Sales by Category' showing trends from 2020 to 2023. Category A grows, Category B shows slight growth, and Category C declines."
    ],
    "medium": [
        "A line graph showing sales data for three different categories over a four-year period. There are multiple lines with markers indicating data points for each year."
    ],
    "bad": [
        "A bar chart displaying monthly temperature fluctuations in three different cities. The horizontal axis represents months, and the vertical axis shows Celsius degrees."
    ]
}

# Re-generate text_embeddings (from cell GKbnbLVwTbgd)
text_embeddings = {}
with torch.no_grad():
    for level, texts in text_descriptions.items():
        text_tokens = clip.tokenize(texts).to(device)
        embeddings = model.encode_text(text_tokens)
        text_embeddings[level] = embeddings

# 1. Convert the `image_embedding` tensor to a NumPy array
# The .cpu() ensures the tensor is on the CPU before converting to NumPy
image_embedding_np = image_embedding.cpu().numpy()

# 2. Convert the 'good', 'medium', and 'bad' text embedding tensors to NumPy arrays
text_embedding_good_np = text_embeddings["good"].cpu().numpy()
text_embedding_medium_np = text_embeddings["medium"].cpu().numpy()
text_embedding_bad_np = text_embeddings["bad"].cpu().numpy()

# 3. Concatenate these NumPy arrays into a single array
all_embeddings_np = np.vstack([
    image_embedding_np,
    text_embedding_good_np,
    text_embedding_medium_np,
    text_embedding_bad_np
])

# 4. Create a list of labels corresponding to each embedding
labels = ['Image', 'Good Text', 'Medium Text', 'Bad Text']

print("Shape of all_embeddings_np:", all_embeddings_np.shape)
print("Labels:", labels)


**Reasoning**:
The previous code failed with a `ModuleNotFoundError: No module named 'clip'`, indicating the CLIP library is not installed. I need to run the `pip install` command for CLIP before attempting to import and use it. I will create a new cell to install the required packages, and then replicate the previous code block, assuming the installation will resolve the `ModuleNotFoundError`.



!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

print("CLIP and dependencies installed.")

**Reasoning**:
With CLIP and its dependencies now installed, I will re-execute the code block that previously failed due to `ModuleNotFoundError`. This block will redefine the necessary variables (`device`, `model`, `preprocess`, `image_path`, `text_descriptions`, `image_embedding`, `text_embeddings`), generate the embeddings, convert them to NumPy arrays, and then combine them into a single `all_embeddings_np` array with corresponding `labels` for PCA.



import torch
import clip
from PIL import Image
import numpy as np

# Re-define device, model, and preprocess (from cell 4KQUuYo8SmL1)
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Re-define image_path (from cell ynk1Kk6KRoZ6)
image_path = "/content/test_chart.png"

# Re-generate image_embedding (from cell 4KQUuYo8SmL1)
image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
with torch.no_grad():
    image_embedding = model.encode_image(image_input)

# Re-define text_descriptions (from cell 7-sc_hWwSMGU)
text_descriptions = {
    "good": [
        "A line chart titled 'Annual Sales by Category' showing trends from 2020 to 2023. Category A grows, Category B shows slight growth, and Category C declines."
    ],
    "medium": [
        "A line graph showing sales data for three different categories over a four-year period. There are multiple lines with markers indicating data points for each year."
    ],
    "bad": [
        "A bar chart displaying monthly temperature fluctuations in three different cities. The horizontal axis represents months, and the vertical axis shows Celsius degrees."
    ]
}

# Re-generate text_embeddings (from cell GKbnbLVwTbgd)
text_embeddings = {}
with torch.no_grad():
    for level, texts in text_descriptions.items():
        text_tokens = clip.tokenize(texts).to(device)
        embeddings = model.encode_text(text_tokens)
        text_embeddings[level] = embeddings

# 1. Convert the `image_embedding` tensor to a NumPy array
# The .cpu() ensures the tensor is on the CPU before converting to NumPy
image_embedding_np = image_embedding.cpu().numpy()

# 2. Convert the 'good', 'medium', and 'bad' text embedding tensors to NumPy arrays
text_embedding_good_np = text_embeddings["good"].cpu().numpy()
text_embedding_medium_np = text_embeddings["medium"].cpu().numpy()
text_embedding_bad_np = text_embeddings["bad"].cpu().numpy()

# 3. Concatenate these NumPy arrays into a single array
all_embeddings_np = np.vstack([
    image_embedding_np,
    text_embedding_good_np,
    text_embedding_medium_np,
    text_embedding_bad_np
])

# 4. Create a list of labels corresponding to each embedding
labels = ['Image', 'Good Text', 'Medium Text', 'Bad Text']

print("Shape of all_embeddings_np:", all_embeddings_np.shape)
print("Labels:", labels)


**Reasoning**:
The previous error was `FileNotFoundError: [Errno 2] No such file or directory: '/content/test_chart.png'`. This indicates that the image file was not created or was lost. To fix this, I need to re-run the code that generates and saves the image (`plt.savefig`) before attempting to open it. I will incorporate the image generation code at the beginning of the current code block.



import torch
import clip
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# --- Image generation (from cell ynk1Kk6KRoZ6) ---
years = ["2020", "2021", "2022", "2023"]
category_a = [100, 150, 200, 250]
category_b = [120, 130, 140, 160]
category_c = [200, 180, 160, 140]

plt.figure(figsize=(6, 4))
plt.plot(years, category_a, marker="o", label="Category A")
plt.plot(years, category_b, marker="o", label="Category B")
plt.plot(years, category_c, marker="o", label="Category C")

plt.title("Annual Sales by Category")
plt.xlabel("Year")
plt.ylabel("Sales")
plt.legend()
plt.grid(True)

image_path = "/content/test_chart.png"
plt.savefig(image_path, bbox_inches="tight")
plt.close() # Close the plot to prevent it from displaying unnecessarily
# --- End of Image generation ---

# Re-define device, model, and preprocess (from cell 4KQUuYo8SmL1)
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Re-generate image_embedding (from cell 4KQUuYo8SmL1)
image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
with torch.no_grad():
    image_embedding = model.encode_image(image_input)

# Re-define text_descriptions (from cell 7-sc_hWwSMGU)
text_descriptions = {
    "good": [
        "A line chart titled 'Annual Sales by Category' showing trends from 2020 to 2023. Category A grows, Category B shows slight growth, and Category C declines."
    ],
    "medium": [
        "A line graph showing sales data for three different categories over a four-year period. There are multiple lines with markers indicating data points for each year."
    ],
    "bad": [
        "A bar chart displaying monthly temperature fluctuations in three different cities. The horizontal axis represents months, and the vertical axis shows Celsius degrees."
    ]
}

# Re-generate text_embeddings (from cell GKbnbLVwTbgd)
text_embeddings = {}
with torch.no_grad():
    for level, texts in text_descriptions.items():
        text_tokens = clip.tokenize(texts).to(device)
        embeddings = model.encode_text(text_tokens)
        text_embeddings[level] = embeddings

# 1. Convert the `image_embedding` tensor to a NumPy array
# The .cpu() ensures the tensor is on the CPU before converting to NumPy
image_embedding_np = image_embedding.cpu().numpy()

# 2. Convert the 'good', 'medium', and 'bad' text embedding tensors to NumPy arrays
text_embedding_good_np = text_embeddings["good"].cpu().numpy()
text_embedding_medium_np = text_embeddings["medium"].cpu().numpy()
text_embedding_bad_np = text_embeddings["bad"].cpu().numpy()

# 3. Concatenate these NumPy arrays into a single array
all_embeddings_np = np.vstack([
    image_embedding_np,
    text_embedding_good_np,
    text_embedding_medium_np,
    text_embedding_bad_np
])

# 4. Create a list of labels corresponding to each embedding
labels = ['Image', 'Good Text', 'Medium Text', 'Bad Text']

print("Shape of all_embeddings_np:", all_embeddings_np.shape)
print("Labels:", labels)


## Apply PCA for Dimensionality Reduction

### Subtask:
Apply Principal Component Analysis (PCA) to reduce the dimensionality of the combined embeddings to 2 components, making them suitable for 2D plotting.


**Reasoning**:
To reduce the dimensionality of the embeddings, I will import the PCA class, instantiate it with 2 components, and then fit and transform the combined embeddings.



from sklearn.decomposition import PCA

# Instantiate PCA with 2 components
pca = PCA(n_components=2)

# Fit and transform the embeddings
embeddings_2d = pca.fit_transform(all_embeddings_np)

print("Shape of 2D embeddings:", embeddings_2d.shape)

## Visualize 2D Embeddings

### Subtask:
Create a scatter plot of the 2D embeddings, distinguishing between the image embedding and the 'good', 'medium', and 'bad' text embeddings using different colors or markers. Include a legend and title.

#### Instructions
1. Import `matplotlib.pyplot` and `seaborn`.
2. Create a DataFrame for easy plotting, including the 2D embeddings and their corresponding labels.
3. Generate a scatter plot using `seaborn.scatterplot` or `plt.scatter`, ensuring different colors/markers for each label.
4. Add a legend, title, and axis labels to the plot.
5. Adjust plot aesthetics for better readability.

**Reasoning**:
To visualize the 2D embeddings, I will prepare the data by creating a Pandas DataFrame that combines the 2D PCA results with their corresponding labels. Then, I will use `seaborn` to generate a scatter plot to show the spatial relationships between the image and text embeddings, differentiating them by color and marker, and adding a legend and title for clarity.



import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a DataFrame for plotting
df_pca = pd.DataFrame({
    'PC1': embeddings_2d[:, 0],
    'PC2': embeddings_2d[:, 1],
    'Label': labels
})

# Define a color palette for better visualization
colors = {'Image': 'red', 'Good Text': 'green', 'Medium Text': 'orange', 'Bad Text': 'blue'}
markers = {'Image': 'X', 'Good Text': 'o', 'Medium Text': 's', 'Bad Text': '^'}

plt.figure(figsize=(10, 8))
sns.scatterplot(
    data=df_pca,
    x='PC1',
    y='PC2',
    hue='Label',
    style='Label',
    palette=colors,
    markers=markers,
    s=150, # Marker size
    edgecolor='k', # Marker edge color
    alpha=0.8
)

plt.title('2D PCA of Image and Text Embeddings', fontsize=16)
plt.xlabel('Principal Component 1', fontsize=12)
plt.ylabel('Principal Component 2', fontsize=12)
plt.legend(title='Embedding Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, linestyle='--', alpha=0.6)
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)
plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)
plt.tight_layout()
plt.show()


## Summary of Spatial Relationships and Semantic Alignment

The 2D PCA visualization effectively illustrates the semantic alignment between the image and its various text descriptions. Observing the scatter plot, we can infer the following:

- **Proximity of 'Image' and 'Good Text'**: The 'Image' embedding and the 'Good Text' embedding are located closest to each other in the 2D space. This indicates a strong semantic similarity, meaning the 'Good Text' accurately captures the core meaning and details of the image. This aligns with our expectation that a well-crafted description should have high similarity with the image.

- **Distance of 'Medium Text'**: The 'Medium Text' embedding is generally further from the 'Image' embedding than the 'Good Text', but still closer than the 'Bad Text'. This reflects its partial accuracy; while it describes some aspects of the chart, it lacks the specificity and detail present in the 'Good Text', leading to a reduced, but still recognizable, semantic connection.

- **Farthest Distance of 'Bad Text'**: The 'Bad Text' embedding is positioned furthest from the 'Image' embedding. This significant distance underscores its lack of semantic relevance, as it describes a completely different type of chart and content. This serves as a clear visual confirmation that the model correctly identifies unrelated descriptions as being semantically distant.

In conclusion, the PCA plot visually corroborates the quantitative cosine similarity scores. It shows that embeddings that are semantically aligned (like the image and its good description) are clustered together in the embedding space, while unrelated embeddings are dispersed further apart. This demonstrates how multi-modal models like CLIP learn to project different modalities (images and text) into a shared semantic space where their meaning-based relationships are preserved.

## Summary of Spatial Relationships and Semantic Alignment

The 2D PCA visualization effectively illustrates the semantic alignment between the image and its various text descriptions. Observing the scatter plot, we can infer the following:

- **Proximity of 'Image' and 'Good Text'**: The 'Image' embedding and the 'Good Text' embedding are located closest to each other in the 2D space. This indicates a strong semantic similarity, meaning the 'Good Text' accurately captures the core meaning and details of the image. This aligns with our expectation that a well-crafted description should have high similarity with the image.

- **Distance of 'Medium Text'**: The 'Medium Text' embedding is generally further from the 'Image' embedding than the 'Good Text', but still closer than the 'Bad Text'. This reflects its partial accuracy; while it describes some aspects of the chart, it lacks the specificity and detail present in the 'Good Text', leading to a reduced, but still recognizable, semantic connection.

- **Farthest Distance of 'Bad Text'**: The 'Bad Text' embedding is positioned furthest from the 'Image' embedding. This significant distance underscores its lack of semantic relevance, as it describes a completely different type of chart and content. This serves as a clear visual confirmation that the model correctly identifies unrelated descriptions as being semantically distant.

In conclusion, the PCA plot visually corroborates the quantitative cosine similarity scores. It shows that embeddings that are semantically aligned (like the image and its good description) are clustered together in the embedding space, while unrelated embeddings are dispersed further apart. This demonstrates how multi-modal models like CLIP learn to project different modalities (images and text) into a shared semantic space where their meaning-based relationships are preserved.

## Final Task

### Subtask:
Summarize the PCA visualization, explaining how the spatial relationships of the embeddings demonstrate the semantic alignment between the image and the different text descriptions.


## Summary:

### Q&A
The spatial relationships in the PCA visualization demonstrate semantic alignment by showing that embeddings with similar meanings are located closer together, while those with dissimilar meanings are further apart. Specifically:
*   The 'Image' embedding and the 'Good Text' embedding are positioned closest to each other, indicating a strong semantic match.
*   The 'Medium Text' embedding is at an intermediate distance from the 'Image' embedding, reflecting its partial relevance.
*   The 'Bad Text' embedding is furthest from the 'Image' embedding, highlighting its complete lack of semantic relevance to the image.

### Data Analysis Key Findings
*   One image embedding and three text embeddings (good, medium, bad) were successfully combined into a single dataset, forming an array of shape (4, 512).
*   Principal Component Analysis (PCA) successfully reduced the dimensionality of these combined embeddings from 512 to 2, resulting in an array of shape (4, 2).
*   The 2D PCA scatter plot effectively visualized these embeddings, clearly distinguishing between the image and different text descriptions using distinct colors and markers.
*   The 'Image' embedding and the 'Good Text' embedding were observed to be in close proximity in the 2D space, demonstrating high semantic similarity.
*   The 'Medium Text' embedding was positioned further from the 'Image' than the 'Good Text', but still closer than the 'Bad Text', reflecting its moderate semantic alignment.
*   The 'Bad Text' embedding was located furthest from the 'Image' embedding, visually confirming its lack of semantic connection.

### Insights or Next Steps
*   The PCA visualization provides strong qualitative evidence that the CLIP model effectively projects images and text into a shared semantic space where proximity indicates semantic similarity, validating its utility for multimodal understanding tasks.
*   This methodology can be extended to evaluate the quality of text descriptions for a larger set of images or to identify outliers in a dataset where image-text pairs might be semantically mismatched.
