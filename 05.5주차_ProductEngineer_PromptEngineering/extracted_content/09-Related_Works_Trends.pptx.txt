[9강] Related Works & Trends 
조현수 
이화여자대학교 인공지능학과 조교수 
© 2025 Upstage Co., Ltd. 
2 저작권 안내 
 
(주)업스테이지가 제공하는 모든 교육 콘텐츠의 지식재산권은 
운영 주체인 (주)업스테이지 또는 해당 저작물의 적법한 관리자에게 귀속되어 있습니다. 
콘텐츠 일부 또는 전부를 복사, 복제, 판매, 재판매 공개, 공유 등을 할 수 없습니다. 
유출될 경우 지식재산권 침해에 대한 책임을 부담할 수 있습니다. 
유출에 해당하여 금지되는 행위의 예시는 다음과 같습니다. 
● 콘텐츠를 재가공하여 온/오프라인으로 공개하는 행위 
● 콘텐츠의 일부 또는 전부를 이용하여 인쇄물을 만드는 행위 
● 콘텐츠의 전부 또는 일부를 녹취 또는 녹화하거나 녹취록을 작성하는 행위 
● 콘텐츠의 전부 또는 일부를 스크린 캡쳐하거나 카메라로 촬영하는 행위 
● 지인을 포함한 제3자에게 콘텐츠의 일부 또는 전부를 공유하는 행위 
● 다른 정보와 결합하여 Upstage Education의 콘텐츠임을 알아볼 수 있는 저작물을 작성, 공개하는 행위 
● 제공된 데이터의 일부 혹은 전부를 Upstage Education 프로젝트/실습 수행 이외의 목적으로 사용하는 행위 
3 강의 목표 
학습 목표 
● Multimodal Prompting에 대해 이해할 수 있다. 
● 개인화와 장기 지속 메모리에 대해 이해할 수 있다. 

4 01. Multimodal Prompting 
(1) Multimodal model 
(2) Multimodal Prompt 
02. 개인화 및 장기 지속 메모리 목차 
5 Multimodal Prompting 01
멀티모달 프롬프팅의 개념 
6 Multimodal Model 
모델이 이해하는 프롬프트의 단위 
기존에는 프롬프트를 자연어 문장으로 인식하지만 실제로 모델이 처리하는 입력은 단어가 아닌 토큰 
토큰이란? 
모델이 텍스트를 처리할 때 사용하는 최소 계산 단위 
인간이 이해하는 ʻ단어' 단위와 일치하지 않을 수 있음 
→ 모델은 문장을 이해하는 것이 아니라 토큰들의 순차적 조합과 확률적 관계를 학습함 
[1] https://medium.com/@ajayvardhan2001/prompt-power-week-10-exploring-multi-modal-prompting-text-images-and-beyond-00aa0d74fc1d 09 Related Works & Trends 
7 Multimodal Model 
Multimodal Prompt는 어떻게 구성할 수 있을까 
텍스트와 같이 이미지와 오디오도 토큰 시퀀스로 구성할 수 있고, 
이 토큰들이 언어 토큰과 의미적으로 정렬될 때(같은 의미 공간에서 대응 가능해질 경우)  모든 모달리티는 프롬프트를 구성할 수 있는 
요소가 됨 
[1] https://medium.com/@ajayvardhan2001/prompt-power-week-10-exploring-multi-modal-prompting-text-images-and-beyond-00aa0d74fc1d 
09 Related Works & Trends 
8 Multimodal Model 
Multimodal Model에서 어떻게 서로 다른 모달리티를 이해할까 
각각 다른 모달리티를 어떻게 동시에 이해할 수 있는지 
● 각각의 인코더로 데이터를 압축 
● 이미지 인코더: 입력된 이미지를 분석하여 시각적 특징(색상, 모양, 객체 등)을 추출하고 이를 숫자 벡터로 변환 
● 텍스트 인코더: 입력된 텍스트를 토큰화하고 문맥적 의미를 담은 숫자 벡터로 변환 
● 오디오 인코더: 소리 파형을 분석하여 음성 특징을 담은 숫자 벡터로 변환 
● 각각의 벡터를 호환 가능하도록 공통 임베딩 공간에 배치 (정렬) 
● (예시) 고양이 사진에 해당하는 임베딩 벡터와 “고양이” 텍스트에 해당하는 벡터가 가까이 놓이도록 정렬 
09 Related Works & Trends 
9 Multimodal Model 
Multimodal Model 
Multimodal Model의 작동 방식 -  “Visual Instruction Tuning" (LLaVA, NeurIPS 2023) 
LLM과 비전 인코더를 결합하여, 시각 정보를 이해하고 자연어 지시를 수행할 수 있는 멀티모달 모델을 제안 
사전 학습된 LLM(Vicuna)와 비전 인코더(CLIP ViT-L/14)를 연결하여 시각 정보를 언어 모델이 이해할 수 있는 형태로 변환 
[1] Liu et al., Visual Instruction Tuning, NeurIPS 2023 
W projection을 통해 이미지 X_v의 시각 특징 
Z_v가 언어 모델이 이해할 수 있는 언어 임베딩 
토큰 H_v로 변환 시각 특징 H_v와 언어 지시 H_q를 함께 처리 09 Related Works & Trends 
10 Multimodal Prompt 
Image + Text 
가장 활발하게 연구되고 상용화된 분야 
● 시각적 질의응답: 이미지 속 내용에 대한 사용자의 질문에 AI가 답하는 작업 
● 이미지 캡션 생성 및 스토리텔링: 이미지를 보고 그 내용을 묘사하는 문장을 생성하거나 이야기를 이어서 창작하는 작업 
● 문서 이해: 이미지 속 글자를 인식하고, 문서의 레이아웃을 이해하고 필요한 정보를 추출하거나 요약하는 작업 
시각적 질의응답 이미지 캡션 생성 
및 스토리 텔링 문서 이해 09 Related Works & Trends 
11 Multimodal Prompt 
Image + Text (1): 시각적 질의응답 
예시: 
[입력 이미지]: 냉장고 내부 사진 
[입력 텍스트]: :”지금 냉장고에 있는 재료로 만들 수 있는 요리 3가지만 추천해 줘” 
[출력]: “계란말이, 파전, 우유 라면을 추천” 
원리: 이미지 인코더가 냉장고 속 재료들을 인식하여 시각적 특징 벡터로 변환하고, 텍스트 인코더가 질문을 언어 벡터로 변환, 두 정보를 융합하여 재료 
목록과 요리 추천이라는 지식을 연결해 답변을 생성할 수 있음. (텍스트 인코더를 통해 이미지 인코더가 주목해야할 부분에 가중치 부여) 
09 Related Works & Trends 
12 Multimodal Prompt 
Image + Text (1): 시각적 질의응답 
Flamingo (DeepMind, 2022) 
이미지·비디오 + 텍스트를 함께 입력받아 Few-shot 추론이 가능해진 VLM을 제안 
강력한 사전학습 LLM은 그대로 유지하되 외부에 VIsion Encoder를 붙이고 Cross-Attention Layer로 시각 정보를 LLM에 주입 
→ LLM의 언어 능력을 보존하면서 시각 이해 능력 추가 
09 Related Works & Trends 
13 Multimodal Prompt 
Image + Text (1): 시각적 질의응답 
Flamingo (DeepMind, 2022) 
이미지는 Vision Encoder에서 처리되고, LLM은 그대로 둔 채, 중간중간 Cross-Attention으로 시각 정보를 받아서 텍스트를 생성 
이미지를 프롬프트의 일부로 취급 많은 시각 토큰을 고정 
개수의 요약 토큰으로 압축 09 Related Works & Trends 
14 Multimodal Prompt 
Image + Text (1): 시각적 질의응답 
Flamingo (DeepMind, 2022) 
서로 다른 모델을 사용해서 태스크를 해결하는 것보다 Flamingo 하나에 대해서 프롬프트를 각기 제공했을 때 오히려 문제 해결 능력이 올라감 
09 Related Works & Trends 
15 Multimodal Prompt 
Image + Text (2): 이미지 캡션 생성 및 스토리텔링 
BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (ICML 2023) 
09 Related Works & Trends 
16 Multimodal Prompt 
Audio + Text 
음성 인식 기술의 발전과 함께 오디오의 비언어적 정보까지 분석하는 방향, 긴 오디오 파일의 내용을 텍스트로 변환하고, 화자 분리, 감정 분석 등을 통해 
핵심 정보를 요약하거나 특정 구간을 찾아내는 작업이 포함 
예시: 
[입력 오디오]: 1시간 분량의 팀 회의 녹음 파일 
[입력 텍스트]: "김 부장님이 화를 냈던 부분이 언제인지 타임스탬프를 찍고, 화낸 이유를 요약해 줘." 
[출력]: "타임스탬프: 35분 20초 ~ 36분 10초. 이유: 프로젝트 납기 지연에 대한 보고가 늦어진 점에 대해 질책함." 
원리: 오디오 모델이 음성을 텍스트로 변환(STT)하는 동시에, 목소리의 높낮이(Pitch), 크기(Volume), 속도 등의 
특징을 분석하여 감정 상태를 추론. 텍스트 프롬프트의 '화'라는 키워드와 분석된 감정 정보를 매칭하여 해당 구간을 
찾고 내용을 요약 
텍스트 변환 화자 분리 
및 감정 분석 정보 요약 09 Related Works & Trends 
17 Multimodal Prompt 
Audio + Text 
"Whisper: Robust Speech Recognition via Large-Scale Weak Supervision" (OpenAI, 2022): 다양한 환경과 언어에서 매우 정확한 음성 인식을 
수행하며, 이후 다양한 멀티모달 오디오 분석 모델의 기반이 되고 있음 
09 Related Works & Trends 
18 Multimodal Prompt 
Video + Text 
긴 비디오 내에서 사용자가 원하는 특정 행동, 사물, 또는 사건이 발생한 시점을 정확히 찾아내거나 관련 정보를 추출하는 작업 
예시: 
[입력 비디오]: 30분짜리 요리 유튜브 영상 
[입력 텍스트]: "파스타 면을 삶기 시작하는 순간이 몇 분 몇 초인지 알려줘." 
[출력]: "05분 23초부터 면을 삶기 시작합니다." 
원리: 비디오를 프레임 단위의 이미지 시퀀스와 오디오 트랙으로 분리, AI는 시각 정보(물이 끓는 장면, 면을 넣는 동작) 
와 청각 정보(물 끓는 소리, 설명하는 목소리)를 종합적으로 분석하여 텍스트 질의('면을 삶기 시작')에 해당하는 시점을 
특정
행동 및 시점 검색 객체 인식 
및 상황 식별 정보 추출 및 요약 09 Related Works & Trends 
19 Multimodal Prompt 
Video + Text 
"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding" (EMNLP 2023): 비디오의 시각 및 청각 
정보를 LLM과 결합하여, 사용자의 자연어 지시에 따라 비디오 내용을 이해하고 답변하는 모델 
"InternVideo: General Video Foundation Models via Hybrid Multimodal Learning" (2022): 다양한 비디오 관련 작업(행동 인식, 검색 등)에서 
우수한 성능을 보이는 범용 비디오 기초 모델 
09 Related Works & Trends 
20 개인화 및 장기 지속 메모리 02
현재 LLM 한계 및 사용자 요구 변화 
● 현재 LLM의 한계: 
○ Stateless Interaction: 대부분의 LLM은 세션이 종료되면 사용자와의 대화 내용을 망각 
○ Fixed Context Window: 모델이 한 번에 처리할 수 있는 토큰 양은 물리적으로 제한, 길어질수록 비용과 시간 증가 
● 패러다임의 변화: 
○ Short-term: 현재 대화의 맥락 (Working Memory) 
○ Long-term: 과거의 경험, 사용자의 선호도, 특수 지식 (Persistent Memory) 
● Goal: 사용자의 성향과 과거 이력을 영구적으로 저장하여, 개인을 위한 LLM 구축 
21 개인화 및 장기 지속 메모리 09 Related Works & Trends 
지능적인 데이터 선별 (Smart Information Filtering) 
● 목적: 모든 것을 저장하면 메모리 효율이 떨어지므로, AI가 ʻ가치 있는 정보'를 골라내야함. 
● 메모리 저장의 우선순위: 
○ 단기 소멸성 정보 (Discard): “오늘 날씨 어때?” , “이 문장 번역해줘" (단순 수행 후 삭제) 
○ 업데이트 정보 (Update): “나 이제 서울 살고 있어” (기존의 ʻ부산 거주' 정보를 수정) 
○ 장기 보존 정보 (Retain): “나는 견과류 알레르기가 있어" , “보고서는 항상 요약부터 해줘" (사용자 고유 특성) 
● 핵심 로직: 
○ 반복성: 여러 번 언급되는 패턴인가? 
○ 중요성: 사용자의 핵심 가치나 선호도인가? 
○ 최신성: 과거의 정보보다 더 유효한 최신 정보인가? 
22 개인화 및 장기 지속 메모리 09 Related Works & Trends 
RAG와의 차별성 
23 개인화 및 장기 지속 메모리 09 Related Works & Trends 
구분 Retrieval-Augmented Generation (RAG) Persistent Memory 
(Personalization) 
대상 외부 문서 데이터  (PDF, Wiki 등)사용자  고유 데이터  ( 선호도 , 
이력)
업데이트 정적(Static) 이거나  주기적  인덱싱 실시간 / 동적(Dynamic) 업데이트 
핵심 목적 할루시네이션  방지 및 최신 정보 제공사용자  맞춤형  응답 및 연속성  
유지
데이터  구조 단순 Vector DB Vector DB + Knowledge Graph + 
User Profile 
개인화의 전제 조건: 신뢰와 보안 
● 목적: 개인화는 민감한 정보를 다루기 때문에, 보안과 사용자 통제권이 필수적 
● 사용자 중심의 데이터 통제: 
○ 열람 및 수정: 내 장기 메모리에 무엇이 저장되어 있는지 언제든 확인 가능 
○ 잊힐 권리 (Forgettable AI): 특정 기억이나 세션을 선택적으로 삭제 요청 
● 보안 아키텍처: 
○ 데이터 암호화: 저장된 개인화 데이터의 비식별화 처리 
○ 로컬 처리 우선: 민감 정보는 서버가 아닌 사용자 기기(On-device)에서 처리 지향 
○ 투명성: AI가 왜 이런 답변을 했는지(어떤 과거 기억을 참조했는지) 근거 제시 
24 개인화 및 장기 지속 메모리 09 Related Works & Trends 
최종 개인화의 목표 
● 수동적 대응에서 능동적 제안: 
○ (과거) 물어보는 말에만 대답 → (미래) 사용자의 습관을 바탕으로 할 일을 먼저 제안 
● 연속성 있는 페르소나: 파편화된 대화가 아닌, 시간이 흐를수록 나를 더 깊이 이해하는 동반자적 AI 
● 초개인화된 에이전트: 나의 업무 스타일, 말투, 가치관을 학습하여 나를 대신해서 복잡한 의사결정을 지원 
25 개인화 및 장기 지속 메모리 09 Related Works & Trends 
26 Summary 09 Related Works & Trends 
Related Works & Trends 
● Multimodal Prompting 
● 개인화 LLM 

www.upstage.ai © 2025 Upstage Co., Ltd. 

